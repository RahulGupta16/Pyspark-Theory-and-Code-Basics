GENERAL INTRODUCTION INTO SPARK-

Apache Spark is a unified computing engine and a set of libraries for parallel data processing on
computer clusters. Spark is the most actively developed open source engine for this task, making
it a standard tool for any developer or data scientist interested in big data. Spark supports 
multiple widely used programming languages (Python, Java, Scala, and R), includes libraries for
diverse tasks ranging from SQL to streaming and machine learning, and runs anywhere from a laptop
to a cluster of thousands of servers. This makes it an easy system to start with and scale-up 
big data processing or incredibly large scale.

Spark is designed to support a wide range of data analytics tasks, ranging from simple data 
loading and SQL queries to machine learning and streaming computation, over the same computing 
engine and with a consistent set of APIs. 


PROPERTIES-
1. Spark’s unified nature makes these tasks both easier and more efficient to write. First, Spark
provides consistent, composable APIs that you can use to build an application out of smaller
pieces or out of existing libraries. It also makes it easy for you to write your own analytics
libraries on top. Spark’s APIs are also designed to enable high performance by optimizing across
the different libraries and functions composed together in a user program. 

For example, if you load data using a SQL query and+
then evaluate a machine learning model over it using Spark’s ML library, the engine can
combine these steps into one scan over the data. The combination of general APIs and high
performance execution, no matter how you combine them, makes Spark a powerful platform
for interactive and production applications.

2. Spark handles loading data from storage systems and
performing computation on it, not permanent storage as the end itself. You can use Spark
with a wide variety of persistent storage systems, including cloud storage systems such as
Azure Storage and Amazon S3, distributed file systems such as Apache Hadoop, key-value
stores such as Apache Cassandra, and message buses such as Apache Kafka. However, Spark
neither stores data long term itself, nor favors one over another. The key motivation here is
that most data already resides in a mix of storage systems. Data is expensive to move so
Spark focuses on performing computations over the data, no matter where it resides. In userfacing 
APIs, Spark works hard to make these storage systems look largely similar so that
applications do not need to worry about where their data is.

3. Spark’s final component is its libraries, which build on its design as a unified engine to
provide a unified API for common data analysis tasks. Spark supports both standard libraries
that ship with the engine as well as a wide array of external libraries published as third-party
packages by the open source communities. Today, Spark’s standard libraries are actually the
bulk of the open source project: the Spark core engine itself has changed little since it was
first released, but the libraries have grown to provide more and more types of functionality.
Spark includes libraries for SQL and structured data (Spark SQL), machine learning (MLlib),
stream processing (Spark Streaming and the newer Structured Streaming), and graph
analytics (GraphX). Beyond these libraries, there are hundreds of open source external
libraries ranging from connectors for various storage systems to machine learning algorithms.


SPARK ARCHITECTURE

Single machines do not have enough power and resources to perform
computations on huge amounts of information (or the user probably does not have the time to
wait for the computation to finish). A cluster (or group of computers), pools the resources of
many machines together, giving us the ability to use all the cumulative resources as if they were
a single computer. Now, a group of machines alone is not powerful, you need a framework to
coordinate work across them. Spark does just that, managing and coordinating the execution of
tasks on data across a cluster of computers.


Spark Applications consist of 
(i): a driver process.
(ii): a set of executor processes.

The driver process runs your main() function, sits on a node in the cluster, and is responsible 
for three things: maintaining information about the Spark Application; responding to a user’s program or input;
and analyzing, distributing, and scheduling work across the executors.  It maintains all
relevant information during the lifetime of the application.

The executors are responsible for actually carrying out the work that the driver assigns them.
This means that each executor is responsible for only two things: executing code assigned to it
by the driver, and reporting the state of the computation on that executor back to the driver node.

In shorts, Spark employs a cluster manager that keeps track of the resources available. The driver process 
is responsible for executing the driver program’s commands across the executors to complete a given task. 


STARTING SPARK
When you start Spark in this interactive mode, you implicitly create a SparkSession that manages
the Spark Application. When you start it through a standalone application, you must create the
SparkSession object yourself in your application code. The SparkSession instance is the way Spark executes
user-defined manipulations across the cluster. There is a one-to-one correspondence between a
SparkSession and a Spark Application. 
  

In Python, the variable is available as "spark" when you start the console.

>>>> spark

Let’s now perform the simple task of creating a range of numbers. This range of numbers is just
like a named column in a spreadsheet:

>>>> myRange = spark.range(1000).toDF("number")

Here above, we created a DataFrame with one column containing 1,000
rows with values from 0 to 999. This range of numbers represents a distributed collection. When
run on a cluster, each part of this range of numbers exists on a different executor. This is a Spark
DataFrame.


DATAFRAMES-
A DataFrame is the most common Structured API and simply represents a table of data with
rows and columns. A spreadsheet sits on one computer in one specific
location, whereas a Spark DataFrame can span thousands of computers.

Partitions-
To allow every executor to perform work in parallel, Spark breaks up the data into chunks called
partitions. A partition is a collection of rows that sit on one physical machine in your cluster. A
DataFrame’s partitions represent how the data is physically distributed across the cluster of
machines during execution. If you have one partition, Spark will have a parallelism of only one,
even if you have thousands of executors. If you have many partitions but only one executor,
Spark will still have a parallelism of only one because there is only one computation resource.
With DataFrames you do not (for the most part) manipulate
partitions manually or individually. You simply specify high-level transformations of data in the
physical partitions, and Spark determines how this work will actually execute on the cluster.


Transformations-
In Spark, the core data structures are immutable, meaning they cannot be changed after they’re
created. To “change” a DataFrame, you need to instruct Spark how you would like to
modify it to do what you want. These instructions are called transformations.

# in Python
>>>> divisBy2 = myRange.where("number % 2 = 0")

Notice that these return no output. This is because we specified only an abstract transformation,
and Spark will not act on transformations until we call an action.

Transformations are the core of how you express your business logic using Spark. There are two
types of transformations: those that specify narrow dependencies, and those that specify wide
dependencies.

Transformations consisting of narrow dependencies (we’ll call them narrow transformations) are
those for which each input partition will contribute to only one output partition. In the preceding
code snippet, the where statement specifies a narrow dependency, where only one partition
contributes to at most one output partition.

A wide dependency (or wide transformation) style transformation will have input partitions
contributing to many output partitions. This is also referred as 'Shuffle'.



Lazy Evaluation-
Lazy evaulation means that Spark will wait until the very last moment to execute the graph of
computation instructions. In Spark, instead of modifying the data immediately when you express
some operation, you build up a plan of transformations that you would like to apply to your
source data. By waiting until the last minute to execute the code, Spark compiles this plan from
your raw DataFrame transformations to a streamlined physical plan that will run as efficiently as
possible across the cluster. This provides some immense benifit as Spark can optimize the
entire data flow from end to end.

If we build a large Spark job but specify a filter at the end that only requires us to
fetch one row from our source data, the most efficient way to execute this is to access the single
record that we need. Spark will actually optimize this for us by pushing the filter down
automatically.


Actions-
Transformations allow us to build up our logical transformation plan. To trigger the computation,
we run an action. An action instructs Spark to compute a result from a series of transformations. 

Eg.

>>>> divisBy2.count()  # gives us the total number of records in the DataFrame

There are three kinds of actions:
1. Actions to view data in the console
2. Actions to collect data to native objects in the respective language
3. Actions to write to output data sources


Lets refer one eg.

Now, let’s sort our data according to the count column, which is an integer type.

Nothing happens to the data when we call sort because it’s just a transformation. However, we
can see that Spark is building up a plan for how it will execute this across the cluster by looking
at the explain plan. We can call "explain" on any DataFrame object to see the DataFrame’s
lineage (or how Spark will execute this query):

>>>> flightData2015.sort("count").explain()                       ### "explain" method is used to see the DataFrame’s lineage (or how Spark will execute this query)

O/P-
== Physical Plan ==
*Sort [count#195 ASC NULLS FIRST], true, 0
+- Exchange rangepartitioning(count#195 ASC NULLS FIRST, 200)
+- *FileScan csv [DEST_COUNTRY_NAME#193,ORIGIN_COUNTRY_NAME#194,count#195] ...

You can read "explain" plans from top to bottom, the
top being the end result, and the bottom being the source(s) of data. In this case, take a look at
the first keywords. You will see sort, exchange, and FileScan. That’s because the sort of our data
is actually a wide transformation because rows will need to be compared with one another.


The logical plan of transformations that we build up defines a lineage for the DataFrame so that
at any given point in time, Spark knows how to recompute any partition by performing all of the
operations it had before on the same input data. This sits at the heart of Spark’s programming
model—functional programming where the same inputs always result in the same outputs when
the transformations on that data stay constant.
We do not manipulate the physical data; instead, we configure physical execution characteristics
through things like the shuffle partitions parameter.


DATAFRAMES AND SQL

We worked through a simple transformation in the previous example, let’s now work through a
more complex one and follow along in both DataFrames and SQL. Spark can run the same
transformations, regardless of the language, in the exact same way. You can express your
business logic in SQL or DataFrames (either in R, Python, Scala, or Java) and Spark will
compile that logic down to an underlying plan (that you can see in the explain plan) before
actually executing your code. With Spark SQL, you can register any DataFrame as a table or
view (a temporary table) and query it using pure SQL. There is no performance difference
between writing SQL queries or writing DataFrame code, they both “compile” to the same
underlying plan that we specify in DataFrame code.

To make any DataFrame into a table or view, call this: 
>>>> flightData2015.createOrReplaceTempView("flight_data_2015") 


# in Python
sqlWay = spark.sql("""
SELECT DEST_COUNTRY_NAME, count(1)
FROM flight_data_2015
GROUP BY DEST_COUNTRY_NAME
""")


dataFrameWay = flightData2015\
.groupBy("DEST_COUNTRY_NAME")\
.count()
sqlWay.explain()
dataFrameWay.explain()


== Physical Plan ==
*HashAggregate(keys=[DEST_COUNTRY_NAME#182], functions=[count(1)])
+- Exchange hashpartitioning(DEST_COUNTRY_NAME#182, 5)
+- *HashAggregate(keys=[DEST_COUNTRY_NAME#182], functions=[partial_count(1)])
+- *FileScan csv [DEST_COUNTRY_NAME#182] ...
== Physical Plan ==
*HashAggregate(keys=[DEST_COUNTRY_NAME#182], functions=[count(1)])
+- Exchange hashpartitioning(DEST_COUNTRY_NAME#182, 5)
+- *HashAggregate(keys=[DEST_COUNTRY_NAME#182], functions=[partial_count(1)])
+- *FileScan csv [DEST_COUNTRY_NAME#182] ...


Let’s pull out some interesting statistics from our data. One thing to understand is that
DataFrames (and SQL) in Spark already have a huge number of manipulations available.

We will use the "max" function, to establish the maximum number of flights to and from any
given location. This just scans each value in the relevant column in the DataFrame and checks
whether it’s greater than the previous values that have been seen. This is a transformation,
because we are effectively filtering down to one row.

>>>> spark.sql("SELECT max(count) from flight_data_2015").take(1)

# in Python
>>>> from pyspark.sql.functions import max
>>>> flightData2015.select(max("count")).take(1)


Let’s perform something a bit more
complicated and find the top five destination countries in the data. This is our first multitransformation query,
so we’ll take it step by step. Let’s begin with a fairly straightforward SQL aggregation:

# in Python
>>>> maxSql = spark.sql("""
>>>> SELECT DEST_COUNTRY_NAME, sum(count) as destination_total
>>>> FROM flight_data_2015
>>>> GROUP BY DEST_COUNTRY_NAME
>>>> ORDER BY sum(count) DESC
>>>> LIMIT 5 """)

>>>> maxSql.show()
+-----------------+-----------------+
|DEST_COUNTRY_NAME|destination_total|
+-----------------+-----------------+
| United States| 411352|
| Canada| 8399|
| Mexico| 7140|
| United Kingdom| 2025|
| Japan| 1548|
+-----------------+-----------------+


Now, let’s move to the DataFrame syntax that is semantically similar but slightly different in
implementation and ordering

# in Python
>>>> from pyspark.sql.functions import desc
>>>> flightData2015\
>>>> .groupBy("DEST_COUNTRY_NAME")\
>>>> .sum("count")\
>>>> .withColumnRenamed("sum(count)", "destination_total")\
>>>> .sort(desc("destination_total"))\
>>>> .limit(5)\
>>>> .show()
+-----------------+-----------------+
|DEST_COUNTRY_NAME|destination_total|
+-----------------+-----------------+
| United States| 411352|
| Canada| 8399|
| Mexico| 7140|
| United Kingdom| 2025|
| Japan| 1548|
+-----------------+-----------------+


NOTE-- The execution plan is a directed acyclic graph (DAG) of transformations, each resulting in a new immutable DataFrame, on which we call an action to generate a result.



CHAPTER 3 -- A Tour of Spark’s Toolset

Building blocks of Apache Spark’s vast ecosystem of tools and libraries. 

Spark is composed of these primitives—the lower-level APIs (RDD, Distributed Variables) 
and the Structured APIs (Datasets, Dataframes, SQL)—and then a series of standard
libraries (Structural Streaming, Advanced Analytics, Libraries & Ecosystem) for additional functionality.
Spark’s libraries support a variety of different tasks, from graph analysis and machine learning to
streaming and integrations with a host of computing and storage systems.

This chapter covers the following:
1. Running production applications with spark-submit
2. Datasets: type-safe APIs for structured data
3. Structured Streaming
4. Machine learning and advanced analytics
5. Resilient Distributed Datasets (RDD): Spark’s low level APIs
6. SparkR
7. The third-party package ecosystem


1. Running production applications with spark-submit-


Spark makes it easy to develop and create big data programs. Spark also makes it easy to turn
your interactive exploration into production applications with a built-in command line tool "spark-submit".
It lets you send your application code to a cluster and launch it to execute there. 
Upon submission, the application will run until it exits (completes the task) or encounters an error.
You can do this with all of Spark’s support cluster managers including Standalone, Mesos, and YARN.

"spark-submit" offers several controls with which you can specify the resources your application
needs as well as how it should be run and its command-line arguments.

This sample application calculates the digits of pi to a certain level of estimation using Python. Here, we’ve
told spark-submit that we want to run on our local machine, which class and which JAR we
would like to run, and some command-line arguments for that class.

./bin/spark-submit \
--master local \
./examples/src/main/python/pi.py 10


2. Datasets: Type-Safe Structured APIs

The first API we’ll describe is a type-safe version of Spark’s structured API called "Datasets", for
writing statically typed code in Java and Scala. The "Dataset" API is NOT available in Python.

The Dataset API gives users the ability to assign a Java/Scala class to the records within a DataFrame
and manipulate it as a collection of typed objects, similar to a Java ArrayList or Scala Seq. 
The APIs available on Datasets are type-safe, meaning that you cannot accidentally view the objects 
in a Dataset as being of another class than the class you put in initially.

One great thing about Datasets is that you can use them only when you need or want to.


3. Structured Streaming-
Structured Streaming is a high-level API for stream processing that became production-ready in
Spark 2.2. With Structured Streaming, you can take the same operations that you perform in
batch mode using Spark’s structured APIs and run them in a streaming fashion. This can reduce
latency and allow for incremental processing. The best thing about Structured Streaming is that it
allows you to rapidly and quickly extract value out of streaming systems with virtually no code
changes. It also makes it easy to conceptualize because you can write your batch job as a way to
prototype it and then you can convert it to a streaming job. The way all of this works is by
incrementally processing that data.






CHAPTER 4- Structured API Overview

The Structured APIs are a tool for manipulating all sorts of data, from unstructured log files to
semi-structured CSV files and highly structured Parquet files. These APIs refer to three core types 
of distributed collection APIs:-

1. Datasets
2. DataFrames
3. SQL tables and views

Structured APIs apply to both batch and streaming computation. This means that when you work with the 
Structured APIs, it should be simple to migrate from batch to streaming (or vice versa) with little to no effort.


DataFrames and Datasets-

DataFrames and Datasets are (distributed) table-like collections with well-defined rows and
columns. Each column must have the same number of rows as all the other columns (although
you can use null to specify the absence of a value) and each column has type information that
must be consistent for every row in the collection. To Spark, DataFrames and Datasets represent
immutable, lazily evaluated plans that specify what operations to apply to data residing at a
location to generate some output. When we perform an action on a DataFrame, we instruct Spark
to perform the actual transformations and return the result. These represent plans of how to
manipulate rows and columns to compute the user’s desired result.

Schema-
A schema defines the column names and types of a DataFrame. You can define schemas
manually or read a schema from a data source (often called schema on read). Schemas consist of
types, meaning that you need a way of specifying what lies where.


Overview of Structured Spark Types-
Spark is effectively a programming language of its own. Internally, Spark uses an engine called
"Catalyst" that maintains its own type information through the planning and processing of work. In
doing so, this opens up a wide variety of execution optimizations that make significant
differences. Spark types map directly to the different language APIs that Spark maintains and
there exists a lookup table for each of these in Scala, Java, Python, SQL, and R. Even if we use
Spark’s Structured APIs from Python or R, the majority of our manipulations will operate strictly
on Spark types, not Python types. For example, the following code does not perform addition in
Scala or Python; it actually performs addition purely in Spark:

>>>> # in Python
>>>> df = spark.range(500).toDF("number")
>>>> df.select(df["number"] + 10)

This addition operation happens because Spark will convert an expression written in an input
language to Spark’s internal Catalyst representation of that same type information. It then will
operate on that internal representation.


DataFrames Versus Datasets-

In essence, within the Structured APIs, there are two more APIs, the “untyped” DataFrames and
the “typed” Datasets. To say that DataFrames are untyped is aslightly inaccurate; they have
types, but Spark maintains them completely and only checks whether those types line up to those
specified in the schema at runtime. Datasets, on the other hand, check whether types conform to
the specification at compile time. Datasets are only available to Java Virtual Machine (JVM)–
based languages (Scala and Java) and we specify types with case classes or Java beans.

To Spark (in Python or R), there is no such thing as a Dataset: everything is a
DataFrame and therefore we always operate on that optimized format.
When you’re using DataFrames, you’re taking advantage of Spark’s
optimized internal format. This format applies the same efficiency gains to all of Spark’s
language APIs.


Columns-

Columns represent a "simple" type like an integer or string, a "complex" type like an array or map, or
a "null" value. Spark tracks all of this type information for you and offers a variety of ways, with
which you can transform columns. For now, you can think about Spark Column types as columns in a table.

Rows-

A row is nothing more than a record of data. Each record in a DataFrame must be of type "Row", as
we can see when we collect the following DataFrames. We can create these rows manually from
SQL, from Resilient Distributed Datasets (RDDs), from data sources, or manually from scratch.

>>>> # in Python
>>>> spark.range(2).collect()		# This results in an array of "Row" objects.


Spark Types-

We mentioned earlier that Spark has a large number of internal type representations. We include
a handy reference table on the next several pages so that you can most easily reference what
type, in your specific language, lines up with the type in Spark.
Before getting to those tables, let’s talk about how we instantiate, or declare, a column to be of a
certain type.

Python types at times have certain requirements, which you can see listed below. To work with the
correct Python types, use the following:

>>>> from pyspark.sql.types import *
>>>> b = ByteType()


Overview of Structured API Execution-

This section will demonstrate how this code is actually executed across a cluster. This will help
you understand (and potentially debug) the process of writing and executing code on clusters, so
let’s walk through the execution of a single structured API query from user code to executed
code. Here’s an overview of the steps:
1. Write DataFrame/Dataset/SQL Code.
2. If valid code, Spark converts this to a "Logical Plan".
3. Spark transforms this "Logical Plan" to a "Physical Plan", checking for optimizations along
the way.
4. Spark then executes this "Physical Plan" (RDD manipulations) on the cluster.

To execute code, we must write code. This code is then submitted to Spark either through the
console or via a submitted job. This code then passes through the Catalyst Optimizer, which
decides how the code should be executed and lays out a plan for doing so before, finally, the
code is run and the result is returned to the user.

Rule. SQL/DataFrames/Datasets ----> Catalyst Optimizer -----> Physical Plan


A. Logical Planning- 
The first phase of execution is meant to take user code and convert it into a logical plan.

Rule. User Code --> Unresolved Logical Plan --(Analysis)--> Resolved Logical Plan --(Logical Optimization)--> Optimized Logical Plan
						  |
					          |
					       Catalog

This logical plan only represents a set of abstract transformations that do not refer to executors or
drivers, it’s purely to convert the user’s set of expressions into the most optimized version. It
does this by converting user code into an unresolved logical plan. This plan is unresolved
because although your code might be valid, the tables or columns that it refers to might or might
not exist. Spark uses the catalog, a repository of all table and DataFrame information, to resolve
columns and tables in the analyzer. The analyzer might reject the unresolved logical plan if the
required table or column name does not exist in the catalog. If the analyzer can resolve it, the
result is passed through the Catalyst Optimizer, a collection of rules that attempt to optimize the
logical plan by pushing down predicates or selections. Packages can extend the Catalyst to
include their own rules for domain-specific optimizations.


B. Physical Planning-
After successfully creating an optimized logical plan, Spark then begins the physical planning
process. 

Rule. Optimized Logical Plan --> Physical Plans --> Cost Model --> Best Physical Plan --> Executes on the cluster.

The physical plan, often called a Spark plan, specifies how the logical plan will execute
on the cluster by generating different physical execution strategies and comparing them through
a cost model. An example of the cost comparison might be choosing
how to perform a given join by looking at the physical attributes of a given table (how big the
table is or how big its partitions are).

Physical planning results in a series of RDDs and transformations. This result is why you might
have heard Spark referred to as a compiler—it takes queries in DataFrames, Datasets, and SQL
and compiles them into RDD transformations for you.


C. Execution-
Upon selecting a physical plan, Spark runs all of this code over RDDs, the lower-level
programming interface of Spark. Spark performs further
optimizations at runtime, generating native Java bytecode that can remove entire tasks or stages
during execution. Finally the result is returned to the user.



CHAPTER 5 - Basic Structured Operations


Definitionally, a DataFrame consists of a series of records (like rows in a table), that are of type
Row, and a number of columns (like columns in a spreadsheet) that represent a computation
expression that can be performed on each individual record in the Dataset. 
Schemas define the name as well as the type of data in each column. 
Partitioning of the DataFrame defines the layout of the DataFrame or Dataset’s physical distribution across the cluster. 
The partitioning scheme defines how that is allocated. You can set this to be based on values in a certain column
or nondeterministically.

Let’s create a DataFrame with which we can work:

>>>> # in Python
>>>> df = spark.read.format("json").load("/data/flight-data/json/2015-summary.json")

We discussed that a DataFame will have columns, and we use a schema to define them. Let’s
take a look at the schema on our current DataFrame:

>>>> df.printSchema()
Schemas tie everything together, so they’re worth belaboring.Let’s create a DataFrame with which we can work:

Schemas-

A schema defines the column names and types of a DataFrame. We can either let a data source
define the schema (called schema-on-read) or we can define it explicitly ourselves.

Let’s begin with a simple file, which we saw in Chapter 4, and let the semi-structured nature of
line-delimited JSON define the structure.

# in Python
>>>>spark.read.format("json").load("/data/flight-data/json/2015-summary.json").schema

Python returns the following:

StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),
StructField(ORIGIN_COUNTRY_NAME,StringType,true),
StructField(count,LongType,true)))

A schema is a StructType made up of a number of fields, StructFields, that have a name,
type, a Boolean flag which specifies whether that column can contain missing or null values,
and, finally, users can optionally specify associated metadata with that column. The metadata is a
way of storing information about this column.
Schemas can contain other StructTypes (Spark’s complex types).

The example that follows shows how to create and enforce a specific schema on a DataFrame-

>>>> from pyspark.sql.types import StructField, StructType, StringType, LongType
>>>> myManualSchema = StructType([
>>>> StructField("DEST_COUNTRY_NAME", StringType(), True),
>>>> StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
>>>> StructField("count", LongType(), False, metadata={"hello":"world"}) ])
>>>> df = spark.read.format("json").schema(myManualSchema).load("/data/flight-data/json/2015-summary.json")

As discussed in Chapter 4, we cannot simply set types via the per-language types because Spark
maintains its own type information.


Columns and Expressions-

Columns in Spark are similar to columns in a spreadsheet, R dataframe, or pandas DataFrame.
You can select, manipulate, and remove columns from DataFrames and these operations are
represented as expressions.

To Spark, columns are logical constructions that simply represent a value computed on a perrecord basis by means of an expression. 
This means that to have a real value for a column, we
need to have a row; and to have a row, we need to have a DataFrame. You cannot manipulate an
individual column outside the context of a DataFrame; you must use Spark transformations
within a DataFrame to modify the contents of a column.

Columns-

There are a lot of different ways to construct and refer to columns but the two simplest ways are
by using the "col" or "column" functions. To use either of these functions, you pass in a column
name:

>>>> # in Python
>>>> from pyspark.sql.functions import col, column
>>>> col("someColumnName")
>>>> column("someColumnName")

As mentioned, this column might or might not exist in our DataFrames. Columns are not "resolved" 
until we compare the column names with those we are maintaining in the catalog. Column and table 
resolution happens in the analyzer phase, as discussed in Chapter 4.

Explicit column references-
If you need to refer to a specific DataFrame’s column, you can use the col method on the
specific DataFrame. This can be useful when you are performing a join and need to refer to a
specific column in one DataFrame that might share a name with another column in the joined
DataFrame. As an added benefit, Spark does not need to resolve
this column itself (during the analyzer phase) because we did that for Spark:

>>>> df.col("count")


Expressions-

An expression is a set of transformations on one or more values in a record in a DataFrame. Think of it like a
function that takes as input one or more column names, resolves them, and then potentially
applies more expressions to create a single value for each record in the dataset. Importantly, this
“single value” can actually be a complex type like a Map or Array.

An expression is created via the "expr" function (just a DataFrame column
reference). In the simplest case, expr("someCol") is equivalent to col("someCol")

Columns as Expressions-
Columns provide a subset of expression functionality. If you use col() and want to perform
transformations on that column, you must perform those on that column reference. When using
an expression, the expr function can actually parse transformations and column references from
a string and can subsequently be passed into further transformations. Let’s look at some
examples.

expr("someCol - 5") is the same transformation as performing col("someCol") - 5, or even
expr("someCol") - 5. That’s because Spark compiles these to a logical tree specifying the
order of operations.

Remember,
1. Columns are just expressions.
2. Columns and transformations of those columns compile to the same logical plan as
parsed expressions.

Let’s ground this with an example:
(((col("someCol") + 5) * 200) - 6) < col("otherCol")

Logical tree-
					<
					/\
				       /  \
				      /    \
				      -    OtherCol
				     /\
  				    /  \
   				   /    \
				  *      6
                                 /\
				/  \
			       /    \
                              +     200
			     /\
                            /  \
                           /    \
			SomeCol  5

>>>> # in Python
>>>> from pyspark.sql.functions import expr
>>>> expr("(((someCol + 5) * 200) - 6) < otherCol")

You can write your expressions as DataFrame code or as SQL
expressions and get the exact same performance characteristics. That’s because this SQL
expression and the previous DataFrame code compile to the same underlying logical tree prior to
execution.

Accessing a DataFrame’s columns-
Sometimes, you’ll need to see a DataFrame’s columns, which you can do by using something
like printSchema; however, if you want to programmatically access columns, you can use the
columns property to see all columns on a DataFrame:

>>>> spark.read.format("json").load("/data/flight-data/json/2015-summary.json").columns


Records and Rows-

In Spark, each row in a DataFrame is a single record. Spark represents this record as an object of
type "Row". Spark manipulates "Row" objects using column expressions in order to produce usable
values. "Row" objects internally represent arrays of bytes. The byte array interface is never shown
to users because we only use column expressions to manipulate them.

Let’s see a row by calling "first" on our DataFrame:
>>>> df.first()

Creating Rows-
You can create rows by manually instantiating a "Row" object with the values that belong in each
column. It’s important to note that only DataFrames have schemas. Rows themselves do not have
schemas. This means that if you create a Row manually, you must specify the values in the same
order as the schema of the DataFrame to which they might be appended.

>>> # in Python
>>> from pyspark.sql import Row
>>> myRow = Row("Hello", None, 1, False)

Accessing data in rows is equally as easy: you just specify the position that you would like.

>>>> # in Python
>>>> myRow[0]
>>>> myRow[2]


Dataframe Transformations-

Now, we will move onto manipulating DataFrames. When working with individual DataFrames 
there are some fundamental objectives. These break down into several core operations-

We can add rows or columns
We can remove rows or columns
We can transform a row into a column (or vice versa)
We can change the order of rows based on the values in columns (Sorting)
