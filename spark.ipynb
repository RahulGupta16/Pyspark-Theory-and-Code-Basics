{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "095a6d6b-db64-4cad-a733-dc8248bc5197",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 156 ms\n",
      "Wall time: 5.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "#os.environ['JAVA_HOME'] = 'C:\\Progr~2\\Java\\jre-1.8'\n",
    "#os.environ['SPARK_HOME'] = 'C:\\Spark\\spark-3.4.0-bin-hadoop3'\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"sync_task\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8cee298-c7aa-4159-89da-9a4989220c65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.40:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>sync_task</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2337fc8fcd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6342a64c-2f5f-4ab2-95b8-51f3cceafc2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## to read a file in any format (\"\".csv\"  here), use below method-\n",
    "\n",
    "first_table = spark.read.format(\"csv\").load(r\"C:\\Users\\Rahul\\Desktop\\Project\\train.csv\", header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f411eb8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bbfe078",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Convert range into DataFrame with 'x' column name using \"toDF(x)\" method. \n",
    "\n",
    "## Eg.\n",
    "myRange = spark.range(1000).toDF(\"number\")\n",
    "\n",
    "myRange.show(10)  # use .show(x) to print top x records of \"myRange\" dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b03db787",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [id#127L AS number#129L]\n",
      "+- *(1) Range (0, 1000, step=1, splits=8)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myRange.explain()       ## \"explain\" method is used to see the DataFrame’s lineage (or how Spark will execute this query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77f9452f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To make any DataFrame into a table or view, call this: \n",
    "\n",
    "\n",
    "first_table.createOrReplaceTempView(\"Table_DF\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2513b43e",
   "metadata": {},
   "source": [
    "Remember, Spark can run the same transformations, regardless of the language, in the exact same way. You can express your\n",
    "business logic in SQL or DataFrames. Spark will compile that logic down to an underlying plan (that you can see in the explain plan) before\n",
    "actually executing your code. With Spark SQL, you can register any DataFrame as a table or view (a temporary table) and query it using pure SQL. There is no performance difference between writing SQL queries or writing DataFrame code, they both “compile” to the same underlying plan that we specify in DataFrame code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa241ee5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------+--------------------+\n",
      "|Cover_Type|max(Elevation(meters))|min(Aspect(Degrees))|\n",
      "+----------+----------------------+--------------------+\n",
      "|         1|                  3844|                   0|\n",
      "|         2|                  3001|                   0|\n",
      "|         3|                  2875|                   0|\n",
      "|         4|                  2425|                 101|\n",
      "|         5|                  3426|                   0|\n",
      "|         6|                  2885|                   0|\n",
      "|         7|                  3677|                   0|\n",
      "+----------+----------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Method 1 - Using Spark SQL\n",
    "sqlWay = spark.sql(\"\"\"\n",
    "SELECT Cover_Type, max(`Elevation(meters)`), min(`Aspect(Degrees)`)\n",
    "FROM Table_DF\n",
    "GROUP BY Cover_Type\n",
    "\"\"\")\n",
    "\n",
    "sqlWay.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4e88089",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortAggregate(key=[Cover_Type#71], functions=[max(Elevation(meters)#17), min(Aspect(Degrees)#18)])\n",
      "   +- Sort [Cover_Type#71 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(Cover_Type#71, 200), ENSURE_REQUIREMENTS, [plan_id=109]\n",
      "         +- SortAggregate(key=[Cover_Type#71], functions=[partial_max(Elevation(meters)#17), partial_min(Aspect(Degrees)#18)])\n",
      "            +- Sort [Cover_Type#71 ASC NULLS FIRST], false, 0\n",
      "               +- FileScan csv [Elevation(meters)#17,Aspect(degrees)#18,Cover_Type#71] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/Rahul/Desktop/Project/train.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Elevation(meters):string,Aspect(degrees):string,Cover_Type:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlWay.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb8bf8bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[Cover_Type#71], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(Cover_Type#71, 200), ENSURE_REQUIREMENTS, [plan_id=123]\n",
      "      +- HashAggregate(keys=[Cover_Type#71], functions=[partial_count(1)])\n",
      "         +- FileScan csv [Cover_Type#71] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/Rahul/Desktop/Project/train.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Cover_Type:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Method 2 - Using spark DataFrame\n",
    "\n",
    "### NOT ABLE TO RUN FOR NOW, WILL TAKE CARE LATER\n",
    "\n",
    "dataFrameWay = first_table\\\n",
    ".groupBy(\"Cover_Type\")\\\n",
    ".count()\n",
    "dataFrameWay.explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "420b838c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(Elevation(meters))='3844')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"take(x)\" can be used to extract \"xth\" value from the dataframe\n",
    "\n",
    "## Using spark SQL\n",
    "spark.sql(\"SELECT max(`Elevation(meters)`) from Table_DF\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf0832eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(Elevation(meters))='3844')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using spark dataframe\n",
    "\n",
    "from pyspark.sql.functions import max       ## importing \"max\" function\n",
    "\n",
    "\n",
    "first_table.select(max(\"Elevation(meters)\")).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad5f9bbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Reading one more csv file for understanding\n",
    "\n",
    "data_table = spark.read.format(\"csv\").load(r\"C:\\Users\\Rahul\\Desktop\\Project\\Final_Train.csv\", header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2393792e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "|       Qualification|         Experience|Rating|               Place|         Profile|  Miscellaneous_Info|Fees|\n",
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "|BHMS, MD - Homeop...|24 years experience|  100%| Kakkanad, Ernakulam|       Homeopath|100% 16 Feedback ...| 100|\n",
      "|BAMS, MD - Ayurve...|12 years experience|   98%|Whitefield, Banga...|        Ayurveda|98% 76 Feedback W...| 350|\n",
      "|MBBS, MS - Otorhi...| 9 years experience|  null|Mathikere - BEL, ...|  ENT Specialist|                null| 300|\n",
      "| BSc - Zoology, BAMS|12 years experience|  null|Bannerghatta Road...|        Ayurveda|Bannerghatta Road...| 250|\n",
      "|                BAMS|20 years experience|  100%|Keelkattalai, Che...|        Ayurveda|100% 4 Feedback K...| 250|\n",
      "|                BAMS| 8 years experience|  null|      Porur, Chennai|        Ayurveda|                null| 100|\n",
      "|                BHMS|42 years experience|  null|   Karol Bagh, Delhi|       Homeopath|                null| 200|\n",
      "|                 BDS|10 years experience|   99%|  Arekere, Bangalore|         Dentist|Dental Fillings C...| 200|\n",
      "|MBBS, MD - Genera...|14 years experience|  null| Old City, Hyderabad|General Medicine|                null| 100|\n",
      "|            BSc, BDS|23 years experience|  null|   Athani, Ernakulam|         Dentist|                null| 100|\n",
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.show(10)\n",
    "\n",
    "data_table.createOrReplaceTempView(\"Table_new\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5787b55e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+\n",
      "|         Profile|Total_Fees|\n",
      "+----------------+----------+\n",
      "|  Dermatologists|   10350.0|\n",
      "|         Dentist|    9650.0|\n",
      "|General Medicine|    8200.0|\n",
      "|  ENT Specialist|    7050.0|\n",
      "|       Homeopath|    5100.0|\n",
      "+----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using Spark SQL\n",
    "maxSql = spark.sql(\"\"\"\n",
    "SELECT Profile, sum(Fees) as Total_Fees\n",
    "FROM Table_new\n",
    "GROUP BY Profile\n",
    "ORDER BY sum(Fees) DESC\n",
    "LIMIT 5 \"\"\")\n",
    "\n",
    "maxSql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f74ac14-2c1e-410d-ad1e-062e0ecb454c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------------+\n",
      "|         Profile|sum(CAST(Fees AS INT))|\n",
      "+----------------+----------------------+\n",
      "|  Dermatologists|                 10350|\n",
      "|        Ayurveda|                  4100|\n",
      "|       Homeopath|                  5100|\n",
      "|  ENT Specialist|                  7050|\n",
      "|General Medicine|                  8200|\n",
      "|         Dentist|                  9650|\n",
      "+----------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using spark DataFrame\n",
    "\n",
    "from pyspark.sql.functions import desc, col, sum\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "data_table.groupby('Profile').agg(sum(col(\"Fees\").cast(IntegerType()))).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e985425-1b31-4141-840a-8cd2b67cef7d",
   "metadata": {},
   "source": [
    "DataFrames and Datasets are (distributed) table-like collections with well-defined rows and columns. Each column must have the same number of rows as all the other columns (although you can use null to specify the absence of a value) and each column has type information that must be consistent for every row in the collection. To Spark, DataFrames and Datasets represent immutable, lazily evaluated plans that specify what operations to apply to data residing at a\n",
    "location to generate some output. When we perform an action on a DataFrame, we instruct Spark to perform the actual transformations and return the result. These represent plans of how to manipulate rows and columns to compute the user’s desired result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc17c5c6-3fad-4602-980a-35d2d014d9a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+---------+---------+\n",
      "|         Experience|         avg(Fees)|min(Fees)|max(Fees)|\n",
      "+-------------------+------------------+---------+---------+\n",
      "| 0 years experience|             100.0|      100|      100|\n",
      "|10 years experience|254.54545454545453|      100|      800|\n",
      "|11 years experience|342.85714285714283|      100|      500|\n",
      "|12 years experience| 376.6666666666667|      100|      800|\n",
      "|13 years experience|428.57142857142856|      100|      750|\n",
      "+-------------------+------------------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_exp_fee = spark.sql(\"\"\"\n",
    "\n",
    "select Experience, avg(Fees), min(Fees), max(Fees)\n",
    "from Table_new\n",
    "group by Experience\n",
    "order by Experience\n",
    "\"\"\")\n",
    "\n",
    "avg_exp_fee.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12cb8d00-ad71-45c6-9585-eb03d2098acd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experience</th>\n",
       "      <th>avg(Fees)</th>\n",
       "      <th>min(Fees)</th>\n",
       "      <th>max(Fees)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0 years experience</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10 years experience</td>\n",
       "      <td>254.545455</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11 years experience</td>\n",
       "      <td>342.857143</td>\n",
       "      <td>100</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12 years experience</td>\n",
       "      <td>376.666667</td>\n",
       "      <td>100</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13 years experience</td>\n",
       "      <td>428.571429</td>\n",
       "      <td>100</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Experience   avg(Fees) min(Fees) max(Fees)\n",
       "0   0 years experience  100.000000       100       100\n",
       "1  10 years experience  254.545455       100       800\n",
       "2  11 years experience  342.857143       100       500\n",
       "3  12 years experience  376.666667       100       800\n",
       "4  13 years experience  428.571429       100       750"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## to convert pyspark dataframes into pandas dataframes, use \".toPandas()\" method as used below-\n",
    "\n",
    "x = avg_exp_fee.toPandas()\n",
    "\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ba4b24-ba94-4b44-abb1-b96a7f6435f9",
   "metadata": {},
   "source": [
    "A schema defines the column names and types of a DataFrame. You can define schemas manually or read a schema from a data source (often called schema on read). Schemas consist of types, meaning that you need a way of specifying what lies where."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51a70b41-36de-426e-a222-e43a930275e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Below code actually performs addition purely in Spark:\n",
    "\n",
    "df_new = spark.range(500).toDF(\"number\")\n",
    "#df_new.select(df_new[\"number\"] + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1015841-26d8-4288-804f-9cb454906ca7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|(number + 25)|\n",
      "+-------------+\n",
      "|           25|\n",
      "|           26|\n",
      "|           27|\n",
      "|           28|\n",
      "|           29|\n",
      "|           30|\n",
      "|           31|\n",
      "|           32|\n",
      "|           33|\n",
      "|           34|\n",
      "|           35|\n",
      "|           36|\n",
      "|           37|\n",
      "|           38|\n",
      "|           39|\n",
      "|           40|\n",
      "|           41|\n",
      "|           42|\n",
      "|           43|\n",
      "|           44|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.select(df_new[\"number\"]+25).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eec5295-3a21-4856-a393-447abd243285",
   "metadata": {
    "tags": []
   },
   "source": [
    "Spark Types - Spark has a large number of internal type representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ae083bb-74bd-44e5-bd65-ad79006c7486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use below method to import spark types- \n",
    "\n",
    "from pyspark.sql.types import *\n",
    "#b = ByteType()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90fd6c33-351a-4d54-b3b7-155a820e6dc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "|       Qualification|         Experience|Rating|               Place|         Profile|  Miscellaneous_Info|Fees|\n",
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "|BHMS, MD - Homeop...|24 years experience|  100%| Kakkanad, Ernakulam|       Homeopath|100% 16 Feedback ...| 100|\n",
      "|BAMS, MD - Ayurve...|12 years experience|   98%|Whitefield, Banga...|        Ayurveda|98% 76 Feedback W...| 350|\n",
      "|MBBS, MS - Otorhi...| 9 years experience|  null|Mathikere - BEL, ...|  ENT Specialist|                null| 300|\n",
      "| BSc - Zoology, BAMS|12 years experience|  null|Bannerghatta Road...|        Ayurveda|Bannerghatta Road...| 250|\n",
      "|                BAMS|20 years experience|  100%|Keelkattalai, Che...|        Ayurveda|100% 4 Feedback K...| 250|\n",
      "|                BAMS| 8 years experience|  null|      Porur, Chennai|        Ayurveda|                null| 100|\n",
      "|                BHMS|42 years experience|  null|   Karol Bagh, Delhi|       Homeopath|                null| 200|\n",
      "|                 BDS|10 years experience|   99%|  Arekere, Bangalore|         Dentist|Dental Fillings C...| 200|\n",
      "|MBBS, MD - Genera...|14 years experience|  null| Old City, Hyderabad|General Medicine|                null| 100|\n",
      "|            BSc, BDS|23 years experience|  null|   Athani, Ernakulam|         Dentist|                null| 100|\n",
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3d93955-d5fb-4321-a2a1-b5caea5f9b1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Qualification: string (nullable = true)\n",
      " |-- Experience: string (nullable = true)\n",
      " |-- Rating: string (nullable = true)\n",
      " |-- Place: string (nullable = true)\n",
      " |-- Profile: string (nullable = true)\n",
      " |-- Miscellaneous_Info: string (nullable = true)\n",
      " |-- Fees: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.printSchema()    # print all schemas using \"printSchema()\" method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bac66c5-c1df-466c-8729-17a4112196b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('Qualification', StringType(), True), StructField('Experience', StringType(), True), StructField('Rating', StringType(), True), StructField('Place', StringType(), True), StructField('Profile', StringType(), True), StructField('Miscellaneous_Info', StringType(), True), StructField('Fees', StringType(), True)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table.schema       # print schema info using \"schema\" method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234f7143-a3eb-4b86-9a69-254f17ade67f",
   "metadata": {},
   "source": [
    "A schema is a StructType made up of a number of fields, StructFields, that have a name,\n",
    "type, a Boolean flag which specifies whether that column can contain missing or null values,\n",
    "and, finally, users can optionally specify associated metadata with that column. The metadata is a\n",
    "way of storing information about this column.\n",
    "Schemas can contain other StructTypes (Spark’s complex types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90e7acd6-1ca6-49e7-ab93-49bc87325535",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+-----+-------+------------------+----+\n",
      "|Qualification|Experience|Rating|Place|Profile|Miscellaneous_Info|Fees|\n",
      "+-------------+----------+------+-----+-------+------------------+----+\n",
      "|         null|      null|  null| null|   null|              null|null|\n",
      "|         null|      null|  null| null|   null|              null|null|\n",
      "|         null|      null|  null| null|   null|              null|null|\n",
      "|         null|      null|  null| null|   null|              null|null|\n",
      "|         null|      null|  null| null|   null|              null|null|\n",
      "+-------------+----------+------+-----+-------+------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The example that follows shows how to create and enforce a specific schema on a DataFrame-\n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "myManualSchema = StructType([\n",
    "StructField(\"Qualification\", StringType(), True),\n",
    "StructField(\"Experience\", StringType(), True),\n",
    "StructField(\"Rating\", StringType(), True),\n",
    "StructField(\"Place\", StringType(), True),\n",
    "StructField(\"Profile\", StringType(), True),\n",
    "StructField(\"Miscellaneous_Info\", StringType(), True),\n",
    "\n",
    "StructField(\"Fees\", LongType(), True, metadata={\"hello\":\"world\"}) ])\n",
    "\n",
    "new_df = spark.read.format(\"json\").schema(myManualSchema).load(r\"C:\\Users\\Rahul\\Desktop\\Project\\Final_Train.csv\", header = True)\n",
    "new_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c8af34-75ab-462b-82b0-1584e5375b34",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Columns-\n",
    "\n",
    "There are a lot of different ways to construct and refer to columns but the two simplest ways are\n",
    "by using the \"col\" or \"column\" functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43db3677-fcc8-48ef-be97-32f59f8289a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Experience'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, column, concat\n",
    "col(\"Experience\")\n",
    "column(\"Experience\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e79ec3-16c0-4938-981a-2af89c507220",
   "metadata": {},
   "source": [
    "If you need to refer to a specific DataFrame’s column, you can use the \"col\" method on the\n",
    "specific DataFrame. This can be useful when you are performing a join and need to refer to a\n",
    "specific column in one DataFrame that might share a name with another column in the joined\n",
    "DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99db7504-20b1-43d5-a011-d09fc9b48a1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------+------+----+\n",
      "|         Experience|         Profile|Rating|Fees|\n",
      "+-------------------+----------------+------+----+\n",
      "|24 years experience|       Homeopath|  100%| 100|\n",
      "|12 years experience|        Ayurveda|   98%| 350|\n",
      "| 9 years experience|  ENT Specialist|  null| 300|\n",
      "|12 years experience|        Ayurveda|  null| 250|\n",
      "|20 years experience|        Ayurveda|  100%| 250|\n",
      "| 8 years experience|        Ayurveda|  null| 100|\n",
      "|42 years experience|       Homeopath|  null| 200|\n",
      "|10 years experience|         Dentist|   99%| 200|\n",
      "|14 years experience|General Medicine|  null| 100|\n",
      "|23 years experience|         Dentist|  null| 100|\n",
      "| 5 years experience|  ENT Specialist|  null| 700|\n",
      "| 7 years experience|        Ayurveda|  null| 100|\n",
      "| 9 years experience|         Dentist|   98%| 200|\n",
      "|21 years experience|         Dentist|  null| 350|\n",
      "|12 years experience|  ENT Specialist|  null| 500|\n",
      "+-------------------+----------------+------+----+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Below is the way by which you can look inside any column of a spark dataframe\n",
    "\n",
    "#data_table.select(col(\"Fees\")).show(15)\n",
    "#data_table.select(col(\"Experience\"),col(\"Fees\")).show(15)\n",
    "data_table.select(col(\"Experience\"), col(\"Profile\"),col(\"Rating\"),col(\"Fees\")).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd073781-9017-4e60-a82d-298d89d6b9e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Expressions-\n",
    "\n",
    "An expression is a set of transformations on one or more values in a record in a DataFrame. Think of it like a\n",
    "function that takes as input one or more column names, resolves them, and then potentially\n",
    "applies more expressions to create a single value for each record in the dataset. Importantly, this\n",
    "“single value” can actually be a complex type like a Map or Array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8db51a2-b393-4fae-9b26-d937b70dfd78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+---------------------------------+----+\n",
      "|(((Fees + 5) * 200) - 6)|((((Fees + 5) * 200) - 6) < Fees)|Fees|\n",
      "+------------------------+---------------------------------+----+\n",
      "|                 20994.0|                            false| 100|\n",
      "|                 70994.0|                            false| 350|\n",
      "|                 60994.0|                            false| 300|\n",
      "|                 50994.0|                            false| 250|\n",
      "|                 50994.0|                            false| 250|\n",
      "|                 20994.0|                            false| 100|\n",
      "|                 40994.0|                            false| 200|\n",
      "|                 40994.0|                            false| 200|\n",
      "|                 20994.0|                            false| 100|\n",
      "|                 20994.0|                            false| 100|\n",
      "|                140994.0|                            false| 700|\n",
      "|                 20994.0|                            false| 100|\n",
      "|                 40994.0|                            false| 200|\n",
      "|                 70994.0|                            false| 350|\n",
      "|                100994.0|                            false| 500|\n",
      "|                 40994.0|                            false| 200|\n",
      "|                 20994.0|                            false| 100|\n",
      "|                 60994.0|                            false| 300|\n",
      "|                 80994.0|                            false| 400|\n",
      "|                 30994.0|                            false| 150|\n",
      "+------------------------+---------------------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "#data_table.select(expr(\"(((Fees + 5) * 200) - 6)\"), \"Fees\").show()\n",
    "data_table.select(expr(\"(((Fees + 5) * 200) - 6)\"),expr(\"(((Fees + 5) * 200) - 6) < Fees\") , \"Fees\").show()\n",
    "\n",
    "#data_table.select(expr(\"(((Fees + 5) * 200) - 6) > Fees\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "779e571d-da70-4a5b-a3c2-739d0a2a49c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Qualification='BHMS, MD - Homeopathy', Experience='24 years experience', Rating='100%', Place='Kakkanad, Ernakulam', Profile='Homeopath', Miscellaneous_Info='100% 16 Feedback Kakkanad, Ernakulam', Fees='100')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table.first()   # print first row of a spark dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f54b11-b9eb-4175-87af-c7b3057c2f8f",
   "metadata": {},
   "source": [
    "# Creating Rows-\n",
    "You can create rows by manually instantiating a \"Row\" object with the values that belong in each\n",
    "column. It’s important to note that only DataFrames have schemas. Rows themselves do not have\n",
    "schemas. This means that if you create a Row manually, you must specify the values in the same\n",
    "order as the schema of the DataFrame to which they might be appended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d62fe5e5-08f5-48b5-819a-9987b7822813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Creating rows for spark dataframes\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "myRow = Row(\"MBBS\",\"105 years experience\",\"90%\", \"Charbagh, Lucknow\",\"Ayurvedic\", \"Rest in Piece GUCCI MANE\",\"AB3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "574544bb-52c3-412d-a0a1-7d536c7132a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "myManualSchema = StructType([\n",
    "StructField(\"Qualification\", StringType(), True),\n",
    "StructField(\"Experience\", StringType(), True),\n",
    "StructField(\"Rating\", StringType(), True),\n",
    "StructField(\"Place\", StringType(), True),\n",
    "StructField(\"Profile\", StringType(), True),\n",
    "StructField(\"Miscellaneous_Info\", StringType(), True),\n",
    "StructField(\"Fees\", StringType(), True)])\n",
    "\n",
    "\n",
    "#my_df = spark.createDataFrame([myRow],myManualSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99c64e0d-aeb4-44a4-aa79-739646a64bb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can not infer schema for type: <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmyRow\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:894\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m    890\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m    891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m    892\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[0;32m    893\u001b[0m     )\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    896\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:936\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    934\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[0;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 936\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    938\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:631\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[1;34m(self, data, schema)\u001b[0m\n\u001b[0;32m    628\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m--> 631\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchemaFromList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[0;32m    633\u001b[0m     tupled_data: Iterable[Tuple] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(converter, data)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:517\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[1;34m(self, data, names)\u001b[0m\n\u001b[0;32m    515\u001b[0m infer_dict_as_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39minferDictAsStruct()\n\u001b[0;32m    516\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[1;32m--> 517\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_merge_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome of types cannot be determined after inferring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:519\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    515\u001b[0m infer_dict_as_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39minferDictAsStruct()\n\u001b[0;32m    516\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[0;32m    517\u001b[0m schema \u001b[38;5;241m=\u001b[39m reduce(\n\u001b[0;32m    518\u001b[0m     _merge_type,\n\u001b[1;32m--> 519\u001b[0m     (\u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data),\n\u001b[0;32m    520\u001b[0m )\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome of types cannot be determined after inferring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\types.py:1302\u001b[0m, in \u001b[0;36m_infer_schema\u001b[1;34m(row, names, infer_dict_as_struct, prefer_timestamp_ntz)\u001b[0m\n\u001b[0;32m   1299\u001b[0m     items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(row\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not infer schema for type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(row))\n\u001b[0;32m   1304\u001b[0m fields \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1305\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m items:\n",
      "\u001b[1;31mTypeError\u001b[0m: Can not infer schema for type: <class 'str'>"
     ]
    }
   ],
   "source": [
    "x = spark.createDataFrame(myRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f75d30fc-a8ff-46fe-bebd-fffb0db3c9bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkSession' object has no attribute 'sqlContext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 9\u001b[0m\n\u001b[0;32m      3\u001b[0m myManualSchema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[0;32m      4\u001b[0m StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msome\u001b[39m\u001b[38;5;124m\"\u001b[39m, LongType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m      5\u001b[0m StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, LongType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m      6\u001b[0m StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, LongType(), \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m ])\n\u001b[0;32m      8\u001b[0m myRow \u001b[38;5;241m=\u001b[39m (Row(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m----> 9\u001b[0m myDf \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqlContext\u001b[49m\u001b[38;5;241m.\u001b[39mcreateDataFrame(myRow, myManualSchema)\n\u001b[0;32m     10\u001b[0m myDf\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SparkSession' object has no attribute 'sqlContext'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "myManualSchema = StructType([\n",
    "StructField(\"some\", LongType(), True),\n",
    "StructField(\"col\", LongType(), True),\n",
    "StructField(\"names\", LongType(), False)\n",
    "])\n",
    "myRow = (Row(1, 1, 1))\n",
    "myDf = spark.sqlContext.createDataFrame(myRow, myManualSchema)\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb91ca79-9cbe-4c0c-bdda-4ad569a14eca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "|       Qualification|         Experience|Rating|               Place|         Profile|  Miscellaneous_Info|Fees|\n",
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "|BHMS, MD - Homeop...|24 years experience|  100%| Kakkanad, Ernakulam|       Homeopath|100% 16 Feedback ...| 100|\n",
      "|BAMS, MD - Ayurve...|12 years experience|   98%|Whitefield, Banga...|        Ayurveda|98% 76 Feedback W...| 350|\n",
      "|MBBS, MS - Otorhi...| 9 years experience|  null|Mathikere - BEL, ...|  ENT Specialist|                null| 300|\n",
      "| BSc - Zoology, BAMS|12 years experience|  null|Bannerghatta Road...|        Ayurveda|Bannerghatta Road...| 250|\n",
      "|                BAMS|20 years experience|  100%|Keelkattalai, Che...|        Ayurveda|100% 4 Feedback K...| 250|\n",
      "|                BAMS| 8 years experience|  null|      Porur, Chennai|        Ayurveda|                null| 100|\n",
      "|                BHMS|42 years experience|  null|   Karol Bagh, Delhi|       Homeopath|                null| 200|\n",
      "|                 BDS|10 years experience|   99%|  Arekere, Bangalore|         Dentist|Dental Fillings C...| 200|\n",
      "|MBBS, MD - Genera...|14 years experience|  null| Old City, Hyderabad|General Medicine|                null| 100|\n",
      "|            BSc, BDS|23 years experience|  null|   Athani, Ernakulam|         Dentist|                null| 100|\n",
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37a7fa1d-eed8-4de8-9def-d92d5d21a4db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|Rating|Fees|\n",
      "+------+----+\n",
      "|  100%| 100|\n",
      "|   98%| 350|\n",
      "|  null| 300|\n",
      "|  null| 250|\n",
      "|  100%| 250|\n",
      "|  null| 100|\n",
      "|  null| 200|\n",
      "|   99%| 200|\n",
      "|  null| 100|\n",
      "|  null| 100|\n",
      "+------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.select('Rating', 'Fees').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68373ceb-93a9-433b-89c1-54c0022151cd",
   "metadata": {},
   "source": [
    "You can refer to columns in a number of different ways; all you need to keep in mind is that you can use them interchangeably:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "063032c5-9ab3-41c3-b67d-c560d8e5c0a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+\n",
      "|Rating|Rating|Rating|Rating|\n",
      "+------+------+------+------+\n",
      "|  100%|  100%|  100%|  100%|\n",
      "|   98%|   98%|   98%|   98%|\n",
      "+------+------+------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Four ways to refer columns of a spark dataframe\n",
    "\n",
    "from pyspark.sql.functions import expr, col, column\n",
    "data_table.select(\n",
    "expr(\"Rating\"),    # Method 1: using \"expr()\" method\n",
    "    \n",
    "col(\"Rating\"),    # Method 2: Using \"col()\" method\n",
    "    \n",
    "column(\"Rating\"),  # Method 3: Using \"Column()\" method\n",
    "    \n",
    "   \"Rating\").show(2)    # Method 4: Generic method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0284925e-ee9e-473e-834d-c22963ed8c30",
   "metadata": {
    "tags": []
   },
   "source": [
    "\"expr\" is the most flexible reference that we can use. It can refer to a plain\n",
    "column or a string manipulation of a column. To illustrate, let’s change the column name, and\n",
    "then change it back by using the AS keyword and then the alias method on the column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a17b4d4-a1d7-4bfa-9e53-b676a167c611",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|       Expertize|\n",
      "+----------------+\n",
      "|       Homeopath|\n",
      "|        Ayurveda|\n",
      "|  ENT Specialist|\n",
      "|        Ayurveda|\n",
      "|        Ayurveda|\n",
      "|        Ayurveda|\n",
      "|       Homeopath|\n",
      "|         Dentist|\n",
      "|General Medicine|\n",
      "|         Dentist|\n",
      "+----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------------+\n",
      "|       Expertize|\n",
      "+----------------+\n",
      "|       Homeopath|\n",
      "|        Ayurveda|\n",
      "|  ENT Specialist|\n",
      "|        Ayurveda|\n",
      "|        Ayurveda|\n",
      "|        Ayurveda|\n",
      "|       Homeopath|\n",
      "|         Dentist|\n",
      "|General Medicine|\n",
      "|         Dentist|\n",
      "+----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------------+\n",
      "|  Medical Domain|\n",
      "+----------------+\n",
      "|       Homeopath|\n",
      "|        Ayurveda|\n",
      "|  ENT Specialist|\n",
      "|        Ayurveda|\n",
      "|        Ayurveda|\n",
      "|        Ayurveda|\n",
      "|       Homeopath|\n",
      "|         Dentist|\n",
      "|General Medicine|\n",
      "|         Dentist|\n",
      "+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renaming a column name-\n",
    "\n",
    "# Method 1 -\n",
    "data_table.select(expr(\"Profile AS Expertize\")).show(10)\n",
    "\n",
    "# Method 2 -\n",
    "data_table.select(expr(\"Profile\").alias(\"Expertize\")).show(10)\n",
    "\n",
    "# Combining both methods together-\n",
    "data_table.select(expr(\"Profile AS Expertize\").alias(\"Medical Domain\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4376d22f-f176-4cb5-a965-ef0f376f136c",
   "metadata": {},
   "source": [
    "Because \"select\" followed by a series of \"expr\" is such a common pattern, Spark has a shorthand\n",
    "for doing this efficiently: \"selectExpr\". This is probably the most convenient interface for\n",
    "everyday use-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2845c490-6cec-4cb8-b20f-d6bcfe8b02d3",
   "metadata": {},
   "source": [
    "Here’s a simple example that adds a new column \"withinCountry\" to our DataFrame that specifies whether the destination and origin are the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "388b534e-adf0-4737-85a7-45c7b2a15102",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+---------+--------------------+----+----------------+\n",
      "|       Qualification|         Experience|Rating|               Place|  Profile|  Miscellaneous_Info|Fees|Compared_Columns|\n",
      "+--------------------+-------------------+------+--------------------+---------+--------------------+----+----------------+\n",
      "|BHMS, MD - Homeop...|24 years experience|  100%| Kakkanad, Ernakulam|Homeopath|100% 16 Feedback ...| 100|            true|\n",
      "|BAMS, MD - Ayurve...|12 years experience|   98%|Whitefield, Banga...| Ayurveda|98% 76 Feedback W...| 350|            true|\n",
      "+--------------------+-------------------+------+--------------------+---------+--------------------+----+----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.selectExpr(\n",
    "\"*\", # all original columns\n",
    "\"(Qualification <> Profile) as Compared_Columns\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8fdc2d79-ecc2-4280-8d81-e5d9f1eda396",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+---------------+\n",
      "|Hillshade_3pm|Slope(degrees)|Aspect(degrees)|\n",
      "+-------------+--------------+---------------+\n",
      "|          158|            14|            186|\n",
      "|          198|            15|            243|\n",
      "|          142|            12|            162|\n",
      "|          164|            17|            345|\n",
      "|          151|             9|              4|\n",
      "|          241|            33|            270|\n",
      "|           92|            25|             34|\n",
      "|          151|             3|            153|\n",
      "|           28|            32|             92|\n",
      "|          205|            26|            226|\n",
      "+-------------+--------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_table.select('Hillshade_3pm','Slope(degrees)','Aspect(degrees)').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f43f3ef2-0210-4483-a00a-33ee5552704f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+------------------+--------------------+----------------------+------------------+--------------------+\n",
      "|    AVG_Hillshade|max(Hillshade_3pm)|min(Hillshade_3pm)|distinct_count_slope|count(Aspect(degrees))|sum(Hillshade_3pm)|max(Aspect(degrees))|\n",
      "+-----------------+------------------+------------------+--------------------+----------------------+------------------+--------------------+\n",
      "|142.8546299483649|                99|                 0|                  52|                 29050|         4149927.0|                  99|\n",
      "+-----------------+------------------+------------------+--------------------+----------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_table.selectExpr(\"avg(Hillshade_3pm) AS AVG_Hillshade\", \"max(Hillshade_3pm)\",\"min(Hillshade_3pm)\",\\\n",
    "              \"count(distinct(`Slope(degrees)`)) AS distinct_count_slope\", \"count(`Aspect(degrees)`)\", \"sum(Hillshade_3pm)\",\\\n",
    "             \"max(`Aspect(degrees)`)\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e111647-e2cc-4edc-8fb8-2ec383e6e6f2",
   "metadata": {},
   "source": [
    "# Converting to Spark Types (Literals)-\n",
    "Sometimes, we need to pass explicit values into Spark that are just a value (rather than a new\n",
    "column). This might be a constant value or something we’ll need to compare to later on. The\n",
    "way we do this is through literals. This is basically a translation from a given programming\n",
    "language’s literal value to one that Spark understands. Literals are expressions and you can use\n",
    "them in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bc5f4cbd-186d-4af5-81a6-a053aea64e85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------+--------------+----------------------------------------+--------------------------------------+---------------------------------------+-------------+--------------+-------------+------------------------------------------+-----------------+-----------------+-----------------+-----------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+----------+-------------+---+\n",
      "|Elevation(meters)|Aspect(degrees)|Slope(degrees)|Horizontal_Distance_To_Hydrology(meters)|Vertical_Distance_To_Hydrology(meters)|Horizontal_Distance_To_Roadways(meters)|Hillshade_9am|Hillshade_Noon|Hillshade_3pm|Horizontal_Distance_To_Fire_Points(meters)|Wilderness_Area_1|Wilderness_Area_2|Wilderness_Area_3|Wilderness_Area_4|Soil_Type_1|Soil_Type_2|Soil_Type_3|Soil_Type_4|Soil_Type_5|Soil_Type_6|Soil_Type_7|Soil_Type_8|Soil_Type_9|Soil_Type_10|Soil_Type_11|Soil_Type_12|Soil_Type_13|Soil_Type_14|Soil_Type_15|Soil_Type_16|Soil_Type_17|Soil_Type_18|Soil_Type_19|Soil_Type_20|Soil_Type_21|Soil_Type_22|Soil_Type_23|Soil_Type_24|Soil_Type_25|Soil_Type_26|Soil_Type_27|Soil_Type_28|Soil_Type_29|Soil_Type_30|Soil_Type_31|Soil_Type_32|Soil_Type_33|Soil_Type_34|Soil_Type_35|Soil_Type_36|Soil_Type_37|Soil_Type_38|Soil_Type_39|Soil_Type_40|Cover_Type|Hillshade_3pm|One|\n",
      "+-----------------+---------------+--------------+----------------------------------------+--------------------------------------+---------------------------------------+-------------+--------------+-------------+------------------------------------------+-----------------+-----------------+-----------------+-----------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+----------+-------------+---+\n",
      "|             2982|            186|            14|                                     323|                                    66|                                   5351|          222|           249|          158|                                      1530|                1|                0|                0|                0|          0|          0|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           1|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|         5|          158|  1|\n",
      "|             2929|            243|            15|                                     335|                                    33|                                   5778|          189|           251|          198|                                      6429|                1|                0|                0|                0|          0|          0|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           1|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|         7|          198|  1|\n",
      "+-----------------+---------------+--------------+----------------------------------------+--------------------------------------+---------------------------------------+-------------+--------------+-------------+------------------------------------------+-----------------+-----------------+-----------------+-----------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+----------+-------------+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "first_table.select(\"*\",'Hillshade_3pm', lit(1).alias(\"One\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c442ae4-13b8-4b43-8c73-89c2d4c7e85d",
   "metadata": {},
   "source": [
    "# Adding Columns-\n",
    "There’s also a more formal way of adding a new column to a DataFrame, and that’s by using the\n",
    "\"withColumn\" method on our DataFrame. For example,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e06a583c-d33c-4173-8eb3-cc33c0561d1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+--------------+--------------------+----+---------+\n",
      "|       Qualification|         Experience|Rating|               Place|       Profile|  Miscellaneous_Info|Fees|NumberOne|\n",
      "+--------------------+-------------------+------+--------------------+--------------+--------------------+----+---------+\n",
      "|BHMS, MD - Homeop...|24 years experience|  100%| Kakkanad, Ernakulam|     Homeopath|100% 16 Feedback ...| 100|    AMKXA|\n",
      "|BAMS, MD - Ayurve...|12 years experience|   98%|Whitefield, Banga...|      Ayurveda|98% 76 Feedback W...| 350|    AMKXA|\n",
      "|MBBS, MS - Otorhi...| 9 years experience|  null|Mathikere - BEL, ...|ENT Specialist|                null| 300|    AMKXA|\n",
      "| BSc - Zoology, BAMS|12 years experience|  null|Bannerghatta Road...|      Ayurveda|Bannerghatta Road...| 250|    AMKXA|\n",
      "|                BAMS|20 years experience|  100%|Keelkattalai, Che...|      Ayurveda|100% 4 Feedback K...| 250|    AMKXA|\n",
      "|                BAMS| 8 years experience|  null|      Porur, Chennai|      Ayurveda|                null| 100|    AMKXA|\n",
      "|                BHMS|42 years experience|  null|   Karol Bagh, Delhi|     Homeopath|                null| 200|    AMKXA|\n",
      "+--------------------+-------------------+------+--------------------+--------------+--------------------+----+---------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Use \"lit()\" method if we want to enter either integer or string as used below-\n",
    "\n",
    "#first_table.withColumn(\"numberOne\", lit(1)).show(2)\n",
    "data_table.withColumn(\"NumberOne\",lit('AMKXA')).show(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3526655a-c415-4a42-af7f-00fb3eb38197",
   "metadata": {},
   "source": [
    "we’ll set a Boolean flag for when profile is the same as the Qualification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "08b1a9fc-3408-44d1-a0a8-85470be842ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+---------+--------------------+----+-------------+\n",
      "|       Qualification|         Experience|Rating|               Place|  Profile|  Miscellaneous_Info|Fees|Profile_Check|\n",
      "+--------------------+-------------------+------+--------------------+---------+--------------------+----+-------------+\n",
      "|BHMS, MD - Homeop...|24 years experience|  100%| Kakkanad, Ernakulam|Homeopath|100% 16 Feedback ...| 100|        false|\n",
      "|BAMS, MD - Ayurve...|12 years experience|   98%|Whitefield, Banga...| Ayurveda|98% 76 Feedback W...| 350|        false|\n",
      "+--------------------+-------------------+------+--------------------+---------+--------------------+----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.withColumn(\"Profile_Check\", expr(\"Profile == Qualification\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947316ed-9ace-4964-ae1d-9908399863b6",
   "metadata": {},
   "source": [
    "# Renaming Columns-\n",
    "Although we can rename a column in the manner that we just described, another alternative is to\n",
    "use the \"withColumnRenamed\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f236415-f250-4cdb-91e7-148c9f1c4bab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Qualification',\n",
       " 'Experience',\n",
       " 'Rating',\n",
       " 'Place',\n",
       " 'Expertize',\n",
       " 'Miscellaneous_Info',\n",
       " 'Fees']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table.withColumnRenamed(\"Profile\", \"Expertize\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b88a9d-d201-47f8-9499-00ad007f2010",
   "metadata": {},
   "source": [
    "# Case Sensitivity-\n",
    "By default Spark is case insensitive; however, you can make Spark case sensitive by setting the\n",
    "configuration:\n",
    "\n",
    "set spark.sql.caseSensitive true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541bbc2c-c1e4-42d2-b786-50e2b400fc46",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Removing Columns-\n",
    "Let’s take a look at how we can remove columns from\n",
    "DataFrames. You likely already noticed that we can do this by using select. However, there is\n",
    "also a dedicated method called \"drop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1b0cd623-db33-4437-be61-30210075389f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Elevation(meters)',\n",
       " 'Aspect(degrees)',\n",
       " 'Slope(degrees)',\n",
       " 'Horizontal_Distance_To_Hydrology(meters)',\n",
       " 'Vertical_Distance_To_Hydrology(meters)',\n",
       " 'Horizontal_Distance_To_Roadways(meters)',\n",
       " 'Hillshade_9am',\n",
       " 'Hillshade_Noon',\n",
       " 'Hillshade_3pm',\n",
       " 'Horizontal_Distance_To_Fire_Points(meters)',\n",
       " 'Wilderness_Area_1',\n",
       " 'Wilderness_Area_2',\n",
       " 'Wilderness_Area_3',\n",
       " 'Wilderness_Area_4',\n",
       " 'Soil_Type_5',\n",
       " 'Soil_Type_6',\n",
       " 'Soil_Type_7',\n",
       " 'Soil_Type_8',\n",
       " 'Soil_Type_9',\n",
       " 'Soil_Type_10',\n",
       " 'Soil_Type_11',\n",
       " 'Soil_Type_12',\n",
       " 'Soil_Type_13',\n",
       " 'Soil_Type_14',\n",
       " 'Soil_Type_15',\n",
       " 'Soil_Type_16',\n",
       " 'Soil_Type_17',\n",
       " 'Soil_Type_18',\n",
       " 'Soil_Type_19',\n",
       " 'Soil_Type_20',\n",
       " 'Soil_Type_21',\n",
       " 'Soil_Type_22',\n",
       " 'Soil_Type_23',\n",
       " 'Soil_Type_24',\n",
       " 'Soil_Type_25',\n",
       " 'Soil_Type_26',\n",
       " 'Soil_Type_27',\n",
       " 'Soil_Type_28',\n",
       " 'Soil_Type_29',\n",
       " 'Soil_Type_30',\n",
       " 'Soil_Type_31',\n",
       " 'Soil_Type_32',\n",
       " 'Soil_Type_33',\n",
       " 'Soil_Type_34',\n",
       " 'Soil_Type_35',\n",
       " 'Soil_Type_36',\n",
       " 'Soil_Type_37',\n",
       " 'Soil_Type_38',\n",
       " 'Soil_Type_39',\n",
       " 'Soil_Type_40',\n",
       " 'Cover_Type']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first_table.drop(\"Soil_Type_1\").columns\n",
    "\n",
    "#We can drop multiple columns by passing in multiple columns as arguments:\n",
    "\n",
    "first_table.drop(\"Soil_Type_1\", \"Soil_Type_2\",\"Soil_Type_3\", \"Soil_Type_4\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dd813e-04ec-4211-9f8a-6232ab98457b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Changing a Column’s Type (cast)-\n",
    "Sometimes, we might need to convert from one type to another; for example, if we have a set of\n",
    "StringType that should be integers. We can convert columns from one type to another by\n",
    "casting the column from one type to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa46de18-fb2b-4fb5-80ff-d17f149f78b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Qualification: string, Experience: string, Rating: string, Place: string, Profile: string, Miscellaneous_Info: string, Fees: string, Extra Taxes: float]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table.withColumn(\"Extra Taxes\", col(\"Fees\").cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d534c46a-9c23-4737-88a3-b470ab1e3593",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Qualification: string, Experience: string, Rating: string, Place: string, Profile: string, Miscellaneous_Info: string, Fees: string, Fees in long: bigint]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table.withColumn(\"Fees in long\", col(\"Fees\").cast(\"long\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45193583-faa7-4dbc-b947-0d8b269377c9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Filtering Rows-\n",
    "To filter rows, we create an expression that evaluates to true or false. You then filter out the rows\n",
    "with an expression that is equal to false.\n",
    "\n",
    "There are two methods to perform this operation: you can use \"where\" or \"filter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2edbf9a1-2977-4bbf-b42a-c343eb9f604c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "|       Qualification|         Experience|Rating|               Place|         Profile|  Miscellaneous_Info|Fees|\n",
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "|BHMS, MD - Homeop...|24 years experience|  100%| Kakkanad, Ernakulam|       Homeopath|100% 16 Feedback ...| 100|\n",
      "|                BAMS| 8 years experience|  null|      Porur, Chennai|        Ayurveda|                null| 100|\n",
      "|MBBS, MD - Genera...|14 years experience|  null| Old City, Hyderabad|General Medicine|                null| 100|\n",
      "|            BSc, BDS|23 years experience|  null|   Athani, Ernakulam|         Dentist|                null| 100|\n",
      "|                BAMS| 7 years experience|  null|Somajiguda, Hyder...|        Ayurveda|                null| 100|\n",
      "|MBBS, Diploma in ...|24 years experience|  null|Tambaram West, Ch...|  ENT Specialist|                null| 100|\n",
      "|       MDS, DNB, BDS|21 years experience|  100%|Pollachi, Coimbatore|         Dentist|100% 7 Feedback P...| 100|\n",
      "|MBBS, Fellowship ...|31 years experience|  null|Thammanam, Ernakulam|General Medicine|                null| 100|\n",
      "|          MBBS, DDVL|23 years experience|  null|     Andheri, Mumbai|  Dermatologists|                null| 100|\n",
      "|                 BDS| 5 years experience|  null|Pattom, Thiruvana...|         Dentist|                null| 100|\n",
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+-------------------+------+--------------------+--------+--------------------+----+\n",
      "|       Qualification|         Experience|Rating|               Place| Profile|  Miscellaneous_Info|Fees|\n",
      "+--------------------+-------------------+------+--------------------+--------+--------------------+----+\n",
      "|BAMS, MD - Ayurve...|12 years experience|   98%|Whitefield, Banga...|Ayurveda|98% 76 Feedback W...| 350|\n",
      "+--------------------+-------------------+------+--------------------+--------+--------------------+----+\n",
      "\n",
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "|       Qualification|         Experience|Rating|               Place|         Profile|  Miscellaneous_Info|Fees|\n",
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "|BHMS, MD - Homeop...|24 years experience|  100%| Kakkanad, Ernakulam|       Homeopath|100% 16 Feedback ...| 100|\n",
      "|                BAMS| 8 years experience|  null|      Porur, Chennai|        Ayurveda|                null| 100|\n",
      "|MBBS, MD - Genera...|14 years experience|  null| Old City, Hyderabad|General Medicine|                null| 100|\n",
      "|            BSc, BDS|23 years experience|  null|   Athani, Ernakulam|         Dentist|                null| 100|\n",
      "|                BAMS| 7 years experience|  null|Somajiguda, Hyder...|        Ayurveda|                null| 100|\n",
      "|MBBS, Diploma in ...|24 years experience|  null|Tambaram West, Ch...|  ENT Specialist|                null| 100|\n",
      "|       MDS, DNB, BDS|21 years experience|  100%|Pollachi, Coimbatore|         Dentist|100% 7 Feedback P...| 100|\n",
      "|MBBS, Fellowship ...|31 years experience|  null|Thammanam, Ernakulam|General Medicine|                null| 100|\n",
      "|          MBBS, DDVL|23 years experience|  null|     Andheri, Mumbai|  Dermatologists|                null| 100|\n",
      "|                 BDS| 5 years experience|  null|Pattom, Thiruvana...|         Dentist|                null| 100|\n",
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.filter(col(\"Fees\")== 100).show(10)       # using \"filter()\" method\n",
    "\n",
    "data_table.filter(col(\"Fees\")>= 350).filter(col(\"Profile\")=='Ayurveda').filter(col('Rating')=='98%').show()     ### using multiple \"filter()\" method togerther\n",
    "\n",
    "#data_table.where(col(\"Fees\")== 100).show(10)     \n",
    "\n",
    "data_table.where(\"Fees = 100\").show(10)\t     ### using \"where\" method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2a75448b-1a18-4e4b-a579-c7753c921129",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "|       Qualification|         Experience|Rating|               Place|         Profile|  Miscellaneous_Info|Fees|\n",
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "|BHMS, MD - Homeop...|24 years experience|  100%| Kakkanad, Ernakulam|       Homeopath|100% 16 Feedback ...| 100|\n",
      "|BAMS, MD - Ayurve...|12 years experience|   98%|Whitefield, Banga...|        Ayurveda|98% 76 Feedback W...| 350|\n",
      "|MBBS, MS - Otorhi...| 9 years experience|  null|Mathikere - BEL, ...|  ENT Specialist|                null| 300|\n",
      "| BSc - Zoology, BAMS|12 years experience|  null|Bannerghatta Road...|        Ayurveda|Bannerghatta Road...| 250|\n",
      "|                BAMS|20 years experience|  100%|Keelkattalai, Che...|        Ayurveda|100% 4 Feedback K...| 250|\n",
      "|                BAMS| 8 years experience|  null|      Porur, Chennai|        Ayurveda|                null| 100|\n",
      "|                BHMS|42 years experience|  null|   Karol Bagh, Delhi|       Homeopath|                null| 200|\n",
      "|                 BDS|10 years experience|   99%|  Arekere, Bangalore|         Dentist|Dental Fillings C...| 200|\n",
      "|MBBS, MD - Genera...|14 years experience|  null| Old City, Hyderabad|General Medicine|                null| 100|\n",
      "|            BSc, BDS|23 years experience|  null|   Athani, Ernakulam|         Dentist|                null| 100|\n",
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ea7269-d41c-4e3f-9cf6-0379f5d56b12",
   "metadata": {},
   "source": [
    "# Getting Unique Rows- \n",
    "The way we do this is by using the distinct method on a\n",
    "DataFrame, which allows us to deduplicate any rows that are in that DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "462651a1-db17-4cca-a792-c8c958ca380b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table.select(\"Experience\",\"Profile\").distinct().count()    # Distinct count of \"Experience\", \"Profile\"\n",
    "data_table.select(\"Experience\",\"Profile\").count()     # Total count of \"Experience\", \"Profile\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11e6d41-6781-45f1-a2b9-dbf4df6a4129",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Random Samples-\n",
    "Sometimes, you might just want to sample some random records from your DataFrame. You can\n",
    "do this by using the \"sample\" method on a DataFrame and whether you’d like to sample with or without\n",
    "replacement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "57670e50-9fc0-4f13-a6e8-fc581fa5a1a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in Python\n",
    "seed = 8\n",
    "withReplacement = False\n",
    "fraction = 0.45\n",
    "data_table.sample(withReplacement, fraction, seed).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0a1d98-0478-4381-b12c-af8ba2b600c8",
   "metadata": {},
   "source": [
    "# Random Splits-\n",
    "Random splits can be helpful when you need to break up your DataFrame into a random “splits”\n",
    "of the original DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5fc7b687-0b51-42be-b057-7febb6bb5025",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrames = data_table.randomSplit([0.25, 0.75], seed)\n",
    "#dataFrames[0].count() > dataFrames[1].count()\n",
    "dataFrames[0].count() > dataFrames[1].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44fda56-021e-4ae8-b0e7-2c89c3e40bae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Concatenating and Appending Rows (Union)- \n",
    "To append to a DataFrame, you must\n",
    "union the original DataFrame along with the new DataFrame. This just concatenates the two\n",
    "DataFramess. To union two DataFrames, you must be sure that they have the same schema and\n",
    "number of columns; otherwise, the union will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ea1a0b98-5a98-43e1-89ba-16e07d14c94e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rahul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\serializers.py\", line 458, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Rahul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"C:\\Users\\Rahul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Rahul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 692, in reducer_override\n",
      "    return self._function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Rahul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 565, in _function_reduce\n",
      "    return self._dynamic_function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Rahul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 546, in _dynamic_function_reduce\n",
      "    state = _function_getstate(func)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Rahul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 157, in _function_getstate\n",
      "    f_globals_ref = _extract_code_globals(func.__code__)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Rahul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle.py\", line 334, in _extract_code_globals\n",
      "    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Rahul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle.py\", line 334, in <dictcomp>\n",
      "    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}\n",
      "                 ~~~~~^^^^^^^\n",
      "IndexError: tuple index out of range\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: IndexError: tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\serializers.py:458\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcloudpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPickleError:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:73\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[0;32m     70\u001b[0m cp \u001b[38;5;241m=\u001b[39m CloudPickler(\n\u001b[0;32m     71\u001b[0m     file, protocol\u001b[38;5;241m=\u001b[39mprotocol, buffer_callback\u001b[38;5;241m=\u001b[39mbuffer_callback\n\u001b[0;32m     72\u001b[0m )\n\u001b[1;32m---> 73\u001b[0m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:602\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 602\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:692\u001b[0m, in \u001b[0;36mCloudPickler.reducer_override\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, types\u001b[38;5;241m.\u001b[39mFunctionType):\n\u001b[1;32m--> 692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    694\u001b[0m     \u001b[38;5;66;03m# fallback to save_global, including the Pickler's\u001b[39;00m\n\u001b[0;32m    695\u001b[0m     \u001b[38;5;66;03m# dispatch_table\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:565\u001b[0m, in \u001b[0;36mCloudPickler._function_reduce\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 565\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamic_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:546\u001b[0m, in \u001b[0;36mCloudPickler._dynamic_function_reduce\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m    545\u001b[0m newargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_getnewargs(func)\n\u001b[1;32m--> 546\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43m_function_getstate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (types\u001b[38;5;241m.\u001b[39mFunctionType, newargs, state, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    548\u001b[0m         _function_setstate)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:157\u001b[0m, in \u001b[0;36m_function_getstate\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m    146\u001b[0m slotstate \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m\"\u001b[39m: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__qualname__\u001b[39m\u001b[38;5;124m\"\u001b[39m: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__closure__\u001b[39m\u001b[38;5;124m\"\u001b[39m: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__closure__\u001b[39m,\n\u001b[0;32m    155\u001b[0m }\n\u001b[1;32m--> 157\u001b[0m f_globals_ref \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_code_globals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__code__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m f_globals \u001b[38;5;241m=\u001b[39m {k: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__globals__\u001b[39m[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f_globals_ref \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m    159\u001b[0m              func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__globals__\u001b[39m}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle.py:334\u001b[0m, in \u001b[0;36m_extract_code_globals\u001b[1;34m(co)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# We use a dict with None values instead of a set to get a\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;66;03m# deterministic order (assuming Python 3.6+) and avoid introducing\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;66;03m# non-deterministic pickle bytes as a results.\u001b[39;00m\n\u001b[1;32m--> 334\u001b[0m out_names \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m[\u001b[49m\u001b[43moparg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moparg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_walk_global_ops\u001b[49m\u001b[43m(\u001b[49m\u001b[43mco\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;66;03m# Declaring a function inside another one using the \"def ...\"\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;66;03m# syntax generates a constant code object corresponding to the one\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;66;03m# of the nested function's As the nested function may itself need\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;66;03m# global variables, we need to introspect its code, extract its\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;66;03m# globals, (look for code object in it's co_consts attribute..) and\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# add the result to code_globals\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle.py:334\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# We use a dict with None values instead of a set to get a\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;66;03m# deterministic order (assuming Python 3.6+) and avoid introducing\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;66;03m# non-deterministic pickle bytes as a results.\u001b[39;00m\n\u001b[1;32m--> 334\u001b[0m out_names \u001b[38;5;241m=\u001b[39m {\u001b[43mnames\u001b[49m\u001b[43m[\u001b[49m\u001b[43moparg\u001b[49m\u001b[43m]\u001b[49m: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _, oparg \u001b[38;5;129;01min\u001b[39;00m _walk_global_ops(co)}\n\u001b[0;32m    336\u001b[0m \u001b[38;5;66;03m# Declaring a function inside another one using the \"def ...\"\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;66;03m# syntax generates a constant code object corresponding to the one\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;66;03m# of the nested function's As the nested function may itself need\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;66;03m# global variables, we need to introspect its code, extract its\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;66;03m# globals, (look for code object in it's co_consts attribute..) and\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# add the result to code_globals\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m parallelizedRows \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39mparallelize(newRows)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#print(parallelizedRows)\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m newDF \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparallelizedRows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#print(newDF)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m df\u001b[38;5;241m.\u001b[39munion(newDF)\\\n\u001b[0;32m     16\u001b[0m \u001b[38;5;241m.\u001b[39mwhere(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount = 1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[0;32m     17\u001b[0m \u001b[38;5;241m.\u001b[39mwhere(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mORIGIN_COUNTRY_NAME\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnited States\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[0;32m     18\u001b[0m \u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:894\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m    890\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m    891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m    892\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[0;32m    893\u001b[0m     )\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    896\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:938\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    936\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n\u001b[0;32m    937\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 938\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_java_object_rdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    939\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39mapplySchemaToPythonRDD(jrdd\u001b[38;5;241m.\u001b[39mrdd(), struct\u001b[38;5;241m.\u001b[39mjson())\n\u001b[0;32m    940\u001b[0m df \u001b[38;5;241m=\u001b[39m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\rdd.py:3113\u001b[0m, in \u001b[0;36mRDD._to_java_object_rdd\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3110\u001b[0m rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pickled()\n\u001b[0;32m   3111\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 3113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mpythonToJava(\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\rdd.py:3505\u001b[0m, in \u001b[0;36mPipelinedRDD._jrdd\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3503\u001b[0m     profiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 3505\u001b[0m wrapped_func \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prev_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler\u001b[49m\n\u001b[0;32m   3507\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3509\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3510\u001b[0m python_rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD(\n\u001b[0;32m   3511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_jrdd\u001b[38;5;241m.\u001b[39mrdd(), wrapped_func, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreservesPartitioning, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_barrier\n\u001b[0;32m   3512\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\rdd.py:3362\u001b[0m, in \u001b[0;36m_wrap_function\u001b[1;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[0;32m   3360\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m serializer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserializer should not be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3361\u001b[0m command \u001b[38;5;241m=\u001b[39m (func, profiler, deserializer, serializer)\n\u001b[1;32m-> 3362\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_for_python_RDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3363\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonFunction(\n\u001b[0;32m   3365\u001b[0m     \u001b[38;5;28mbytearray\u001b[39m(pickled_command),\n\u001b[0;32m   3366\u001b[0m     env,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3371\u001b[0m     sc\u001b[38;5;241m.\u001b[39m_javaAccumulator,\n\u001b[0;32m   3372\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\rdd.py:3345\u001b[0m, in \u001b[0;36m_prepare_for_python_RDD\u001b[1;34m(sc, command)\u001b[0m\n\u001b[0;32m   3342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_for_python_RDD\u001b[39m(sc: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkContext\u001b[39m\u001b[38;5;124m\"\u001b[39m, command: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mbytes\u001b[39m, Any, Any, Any]:\n\u001b[0;32m   3343\u001b[0m     \u001b[38;5;66;03m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[0;32m   3344\u001b[0m     ser \u001b[38;5;241m=\u001b[39m CloudPickleSerializer()\n\u001b[1;32m-> 3345\u001b[0m     pickled_command \u001b[38;5;241m=\u001b[39m \u001b[43mser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3346\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) \u001b[38;5;241m>\u001b[39m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mgetBroadcastThreshold(sc\u001b[38;5;241m.\u001b[39m_jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[0;32m   3348\u001b[0m         \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\serializers.py:468\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    466\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, emsg)\n\u001b[0;32m    467\u001b[0m print_exec(sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m--> 468\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPicklingError(msg)\n",
      "\u001b[1;31mPicklingError\u001b[0m: Could not serialize object: IndexError: tuple index out of range"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "schema = data_table.schema\n",
    "newRows = [\n",
    "Row(\"B.Tech\", \"2994 years experience\", \"8918%\", \"Janakpuri, Lucknow\", \"Software Engineer\", \"No such info.\", 3450),\n",
    "Row(\"B.Tech\", \"9282 years experience\", \"1111%\", \"Dubagga, Lucknow\", \"Senior Software Engineer\", \"No info.\", 1928)\n",
    "]\n",
    " \n",
    "#print(newRows)\n",
    "parallelizedRows = spark.sparkContext.parallelize(newRows)\n",
    "#print(parallelizedRows)\n",
    "newDF = spark.createDataFrame(parallelizedRows, schema)\n",
    "#print(newDF)\n",
    "\n",
    "\n",
    "df.union(newDF)\\\n",
    ".where(\"count = 1\")\\\n",
    ".where(col(\"ORIGIN_COUNTRY_NAME\") != \"United States\")\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe87847-dd15-4564-bda2-ab4ee952bb92",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Sorting Rows-\n",
    "When we sort the values in a DataFrame, we always want to sort with either the largest or\n",
    "smallest values at the top of a DataFrame. There are two equivalent operations to do this \"sort\"\n",
    "and \"orderBy\" that work the exact same way. \n",
    "\n",
    "They accept both column expressions and strings as\n",
    "well as multiple columns. The default is to sort in ascending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df47113-6684-444d-8f4f-db3afc49514d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_table.sort(\"Fees\").show(5)\n",
    "data_table.orderBy(\"Fees\", \"Rating\",\"Experience\").show(5)\n",
    "data_table.orderBy(col(\"Fees\"), col(\"Rating\"), col(\"Experience\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "df572b90-af18-4b28-a09e-0a381e172fb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "|       Qualification|         Experience|Rating|               Place|         Profile|  Miscellaneous_Info|Fees|\n",
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "|                MBBS| 5 years experience|  null|Greater Kailash P...|General Medicine|Diabetes Manageme...| 800|\n",
      "|MBBS, MD - Dermat...|12 years experience|  null|Panchsheel Park, ...|  Dermatologists|Panchsheel Park, ...| 800|\n",
      "|      MBBS, MS - ENT|39 years experience|   86%|    Pitampura, Delhi|  ENT Specialist|86% 15 Feedback P...| 800|\n",
      "|          MBBS, DDVL|10 years experience|   89%| Vadapalani, Chennai|  Dermatologists|Dermabrasion Chem...| 800|\n",
      "|                MBBS|13 years experience|   96%|Jubilee Hills, Hy...|General Medicine|96% 20 Feedback J...| 750|\n",
      "|MBBS, MD - Dermat...|33 years experience|  null|   Tuglakabad, Delhi|  Dermatologists|                null| 700|\n",
      "| MBBS, MS, DNB - ENT| 5 years experience|  null|Thousand Lights, ...|  ENT Specialist|                null| 700|\n",
      "|MD - Dermatology ...|13 years experience|   97%|   Green Park, Delhi|  Dermatologists|General dermatolo...| 700|\n",
      "|          MBBS, DDVL|22 years experience|  null| Madhapur, Hyderabad|  Dermatologists|                null| 650|\n",
      "|MBBS, MS - ENT, M...|25 years experience|   36%|Whitefield, Banga...|  ENT Specialist|36% 7 Feedback Wh...| 600|\n",
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To more explicitly specify sort direction, you need to use the asc and desc functions if operating\n",
    "#on a column. These allow you to specify the order in which a given column should be sorted:\n",
    "\n",
    "from pyspark.sql.functions import desc, asc\n",
    "#data_table.orderBy(expr(\"Fees desc\")).show(5)\n",
    "#data_table.orderBy(expr(\"Fees desc\"), expr(\"Rating desc\"),expr(\"Experience desc\")).show(5)\n",
    "\n",
    "data_table.orderBy(col(\"Fees\").desc(), col(\"Rating\").asc()).show(10)\n",
    "#df.orderBy(col(\"count\").desc(), col(\"DEST_COUNTRY_NAME\").asc()).show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c194f92-dbad-4dbc-9c18-e8c8d08a5754",
   "metadata": {},
   "source": [
    "An advanced tip is to use \"asc_nulls_first()\", \"desc_nulls_first()\", \"asc_nulls_last()\", or\n",
    "\"desc_nulls_last()\" method to specify where you would like your null values to appear in an ordered\n",
    "DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "440629b2-eee4-4f9d-aba3-b4241689ef3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "|       Qualification|         Experience|Rating|               Place|         Profile|  Miscellaneous_Info|Fees|\n",
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "|                 BDS|10 years experience|  100%|Vileparle East, M...|         Dentist|Ceramic Veneers /...| 100|\n",
      "|BHMS, MD - Homeop...|24 years experience|  100%| Kakkanad, Ernakulam|       Homeopath|100% 16 Feedback ...| 100|\n",
      "|BDS, MDS - Period...|20 years experience|  100%|Mogappair East, C...|         Dentist|100% 51 Feedback ...| 100|\n",
      "|       MDS, DNB, BDS|21 years experience|  100%|Pollachi, Coimbatore|         Dentist|100% 7 Feedback P...| 100|\n",
      "|MD - General Medi...|34 years experience|  100%|Borivali West, Mu...|General Medicine|100% 2 Feedback B...| 100|\n",
      "|MBBS, MS - ENT, D...|31 years experience|   80%|        Saket, Delhi|  ENT Specialist|80% 3 Feedback Sa...| 100|\n",
      "|DDVL, Diploma in ...|13 years experience|   90%|Defence Colony, D...|  Dermatologists|90% 55 Feedback D...| 100|\n",
      "|      MBBS, MS - ENT|14 years experience|   94%|      CR Park, Delhi|  ENT Specialist|Functional Endosc...| 100|\n",
      "|           MBBS, DDV|21 years experience|   95%|   Khar West, Mumbai|  Dermatologists|95% 84 Feedback K...| 100|\n",
      "|                BAMS|12 years experience|   96%|  Dadar West, Mumbai|        Ayurveda|96% 30 Feedback D...| 100|\n",
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+-------------------+------+--------------------+--------------+--------------------+----+\n",
      "|       Qualification|         Experience|Rating|               Place|       Profile|  Miscellaneous_Info|Fees|\n",
      "+--------------------+-------------------+------+--------------------+--------------+--------------------+----+\n",
      "|           MBBS, DDV|11 years experience|   99%|     Girgaon, Mumbai|Dermatologists|99% 43 Feedback G...| 100|\n",
      "|                BAMS|12 years experience|   96%|  Dadar West, Mumbai|      Ayurveda|96% 30 Feedback D...| 100|\n",
      "|           MBBS, DDV|21 years experience|   95%|   Khar West, Mumbai|Dermatologists|95% 84 Feedback K...| 100|\n",
      "|      MBBS, MS - ENT|14 years experience|   94%|      CR Park, Delhi|ENT Specialist|Functional Endosc...| 100|\n",
      "|DDVL, Diploma in ...|13 years experience|   90%|Defence Colony, D...|Dermatologists|90% 55 Feedback D...| 100|\n",
      "|MBBS, MS - ENT, D...|31 years experience|   80%|        Saket, Delhi|ENT Specialist|80% 3 Feedback Sa...| 100|\n",
      "|                 BDS|10 years experience|  100%|Vileparle East, M...|       Dentist|Ceramic Veneers /...| 100|\n",
      "|BDS, MDS - Period...|20 years experience|  100%|Mogappair East, C...|       Dentist|100% 51 Feedback ...| 100|\n",
      "|       MDS, DNB, BDS|21 years experience|  100%|Pollachi, Coimbatore|       Dentist|100% 7 Feedback P...| 100|\n",
      "|BHMS, MD - Homeop...|24 years experience|  100%| Kakkanad, Ernakulam|     Homeopath|100% 16 Feedback ...| 100|\n",
      "+--------------------+-------------------+------+--------------------+--------------+--------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.orderBy(col(\"Fees\").asc(), col(\"Rating\").asc_nulls_last()).show(10)\n",
    "\n",
    "data_table.orderBy(col(\"Fees\").asc(), col(\"Rating\").desc_nulls_last(), col(\"Experience\").asc_nulls_last()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3eca13-6a2f-44a6-abd4-d8af7256a1c4",
   "metadata": {},
   "source": [
    "For optimization purposes, it’s sometimes advisable to sort within each partition before another\n",
    "set of transformations. You can use the \"sortWithinPartitions()\" method to do this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e990e096-7c8d-429f-aedd-6ebc8fe6e6a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+--------+--------------------+----+\n",
      "|       Qualification|         Experience|Rating|               Place| Profile|  Miscellaneous_Info|Fees|\n",
      "+--------------------+-------------------+------+--------------------+--------+--------------------+----+\n",
      "|BAMS, MD - Ayurve...|12 years experience|   98%|Whitefield, Banga...|Ayurveda|98% 76 Feedback W...| 350|\n",
      "| BSc - Zoology, BAMS|12 years experience|  null|Bannerghatta Road...|Ayurveda|Bannerghatta Road...| 250|\n",
      "|                BAMS|20 years experience|  100%|Keelkattalai, Che...|Ayurveda|100% 4 Feedback K...| 250|\n",
      "|                BAMS| 8 years experience|  null|      Porur, Chennai|Ayurveda|                null| 100|\n",
      "|                BAMS| 7 years experience|  null|Somajiguda, Hyder...|Ayurveda|                null| 100|\n",
      "|                BAMS| 9 years experience|  null| IP Extension, Delhi|Ayurveda|                null| 300|\n",
      "|BAMS, Diploma in ...|31 years experience|  100%| RT Nagar, Bangalore|Ayurveda|100% 7 Feedback R...| 500|\n",
      "|                BAMS| 8 years experience|  null|Saroor Nagar, Hyd...|Ayurveda|                null| 300|\n",
      "|                BAMS| 9 years experience|  null|    Chetpet, Chennai|Ayurveda|                null| 150|\n",
      "|                BAMS| 9 years experience|  100%|     KPHB, Hyderabad|Ayurveda|Arthritis Managem...| 300|\n",
      "|                BAMS|20 years experience|  null|     Sagarpur, Delhi|Ayurveda|                null|  50|\n",
      "|BAMS, Post Gradua...| 8 years experience|  null|Dahisar West, Mumbai|Ayurveda|                null|  50|\n",
      "|BAMS, MD - Acupun...|15 years experience|  null|Anna Nagar East, ...|Ayurveda|                null| 250|\n",
      "|                BAMS|27 years experience|  null|Dahisar East, Mumbai|Ayurveda|                null| 100|\n",
      "| BAMS, MS - Ayurveda|16 years experience|  100%|Yelahanka, Bangalore|Ayurveda|100% 3 Feedback Y...| 300|\n",
      "|                BAMS|12 years experience|   96%|  Dadar West, Mumbai|Ayurveda|96% 30 Feedback D...| 100|\n",
      "|MD - Ayurveda Med...|20 years experience|  null|Goregaon West, Mu...|Ayurveda|                null| 500|\n",
      "| BAMS, MS - Ayurveda|10 years experience|  null|   Bakkarwala, Delhi|Ayurveda|                null| 150|\n",
      "|                 BDS|10 years experience|   99%|  Arekere, Bangalore| Dentist|Dental Fillings C...| 200|\n",
      "|            BSc, BDS|23 years experience|  null|   Athani, Ernakulam| Dentist|                null| 100|\n",
      "+--------------------+-------------------+------+--------------------+--------+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.sortWithinPartitions(\"Profile\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ad9121-44d9-4fee-b57d-ee11b272dde6",
   "metadata": {},
   "source": [
    "# Limit-\n",
    "Oftentimes, you might want to restrict what you extract from a DataFrame; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3dcb75d8-b35f-499a-a826-cbb609814eec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "|       Qualification|         Experience|Rating|               Place|         Profile|  Miscellaneous_Info|Fees|\n",
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "|      MBBS, MS - ENT|39 years experience|   86%|    Pitampura, Delhi|  ENT Specialist|86% 15 Feedback P...| 800|\n",
      "|          MBBS, DDVL|10 years experience|   89%| Vadapalani, Chennai|  Dermatologists|Dermabrasion Chem...| 800|\n",
      "|                MBBS| 5 years experience|  null|Greater Kailash P...|General Medicine|Diabetes Manageme...| 800|\n",
      "|MBBS, MD - Dermat...|12 years experience|  null|Panchsheel Park, ...|  Dermatologists|Panchsheel Park, ...| 800|\n",
      "|                MBBS|13 years experience|   96%|Jubilee Hills, Hy...|General Medicine|96% 20 Feedback J...| 750|\n",
      "| MBBS, MS, DNB - ENT| 5 years experience|  null|Thousand Lights, ...|  ENT Specialist|                null| 700|\n",
      "|MBBS, MD - Dermat...|33 years experience|  null|   Tuglakabad, Delhi|  Dermatologists|                null| 700|\n",
      "|MD - Dermatology ...|13 years experience|   97%|   Green Park, Delhi|  Dermatologists|General dermatolo...| 700|\n",
      "|          MBBS, DDVL|22 years experience|  null| Madhapur, Hyderabad|  Dermatologists|                null| 650|\n",
      "|           MBBS, DDV|15 years experience|   95%|  Chandivali, Mumbai|  Dermatologists|95% 18 Feedback C...| 600|\n",
      "|MD - Dermatology,...|12 years experience|   97%|Shalimar Bagh, Delhi|  Dermatologists|97% 47 Feedback S...| 600|\n",
      "|MBBS, MS - ENT, M...|25 years experience|   36%|Whitefield, Banga...|  ENT Specialist|36% 7 Feedback Wh...| 600|\n",
      "|MBBS, MD - Dermat...| 8 years experience|  null|Pallikaranai, Che...|General Medicine|1 Feedback Pallik...| 500|\n",
      "|   MBBS, IBCLC (USA)|25 years experience|  100%|HSR Layout, Banga...|General Medicine|100% 46 Feedback ...| 500|\n",
      "|MBBS, Diploma in ...|12 years experience|  null|       Kondli, Delhi|  ENT Specialist|                null| 500|\n",
      "|MBBS, DDVL, MD - ...|36 years experience|   78%|Himayat Nagar, Hy...|  Dermatologists|78% 24 Feedback H...| 500|\n",
      "|BHMS, M. D. Hom. ...|12 years experience|  100%|Dahisar West, Mumbai|       Homeopath|100% 21 Feedback ...| 500|\n",
      "|MDS - Oral & Maxi...| 6 years experience|  100%|Andheri West, Mumbai|         Dentist|Oral Surgery Proc...| 500|\n",
      "|MBBS, MD - Dermat...| 7 years experience|  null|Basheerbagh, Hyde...|  Dermatologists|                null| 500|\n",
      "|      MBBS, MS - ENT|13 years experience|   99%| Punjabi Bagh, Delhi|  ENT Specialist|99% 44 Feedback P...| 500|\n",
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.limit(15).count()\n",
    "data_table.orderBy(col(\"Fees\").desc_nulls_last()).limit(20).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb0d981-1191-4093-95fc-52f3878a436f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Converting to Spark Types-\n",
    "\n",
    "One thing you’ll see us do throughout this chapter is convert native types to Spark types. We do\n",
    "this by using the \"lit\" function. This function converts a type in another language to its correspnding Spark representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e6c6d3cf-075d-4e36-a215-eb09b0410f14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[237: int, SLS: string, 87.48: double]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table.select(lit(237),lit('SLS'),lit(87.48))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f260990-d8f0-4380-bdeb-6614ecb9b0a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Working with Booleans-\n",
    "\n",
    "Booleans are essential when it comes to data analysis because they are the foundation for all\n",
    "filtering. \n",
    "Boolean statements consist of four elements: \"and\", \"or\", \"true\", and \"false\". We can specify equality as well as\n",
    "less-than or greater-than"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3af439cb-8770-4afa-9609-0476237d5dc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------+------------------------------------------------------------------+----+\n",
      "|Rating|Place                   |Qualification                                                     |Fees|\n",
      "+------+------------------------+------------------------------------------------------------------+----+\n",
      "|null  |Thousand Lights, Chennai|MBBS, MS, DNB - ENT                                               |700 |\n",
      "|null  |Kondli, Delhi           |MBBS, Diploma in Otorhinolaryngology (DLO), DNB - ENT             |500 |\n",
      "|null  |Vasundhra Enclave, Delhi|MBBS, DNB - ENT                                                   |500 |\n",
      "|100%  |HSR Layout, Bangalore   |MBBS, IBCLC (USA)                                                 |500 |\n",
      "|97%   |Defence Colony, Delhi   |Diploma in Dermatology, MBBS                                      |500 |\n",
      "|97%   |Banjara Hills, Hyderabad|MBBS, DDVL, Fellowship in Aesthetic Medicine                      |500 |\n",
      "|null  |Sion West, Mumbai       |BDS                                                               |500 |\n",
      "|88%   |Mogappair East, Chennai |DM - Neurology, MD - Pediatrics, MBBS                             |500 |\n",
      "|100%  |Dahisar West, Mumbai    |BHMS, M. D. Hom. (Practice of Medicine)                           |500 |\n",
      "|100%  |RT Nagar, Bangalore     |BAMS, Diploma in Emergency Medicine, Diploma in Counselling Skills|500 |\n",
      "+------+------------------------+------------------------------------------------------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "data_table.where(col(\"Fees\") >= 500)\\\n",
    ".select(\"Rating\", \"Place\", \"Qualification\",\"Fees\")\\\n",
    ".show(10, False)     #### here, True/False is showing the trucation of records in the spark table. In case if length of record becomes very large, then \n",
    "                        # spark displays is with dots(....). In order to see complete word of the entry, keep it \"False\" else \"True\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3c679448-025c-43e3-ba0b-e65f31cc67af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+----+\n",
      "|Rating|               Place|       Qualification|Fees|\n",
      "+------+--------------------+--------------------+----+\n",
      "|  100%| Kakkanad, Ernakulam|BHMS, MD - Homeop...| 100|\n",
      "|  100%|Keelkattalai, Che...|                BAMS| 250|\n",
      "|  100%|HSR Layout, Banga...|                MBBS| 150|\n",
      "|  100%|Pollachi, Coimbatore|       MDS, DNB, BDS| 100|\n",
      "|  100%|HSR Layout, Banga...|   MBBS, IBCLC (USA)| 500|\n",
      "|  100%|Safdarjung Enclav...|BDS, MDS - Oral &...| 400|\n",
      "|  100%|Hyder Nagar, Hyde...|BDS, MDS - Prosth...| 250|\n",
      "|  100%|Dahisar West, Mumbai|BHMS, M. D. Hom. ...| 500|\n",
      "|  100%| RT Nagar, Bangalore|BAMS, Diploma in ...| 500|\n",
      "|  100%|Musheerabad, Hyde...|BDS, MDS - Oral a...| 200|\n",
      "+------+--------------------+--------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# One more example\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "data_table.where(col(\"Rating\") == '100%')\\\n",
    ".select(\"Rating\",\"Place\" , \"Qualification\",\"Fees\")\\\n",
    ".show(10, True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698a9952-dbfa-4adb-84b4-6741ffa86f07",
   "metadata": {},
   "source": [
    "In Spark, you should always chain together and filters as a sequential filter.\n",
    "The reason is that even if Boolean statements are expressed serially (one after the other),\n",
    "Spark will flatten all of these filters into one statement and perform the filter at the same time,\n",
    "creating the \"and\" statement for us.\n",
    "Although you can specify your statements explicitly by using\n",
    "\"and\" if you like, they’re often easier to understand and to read if you specify them serially. \"or\"\n",
    "statements need to be specified in the same statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "82a202f0-c76c-4486-a441-e207e8bfd65e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<'(Fees < 600)'>\n",
      "Column<'(instr(Qualification, MBBS) >= 1)'>\n",
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "|       Qualification|         Experience|Rating|               Place|         Profile|  Miscellaneous_Info|Fees|\n",
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "|MBBS, MD - Genera...|14 years experience|  null| Old City, Hyderabad|General Medicine|                null| 100|\n",
      "|MBBS, MD - Genera...|10 years experience|  null|Saroor Nagar, Hyd...|General Medicine|                null| 200|\n",
      "|                MBBS|19 years experience|  100%|HSR Layout, Banga...|General Medicine|100% 4 Feedback H...| 150|\n",
      "|                MBBS|41 years experience|  null|     Chembur, Mumbai|General Medicine|                null| 200|\n",
      "|MBBS, Fellowship ...|31 years experience|  null|Thammanam, Ernakulam|General Medicine|                null| 100|\n",
      "|   MBBS, IBCLC (USA)|25 years experience|  100%|HSR Layout, Banga...|General Medicine|100% 46 Feedback ...| 500|\n",
      "|MBBS, MD - Genera...|12 years experience|   95%|Kukatpally, Hyder...|General Medicine|95% 6 Feedback Ku...| 250|\n",
      "|DM - Neurology, M...|23 years experience|   88%|Mogappair East, C...|General Medicine|88% 2 Feedback Mo...| 500|\n",
      "|          MBBS, AFIH|16 years experience|  null|       Malad, Mumbai|General Medicine|Dermabrasion Wrin...| 100|\n",
      "|                MBBS|31 years experience|  null|AS Rao Nagar, Hyd...|General Medicine|                null| 200|\n",
      "|                MBBS|42 years experience|  null|Gandhinagar, Hyde...|General Medicine|                null| 100|\n",
      "|MBBS, Member of t...|24 years experience|  null|Kodambakkam, Chennai|General Medicine|3 Feedback Kodamb...| 500|\n",
      "|MBBS, MD - Genera...|38 years experience|   99%|Malleswaram, Bang...|General Medicine|99% 54 Feedback M...| 500|\n",
      "|MBBS, MD - Dermat...| 8 years experience|  null|Pallikaranai, Che...|General Medicine|1 Feedback Pallik...| 500|\n",
      "|                MBBS|46 years experience|  null|      Kalina, Mumbai|General Medicine|                null| 100|\n",
      "|MBBS, MD - Rheuma...|40 years experience|   93%|Lakdikapul, Hyder...|General Medicine|93% 9 Feedback La...| 500|\n",
      "|MBBS, MD - Genera...|49 years experience|  null|Nungambakkam, Che...|General Medicine|Viral Fever Treat...| 100|\n",
      "|MBBS, MD - Genera...|16 years experience|  null|Palayam, Thiruvan...|General Medicine|                null| 150|\n",
      "|                MBBS|36 years experience|  null|Moosarambagh, Hyd...|General Medicine|                null|  50|\n",
      "|          MBBS, FRCP|52 years experience|  null|Greams Road, Chennai|General Medicine|                null| 100|\n",
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "priceFilter = data_table.Fees < 600\n",
    "print(priceFilter)\n",
    "descripFilter = instr(data_table.Qualification, \"MBBS\") >=1           # \"instr(x,y)\" returns 1st index of character 'y' from string x for each record\n",
    "print(descripFilter)\n",
    "\n",
    "data_table.where(data_table.Profile.isin(\"General Medicine\")).where(priceFilter & descripFilter).show(truncate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0252db9d-51e7-4928-a938-c9abfcc2023c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<'(Fees < 600)'>\n",
      "Column<'(instr(Qualification, BHMS) = 1)'>\n",
      "+--------------------+-------------------+------+--------------------+---------+--------------------+----+\n",
      "|       Qualification|         Experience|Rating|               Place|  Profile|  Miscellaneous_Info|Fees|\n",
      "+--------------------+-------------------+------+--------------------+---------+--------------------+----+\n",
      "|BHMS, MD - Homeop...|24 years experience|  100%| Kakkanad, Ernakulam|Homeopath|100% 16 Feedback ...| 100|\n",
      "|BHMS, M. D. Hom. ...|12 years experience|  100%|Dahisar West, Mumbai|Homeopath|100% 21 Feedback ...| 500|\n",
      "|BHMS, MS - Psycho...|14 years experience|  100%|HSR Layout, Banga...|Homeopath|100% 43 Feedback ...| 200|\n",
      "|                BHMS|12 years experience|  100%|Kanakpura Road, B...|Homeopath|Pediaterics Skin ...| 500|\n",
      "|                BHMS| 8 years experience|  100%|South Extension 2...|Homeopath|100% 9 Feedback S...| 350|\n",
      "+--------------------+-------------------+------+--------------------+---------+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "priceFilter = data_table.Fees < 600\n",
    "print(priceFilter)\n",
    "descripFilter = instr(data_table.Qualification, \"BHMS\") == 1     # \"instr(x,y)\" returns 1st index of character 'y' from string x for each record\n",
    "print(descripFilter)\n",
    "\n",
    "data_table.where(data_table.Rating.isin(\"100%\")).where(priceFilter & descripFilter).show(truncate = True)\n",
    "\n",
    "\n",
    "## Here, we did not need to specify our filter as an expression and we used a column name without any extra work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f01a8d-855b-469e-91c1-76e46df7293e",
   "metadata": {},
   "source": [
    "Boolean expressions are not just reserved to filters. To filter a DataFrame, you can also just\n",
    "specify a Boolean column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "96570cf0-25c0-410a-ad74-4ec27a8da02a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------+-----------+\n",
      "|Fees|       Qualification|Rating|isExpensive|\n",
      "+----+--------------------+------+-----------+\n",
      "| 100|BHMS, MD - Homeop...|  100%|       true|\n",
      "| 350|BAMS, MD - Ayurve...|   98%|       true|\n",
      "| 250| BSc - Zoology, BAMS|  null|       true|\n",
      "| 250|                BAMS|  100%|       true|\n",
      "| 500|MBBS, Diploma in ...|  null|       true|\n",
      "+----+--------------------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in Python\n",
    "from pyspark.sql.functions import instr\n",
    "DOTCodeFilter = col(\"Rating\") == \"100%\"\n",
    "priceFilter = col(\"Fees\") > 200\n",
    "descripFilter = instr(col(\"Experience\"), \"12\") == 1\n",
    "data_table.withColumn(\"isExpensive\", DOTCodeFilter | (priceFilter & descripFilter))\\\n",
    ".where(col(\"isExpensive\") == 'true')\\\n",
    ".select(\"Fees\", \"Qualification\",\"Rating\",\"isExpensive\").show(5)\n",
    "\n",
    "# Here, we did not need to specify our filter as an expression and we used a column name without any extra work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75095749-0e3f-4245-9be3-d40a71389c33",
   "metadata": {},
   "source": [
    "I’s often easier to just express\n",
    "filters as SQL statements than using the programmatic DataFrame interface and Spark SQL\n",
    "allows us to do this without paying any performance penalty. Like this-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "adccb295-e7cd-4d8f-85a1-6fafa2657216",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------+------+----+-----------------------------+\n",
      "|Qualification                                        |Rating|Fees|New_Column_added_as_Expensive|\n",
      "+-----------------------------------------------------+------+----+-----------------------------+\n",
      "|BAMS, MD - Ayurveda Medicine                         |98%   |350 |true                         |\n",
      "|MBBS, MS - Otorhinolaryngology                       |null  |300 |true                         |\n",
      "|MBBS, MS, DNB - ENT                                  |null  |700 |true                         |\n",
      "|BDS, MDS - Oral & Maxillofacial Surgery              |null  |350 |true                         |\n",
      "|MBBS, Diploma in Otorhinolaryngology (DLO), DNB - ENT|null  |500 |true                         |\n",
      "+-----------------------------------------------------+------+----+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in Python\n",
    "from pyspark.sql.functions import expr\n",
    "data_table.withColumn(\"New_Column_added_as_Expensive\", expr(\"NOT Fees <= 250\"))\\\n",
    ".where(\"New_Column_added_as_Expensive = 'true'\")\\\n",
    ".select(\"Qualification\", \"Rating\",\"Fees\",\"New_Column_added_as_Expensive\").show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3c0faa-06e7-4ca9-bfc5-c2c3c539032d",
   "metadata": {},
   "source": [
    "# Working with Numbers-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e910bddc-9448-4f0e-b8e4-86fc3603026e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+\n",
      "|Fees|Calculated Fee|\n",
      "+----+--------------+\n",
      "| 100|        1000.0|\n",
      "| 350|       12250.0|\n",
      "| 300|        9000.0|\n",
      "| 250|        6250.0|\n",
      "| 250|        6250.0|\n",
      "| 100|        1000.0|\n",
      "| 200|        4000.0|\n",
      "| 200|        4000.0|\n",
      "| 100|        1000.0|\n",
      "| 100|        1000.0|\n",
      "+----+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, pow\n",
    "fabricatedQuantity = pow(col(\"Fees\"), 2)/10\n",
    "data_table.select(expr(\"Fees\"), fabricatedQuantity.alias(\"Calculated Fee\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a7a2d133-a072-4f03-a1e6-a3335cb2363d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|               Place|Calculated_New_fee|\n",
      "+--------------------+------------------+\n",
      "| Kakkanad, Ernakulam|13274.999999999996|\n",
      "|Whitefield, Banga...|162056.24999999994|\n",
      "|Mathikere - BEL, ...|          119075.0|\n",
      "|Bannerghatta Road...|          82706.25|\n",
      "|Keelkattalai, Che...|          82706.25|\n",
      "|      Porur, Chennai|13274.999999999996|\n",
      "|   Karol Bagh, Delhi|52949.999999999985|\n",
      "|  Arekere, Bangalore|52949.999999999985|\n",
      "| Old City, Hyderabad|13274.999999999996|\n",
      "|   Athani, Ernakulam|13274.999999999996|\n",
      "+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.selectExpr(\n",
    "\"Place\",\n",
    "\"(POWER((Fees * 1.15), 2.0) + 50) as Calculated_New_fee\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda04b8d-a5b4-4130-90f9-681c818cc787",
   "metadata": {},
   "source": [
    "Another common numerical task is rounding. By default, the \"round\" function rounds up if you’re exactly in between two numbers. You can\n",
    "round down by using the \"bround\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "28e378eb-abd8-4e6c-bf1a-0ffc95e3de08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|round(2.5, 0)|bround(2.5, 0)|\n",
      "+-------------+--------------+\n",
      "|          3.0|           2.0|\n",
      "|          3.0|           2.0|\n",
      "|          3.0|           2.0|\n",
      "|          3.0|           2.0|\n",
      "|          3.0|           2.0|\n",
      "|          3.0|           2.0|\n",
      "|          3.0|           2.0|\n",
      "|          3.0|           2.0|\n",
      "|          3.0|           2.0|\n",
      "|          3.0|           2.0|\n",
      "+-------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, round, bround\n",
    "data_table.select(round(lit(\"2.5\")), bround(lit(\"2.5\"))).show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8499a0fe-e30e-46d3-a0ab-b5c07d3014c6",
   "metadata": {},
   "source": [
    "Another numerical task is to compute the correlation of two columns. For example, we can see\n",
    "the Pearson correlation coefficient for two columns to see if cheaper things are typically bought\n",
    "in greater quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a43bf641-88d4-4bf0-8cf3-b01d683f8631",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Elevation(meters): string, Aspect(degrees): string, Slope(degrees): string, Horizontal_Distance_To_Hydrology(meters): string, Vertical_Distance_To_Hydrology(meters): string, Horizontal_Distance_To_Roadways(meters): string, Hillshade_9am: string, Hillshade_Noon: string, Hillshade_3pm: string, Horizontal_Distance_To_Fire_Points(meters): string, Wilderness_Area_1: float, Wilderness_Area_2: string, Wilderness_Area_3: string, Wilderness_Area_4: string, Soil_Type_1: string, Soil_Type_2: string, Soil_Type_3: string, Soil_Type_4: string, Soil_Type_5: string, Soil_Type_6: string, Soil_Type_7: string, Soil_Type_8: string, Soil_Type_9: string, Soil_Type_10: string, Soil_Type_11: string, Soil_Type_12: string, Soil_Type_13: string, Soil_Type_14: string, Soil_Type_15: string, Soil_Type_16: string, Soil_Type_17: string, Soil_Type_18: string, Soil_Type_19: string, Soil_Type_20: string, Soil_Type_21: string, Soil_Type_22: string, Soil_Type_23: string, Soil_Type_24: string, Soil_Type_25: string, Soil_Type_26: string, Soil_Type_27: string, Soil_Type_28: string, Soil_Type_29: string, Soil_Type_30: string, Soil_Type_31: string, Soil_Type_32: string, Soil_Type_33: string, Soil_Type_34: string, Soil_Type_35: string, Soil_Type_36: string, Soil_Type_37: string, Soil_Type_38: string, Soil_Type_39: string, Soil_Type_40: string, Cover_Type: string]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_table.withColumn('Wilderness_Area_1', col('Wilderness_Area_1').cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ebf2ca74-4f80-4a17-8d8e-874fb2be160f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Wilderness_Area_1'>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_table.Wilderness_Area_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8f9e3b84-912f-47fb-9099-96a1875b4209",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------+\n",
      "|corr(CAST(Wilderness_Area_1 AS BIGINT), CAST(Elevation(meters) AS BIGINT))|\n",
      "+--------------------------------------------------------------------------+\n",
      "|                                                        0.1301108910010127|\n",
      "+--------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in Python\n",
    "from pyspark.sql.functions import corr\n",
    "#first_table.stat.corr(col(\"Wilderness_Area_1\"), col(\"Elevation(meters)\").cast('long'))\t\t## Method 1\n",
    "first_table.select(corr(col(\"Wilderness_Area_1\").cast('long'), col(\"Elevation(meters)\").cast('long'))).show()\t\t## Method 2\n",
    "\n",
    "#df.select(corr(\"Quantity\", \"UnitPrice\")).show()\t\t## Method 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c810f9-1db4-4d6a-99ae-6748576eabab",
   "metadata": {
    "tags": []
   },
   "source": [
    "Another common task is to compute summary statistics for a column or set of columns. We can\n",
    "use the \"describe\" method to achieve exactly this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d2c350aa-94a9-4213-b1d4-aef763c8dc60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+------+--------------------+---------+--------------------+------------------+\n",
      "|summary|       Qualification|        Experience|Rating|               Place|  Profile|  Miscellaneous_Info|              Fees|\n",
      "+-------+--------------------+------------------+------+--------------------+---------+--------------------+------------------+\n",
      "|  count|                 149|               149|    75|                 148|      149|                  92|               149|\n",
      "|   mean|                null|              null|  null|                null|     null|                null|298.32214765100673|\n",
      "| stddev|                null|              null|  null|                null|     null|                null|187.12043630197886|\n",
      "|    min|                BAMS|0 years experience|  100%|AS Rao Nagar, Hyd...| Ayurveda|1 Feedback Pallik...|               100|\n",
      "|    max|PhD - Orthodontic...|9 years experience|   99%|Yelahanka, Bangalore|Homeopath|Viral Fever Treat...|               800|\n",
      "+-------+--------------------+------------------+------+--------------------+---------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d1f177-eaf1-47ca-995b-bf1193d0b06a",
   "metadata": {},
   "source": [
    "If you need these exact numbers, you can also perform this as an aggregation yourself by\n",
    "importing the functions and applying them to the columns that you need:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "10be7b38-4ef5-40b4-b017-a48c17d5b226",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+\n",
      "|summary|   Slope(degrees)| Elevation(meters)|\n",
      "+-------+-----------------+------------------+\n",
      "|  count|            29050|             29050|\n",
      "|   mean|14.02853700516351| 2959.328330464716|\n",
      "| stddev|7.458200139042338|277.57822696412705|\n",
      "|    min|                0|              1879|\n",
      "|    max|                9|              3844|\n",
      "+-------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in Python\n",
    "from pyspark.sql.functions import count, mean, stddev_pop, min, max\n",
    "\n",
    "first_table.describe('Slope(degrees)','Elevation(meters)').show()\n",
    "#first_table.select(mean(col(\"Slope(degrees)\").cast('long'))).show()\t\t## Method 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57a6221-d7bb-4df9-8abd-e9cf807565f0",
   "metadata": {},
   "source": [
    "There are a number of statistical functions available in the StatFunctions Package (accessible\n",
    "using \"stat\" as we see in the code block below). These are DataFrame methods that you can use\n",
    "to calculate a variety of different things. For instance, you can calculate either exact or\n",
    "approximate quantiles of your data using the \"approxQuantile\" method:\n",
    "\n",
    "\n",
    "colName = \"Fees\"\n",
    "quantileProbs = [0.5]\n",
    "relError = 0.05\n",
    "first_table.stat.approxQuantile(\"Elevation(meters)\", quantileProbs, relError)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e143836-52c7-4332-b9e2-1a63abb2738e",
   "metadata": {},
   "source": [
    "You also can use this to see a cross-tabulation or frequent item pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4bebbecb-59da-4a3c-a715-2ccb8e3f4b44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|  Qualification_Fees|100|150|200|250|300|350|400| 50|500|600|650|700|750|800|\n",
      "+--------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|BDS, MDS - Oral &...|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|MDS-Oral Patholog...|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|MBBS, MD - Dermat...|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  1|\n",
      "|BHMS, Diploma In ...|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|          MBBS, AFIH|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|MBBS, DDVL, MD - ...|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|\n",
      "|BHMS, MD - Homeop...|  1|  0|  0|  1|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|\n",
      "|                BHMS|  2|  1|  3|  0|  0|  1|  0|  0|  2|  0|  0|  0|  0|  0|\n",
      "|                 BDS|  2|  0|  4|  4|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|\n",
      "|MD - Ayurveda Med...|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|\n",
      "|MD - Homeopathy, ...|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|\n",
      "|MBBS, MF- Homeopathy|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|BDS, MDS - Conser...|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|\n",
      "|MBBS, MD - Dermat...|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  1|  0|  0|\n",
      "|   MBBS, IBCLC (USA)|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|\n",
      "| BAMS, MS - Ayurveda|  0|  1|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|BDS, MDS - Paedod...|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|PhD - Orthodontic...|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|BAMS, MD - Ayurve...|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|MDS - Periodontol...|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "+--------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------+--------+-------+--------------+--------------+----------------+---------+\n",
      "|Rating_Profile|Ayurveda|Dentist|Dermatologists|ENT Specialist|General Medicine|Homeopath|\n",
      "+--------------+--------+-------+--------------+--------------+----------------+---------+\n",
      "|           87%|       0|      0|             1|             0|               0|        0|\n",
      "|          null|      12|     14|             9|             9|              20|       10|\n",
      "|           82%|       0|      0|             0|             0|               0|        1|\n",
      "|           80%|       0|      0|             0|             1|               0|        0|\n",
      "|           97%|       0|      0|             5|             0|               0|        0|\n",
      "|           78%|       0|      0|             1|             0|               0|        0|\n",
      "|           98%|       1|      6|             0|             0|               0|        0|\n",
      "|           88%|       0|      0|             0|             0|               1|        0|\n",
      "|           79%|       0|      0|             0|             1|               0|        0|\n",
      "|           36%|       0|      0|             0|             1|               0|        0|\n",
      "|          100%|       4|     15|             0|             2|               3|        5|\n",
      "|           90%|       0|      0|             2|             1|               0|        1|\n",
      "|           89%|       0|      0|             2|             0|               1|        0|\n",
      "|           86%|       0|      0|             0|             1|               0|        0|\n",
      "|           93%|       0|      1|             1|             0|               1|        0|\n",
      "|           94%|       0|      0|             1|             1|               0|        0|\n",
      "|           95%|       0|      0|             2|             0|               1|        0|\n",
      "|           99%|       0|      4|             1|             1|               1|        1|\n",
      "|           74%|       0|      0|             0|             1|               0|        0|\n",
      "|           96%|       1|      0|             0|             0|               1|        0|\n",
      "+--------------+--------+-------+--------------+--------------+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in Python\n",
    "data_table.stat.crosstab(\"Qualification\", \"Fees\").show()\n",
    "data_table.stat.crosstab(\"Rating\", \"Profile\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "27207690-dc9e-4d34-b50e-c906184b3aa3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------+\n",
      "|Rating_freqItems                                                                                      |\n",
      "+------------------------------------------------------------------------------------------------------+\n",
      "|[79%, 80%, 97%, 82%, 94%, 99%, 78%, 96%, 36%, 87%, 93%, 90%, 89%, 95%, 100%, 98%, 74%, 86%, 88%, null]|\n",
      "+------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "+------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "|Rating_freqItems                                                                                      |Profile_freqItems                                                               |\n",
      "+------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "|[79%, 80%, 97%, 82%, 94%, 99%, 78%, 96%, 36%, 87%, 93%, 90%, 89%, 95%, 100%, 98%, 74%, 86%, 88%, null]|[ENT Specialist, General Medicine, Homeopath, Dermatologists, Ayurveda, Dentist]|\n",
      "+------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "\n",
      "+------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+---------------------------------------------------------------------+\n",
      "|Rating_freqItems                                                                                      |Profile_freqItems                                                               |Fees_freqItems                                                       |\n",
      "+------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+---------------------------------------------------------------------+\n",
      "|[79%, 80%, 97%, 82%, 94%, 99%, 78%, 96%, 36%, 87%, 93%, 90%, 89%, 95%, 100%, 98%, 74%, 86%, 88%, null]|[ENT Specialist, General Medicine, Homeopath, Dermatologists, Ayurveda, Dentist]|[600, 650, 350, 500, 800, 200, 250, 50, 400, 700, 100, 750, 150, 300]|\n",
      "+------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+---------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in Python\n",
    "data_table.stat.freqItems([\"Rating\"]).show(20,False)\n",
    "data_table.stat.freqItems([\"Rating\",\"Profile\"]).show(20,False)\n",
    "data_table.stat.freqItems([\"Rating\",\"Profile\",\"Fees\"]).show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520d1e7e-30c0-4035-a2d4-cac208aa23ef",
   "metadata": {},
   "source": [
    "we can also add a unique ID to each row by using the function\n",
    "\"monotonically_increasing_id\". This function generates a unique value for each row, starting\n",
    "with 0:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "11345d30-e787-4ce1-8814-0aeef71d6475",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+-----------------------------+\n",
      "|       Qualification|         Experience|Rating|               Place|         Profile|  Miscellaneous_Info|Fees|monotonically_increasing_id()|\n",
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+-----------------------------+\n",
      "|BHMS, MD - Homeop...|24 years experience|  100%| Kakkanad, Ernakulam|       Homeopath|100% 16 Feedback ...| 100|                            0|\n",
      "|BAMS, MD - Ayurve...|12 years experience|   98%|Whitefield, Banga...|        Ayurveda|98% 76 Feedback W...| 350|                            1|\n",
      "|MBBS, MS - Otorhi...| 9 years experience|  null|Mathikere - BEL, ...|  ENT Specialist|                null| 300|                            2|\n",
      "| BSc - Zoology, BAMS|12 years experience|  null|Bannerghatta Road...|        Ayurveda|Bannerghatta Road...| 250|                            3|\n",
      "|                BAMS|20 years experience|  100%|Keelkattalai, Che...|        Ayurveda|100% 4 Feedback K...| 250|                            4|\n",
      "|                BAMS| 8 years experience|  null|      Porur, Chennai|        Ayurveda|                null| 100|                            5|\n",
      "|                BHMS|42 years experience|  null|   Karol Bagh, Delhi|       Homeopath|                null| 200|                            6|\n",
      "|                 BDS|10 years experience|   99%|  Arekere, Bangalore|         Dentist|Dental Fillings C...| 200|                            7|\n",
      "|MBBS, MD - Genera...|14 years experience|  null| Old City, Hyderabad|General Medicine|                null| 100|                            8|\n",
      "|            BSc, BDS|23 years experience|  null|   Athani, Ernakulam|         Dentist|                null| 100|                            9|\n",
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+-----------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in Python\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "data_table.select('*',monotonically_increasing_id()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac346633-63b8-4eee-894c-cda2a591f635",
   "metadata": {},
   "source": [
    "# Working with Strings-\n",
    "\n",
    "String manipulation shows up in nearly every data flow, and it’s worth explaining what you can\n",
    "do with strings. You might be manipulating log files performing regular expression extraction or\n",
    "substitution, or checking for simple string existence, or making all strings uppercase or\n",
    "lowercase.\n",
    "\n",
    "The \"initcap\" function will capitalize every word in a given string when that word is separated from another by a space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a4bfe892-2e59-4cf0-9d25-bd137a807463",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------+-------------------+-------------------+\n",
      "|       Qualification|initcap(Qualification)|         Experience|initcap(Experience)|\n",
      "+--------------------+----------------------+-------------------+-------------------+\n",
      "|BHMS, MD - Homeop...|  Bhms, Md - Homeop...|24 years experience|24 Years Experience|\n",
      "|BAMS, MD - Ayurve...|  Bams, Md - Ayurve...|12 years experience|12 Years Experience|\n",
      "|MBBS, MS - Otorhi...|  Mbbs, Ms - Otorhi...| 9 years experience| 9 Years Experience|\n",
      "| BSc - Zoology, BAMS|   Bsc - Zoology, Bams|12 years experience|12 Years Experience|\n",
      "|                BAMS|                  Bams|20 years experience|20 Years Experience|\n",
      "|                BAMS|                  Bams| 8 years experience| 8 Years Experience|\n",
      "|                BHMS|                  Bhms|42 years experience|42 Years Experience|\n",
      "|                 BDS|                   Bds|10 years experience|10 Years Experience|\n",
      "|MBBS, MD - Genera...|  Mbbs, Md - Genera...|14 years experience|14 Years Experience|\n",
      "|            BSc, BDS|              Bsc, Bds|23 years experience|23 Years Experience|\n",
      "| MBBS, MS, DNB - ENT|   Mbbs, Ms, Dnb - Ent| 5 years experience| 5 Years Experience|\n",
      "|                BAMS|                  Bams| 7 years experience| 7 Years Experience|\n",
      "|            BDS, MDS|              Bds, Mds| 9 years experience| 9 Years Experience|\n",
      "|BDS, MDS - Oral &...|  Bds, Mds - Oral &...|21 years experience|21 Years Experience|\n",
      "|MBBS, Diploma in ...|  Mbbs, Diploma In ...|12 years experience|12 Years Experience|\n",
      "+--------------------+----------------------+-------------------+-------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import initcap\n",
    "data_table.select(col(\"Qualification\"),initcap(col(\"Qualification\")),col('Experience'),initcap(col(\"Experience\"))).show(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccffb51d-9f0b-4c0d-b66c-878b4609c26c",
   "metadata": {},
   "source": [
    "As just mentioned, you can cast strings in uppercase and lowercase, as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2b7b049b-c2bd-40b0-a204-f4f9814efabf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------------------------------+------------------------------+\n",
      "|Qualification                 |lower(Qualification)          |upper(lower(Qualification))   |\n",
      "+------------------------------+------------------------------+------------------------------+\n",
      "|BHMS, MD - Homeopathy         |bhms, md - homeopathy         |BHMS, MD - HOMEOPATHY         |\n",
      "|BAMS, MD - Ayurveda Medicine  |bams, md - ayurveda medicine  |BAMS, MD - AYURVEDA MEDICINE  |\n",
      "|MBBS, MS - Otorhinolaryngology|mbbs, ms - otorhinolaryngology|MBBS, MS - OTORHINOLARYNGOLOGY|\n",
      "|BSc - Zoology, BAMS           |bsc - zoology, bams           |BSC - ZOOLOGY, BAMS           |\n",
      "|BAMS                          |bams                          |BAMS                          |\n",
      "+------------------------------+------------------------------+------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower, upper\n",
    "data_table.select(col(\"Qualification\"),lower(col(\"Qualification\")),upper(lower(col(\"Qualification\")))).show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68131ac7-328c-4f4a-9f2f-d9ce15fe5970",
   "metadata": {},
   "source": [
    "Another trivial task is adding or removing spaces around a string. \n",
    "You can do this by using \"lpad\", \"ltrim\", \"rpad\" and \"rtrim\", \"trim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6ddfe668-df1c-463a-a04e-c424b913b50c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+---+----------+\n",
      "| ltrim| rtrim| trim| lp|        rp|\n",
      "+------+------+-----+---+----------+\n",
      "|HELLO | HELLO|HELLO|HEL|HELLO     |\n",
      "|HELLO | HELLO|HELLO|HEL|HELLO     |\n",
      "|HELLO | HELLO|HELLO|HEL|HELLO     |\n",
      "|HELLO | HELLO|HELLO|HEL|HELLO     |\n",
      "|HELLO | HELLO|HELLO|HEL|HELLO     |\n",
      "|HELLO | HELLO|HELLO|HEL|HELLO     |\n",
      "|HELLO | HELLO|HELLO|HEL|HELLO     |\n",
      "|HELLO | HELLO|HELLO|HEL|HELLO     |\n",
      "|HELLO | HELLO|HELLO|HEL|HELLO     |\n",
      "|HELLO | HELLO|HELLO|HEL|HELLO     |\n",
      "+------+------+-----+---+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim\n",
    "data_table.select(\n",
    "ltrim(lit(\" HELLO \")).alias(\"ltrim\"),\n",
    "rtrim(lit(\" HELLO \")).alias(\"rtrim\"),\n",
    "trim(lit(\" HELLO \")).alias(\"trim\"),\n",
    "lpad(lit(\"HELLO\"), 3, \" \").alias(\"lp\"),\n",
    "rpad(lit(\"HELLO\"), 10, \" \").alias(\"rp\")).show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dd1f51-b5e5-467d-9bb4-d18531c0517b",
   "metadata": {},
   "source": [
    "## Regular Expressions-\n",
    "\n",
    "Regular Expressions are most frequently performed tasks for searching the existence of one string\n",
    "in another or replacing all mentions of a string with another value. It gives\n",
    "the user an ability to specify a set of rules to use to either extract values from a string or replace\n",
    "them with some other values.\n",
    "\n",
    "There are two key functions in Spark that you’ll need in\n",
    "order to perform regular expression tasks: \"regexp_extract()\" and \"regexp_replace()\". These functions extract values and replace values, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "42fb8e8f-6bf4-4c1d-b543-ac7581c097a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------+-----------------------------------------------------+\n",
      "|New_one_Qualification                                      |Qualification                                        |\n",
      "+-----------------------------------------------------------+-----------------------------------------------------+\n",
      "|BHMS, GUCCI MANE - Homeopathy                              |BHMS, MD - Homeopathy                                |\n",
      "|BAMS, GUCCI MANE - Ayurveda Medicine                       |BAMS, MD - Ayurveda Medicine                         |\n",
      "|GUCCI MANE, MS - Otorhinolaryngology                       |MBBS, MS - Otorhinolaryngology                       |\n",
      "|BSc - Zoology, BAMS                                        |BSc - Zoology, BAMS                                  |\n",
      "|BAMS                                                       |BAMS                                                 |\n",
      "|BAMS                                                       |BAMS                                                 |\n",
      "|BHMS                                                       |BHMS                                                 |\n",
      "|GUCCI MANE                                                 |BDS                                                  |\n",
      "|GUCCI MANE, GUCCI MANE - General Medicine                  |MBBS, MD - General Medicine                          |\n",
      "|BSc, GUCCI MANE                                            |BSc, BDS                                             |\n",
      "|GUCCI MANE, MS, DNB - ENT                                  |MBBS, MS, DNB - ENT                                  |\n",
      "|BAMS                                                       |BAMS                                                 |\n",
      "|GUCCI MANE, GUCCI MANES                                    |BDS, MDS                                             |\n",
      "|GUCCI MANE, GUCCI MANES - Oral & Maxillofacial Surgery     |BDS, MDS - Oral & Maxillofacial Surgery              |\n",
      "|GUCCI MANE, Diploma in Otorhinolaryngology (DLO), DNB - ENT|MBBS, Diploma in Otorhinolaryngology (DLO), DNB - ENT|\n",
      "|GUCCI MANE, GUCCI MANE - General Medicine                  |MBBS, MD - General Medicine                          |\n",
      "|GUCCI MANE, Diploma in Otorhinolaryngology (DLO)           |MBBS, Diploma in Otorhinolaryngology (DLO)           |\n",
      "|GUCCI MANE, MF- Homeopathy                                 |MBBS, MF- Homeopathy                                 |\n",
      "|GUCCI MANE, MS - ENT                                       |MBBS, MS - ENT                                       |\n",
      "|GUCCI MANE                                                 |MBBS                                                 |\n",
      "+-----------------------------------------------------------+-----------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search for \"MBBS\", \"MD\" and \"BDS\" word from column \"Qualification\" and replacing them with \"GUCCI MANE\" word\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace,regexp_extract\n",
    "regex_string = \"MBBS|MD|BDS\"          \n",
    "data_table.select(\n",
    "regexp_replace(col(\"Qualification\"), regex_string, \"GUCCI MANE\").alias(\"New_one_Qualification\"),col(\"Qualification\")).show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "952ccb15-6b04-493b-8a7b-aa8a5eb4e86d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----------------------------------------------------+-----------------------------------------------------+\n",
      "|Rating|Fees|Qualification                                        |Qualificationnew                                     |\n",
      "+------+----+-----------------------------------------------------+-----------------------------------------------------+\n",
      "|100%  |100 |BHMS, MD - Homeopathy                                |BHMS, MD - NONONHOEOPATHY                            |\n",
      "|98%   |350 |BAMS, MD - Ayurveda Medicine                         |BAMS, MD - Ayurveda COMPOUNDER                       |\n",
      "|null  |300 |MBBS, MS - Otorhinolaryngology                       |MBBS, MS - Otorhinolaryngology                       |\n",
      "|null  |250 |BSc - Zoology, BAMS                                  |BSc - Zoology, BAMS                                  |\n",
      "|100%  |250 |BAMS                                                 |BAMS                                                 |\n",
      "|null  |100 |BAMS                                                 |BAMS                                                 |\n",
      "|null  |200 |BHMS                                                 |BHMS                                                 |\n",
      "|99%   |200 |BDS                                                  |BDS                                                  |\n",
      "|null  |100 |MBBS, MD - General Medicine                          |MBBS, MD - General COMPOUNDER                        |\n",
      "|null  |100 |BSc, BDS                                             |BSc, BDS                                             |\n",
      "|null  |700 |MBBS, MS, DNB - ENT                                  |MBBS, MS, DNB - ENT                                  |\n",
      "|null  |100 |BAMS                                                 |BAMS                                                 |\n",
      "|98%   |200 |BDS, MDS                                             |BDS, MDS                                             |\n",
      "|null  |350 |BDS, MDS - Oral & Maxillofacial Surgery              |BDS, MDS - Oral & Maxillofacial Surgery              |\n",
      "|null  |500 |MBBS, Diploma in Otorhinolaryngology (DLO), DNB - ENT|MBBS, Diploma in Otorhinolaryngology (DLO), DNB - ENT|\n",
      "|null  |200 |MBBS, MD - General Medicine                          |MBBS, MD - General COMPOUNDER                        |\n",
      "|null  |100 |MBBS, Diploma in Otorhinolaryngology (DLO)           |MBBS, Diploma in Otorhinolaryngology (DLO)           |\n",
      "|null  |300 |MBBS, MF- Homeopathy                                 |MBBS, MF- NONONHOEOPATHY                             |\n",
      "|79%   |400 |MBBS, MS - ENT                                       |MBBS, MS - ENT                                       |\n",
      "|100%  |150 |MBBS                                                 |MBBS                                                 |\n",
      "+------+----+-----------------------------------------------------+-----------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Replace string column value conditionally\n",
    "from pyspark.sql.functions import when\n",
    "data_table.withColumn('Qualificationnew', \n",
    "    when(data_table.Qualification.endswith('Homeopathy'),regexp_replace(data_table.Qualification,'Homeopathy','NONONHOEOPATHY')) \\\n",
    "   .when(data_table.Qualification.endswith('Medicine'),regexp_replace(data_table.Qualification,'Medicine','COMPOUNDER')) \\\n",
    "   .otherwise(data_table.Qualification)) \\\n",
    "   .select('Rating','Fees' ,'Qualification','Qualificationnew').show(20,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ebc3ab-c98b-4ce7-87df-57f50a82817b",
   "metadata": {},
   "source": [
    "Another task might be to replace given characters with other characters. Building this as a\n",
    "regular expression could be tedious, so Spark also provides the \"translate()\" function to replace these\n",
    "values. This is done at the character level and will replace all instances of a character with the\n",
    "indexed character in the replacement string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "81c688f3-afd5-4c22-8698-0883887e70fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------+-----------------------------------------------------+\n",
      "|translate(Qualification, -, &)                       |Qualification                                        |\n",
      "+-----------------------------------------------------+-----------------------------------------------------+\n",
      "|BHMS, MD & Homeopathy                                |BHMS, MD - Homeopathy                                |\n",
      "|BAMS, MD & Ayurveda Medicine                         |BAMS, MD - Ayurveda Medicine                         |\n",
      "|MBBS, MS & Otorhinolaryngology                       |MBBS, MS - Otorhinolaryngology                       |\n",
      "|BSc & Zoology, BAMS                                  |BSc - Zoology, BAMS                                  |\n",
      "|BAMS                                                 |BAMS                                                 |\n",
      "|BAMS                                                 |BAMS                                                 |\n",
      "|BHMS                                                 |BHMS                                                 |\n",
      "|BDS                                                  |BDS                                                  |\n",
      "|MBBS, MD & General Medicine                          |MBBS, MD - General Medicine                          |\n",
      "|BSc, BDS                                             |BSc, BDS                                             |\n",
      "|MBBS, MS, DNB & ENT                                  |MBBS, MS, DNB - ENT                                  |\n",
      "|BAMS                                                 |BAMS                                                 |\n",
      "|BDS, MDS                                             |BDS, MDS                                             |\n",
      "|BDS, MDS & Oral & Maxillofacial Surgery              |BDS, MDS - Oral & Maxillofacial Surgery              |\n",
      "|MBBS, Diploma in Otorhinolaryngology (DLO), DNB & ENT|MBBS, Diploma in Otorhinolaryngology (DLO), DNB - ENT|\n",
      "|MBBS, MD & General Medicine                          |MBBS, MD - General Medicine                          |\n",
      "|MBBS, Diploma in Otorhinolaryngology (DLO)           |MBBS, Diploma in Otorhinolaryngology (DLO)           |\n",
      "|MBBS, MF& Homeopathy                                 |MBBS, MF- Homeopathy                                 |\n",
      "|MBBS, MS & ENT                                       |MBBS, MS - ENT                                       |\n",
      "|MBBS                                                 |MBBS                                                 |\n",
      "+-----------------------------------------------------+-----------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Replacing \"-\" with \"&\" from Qualification column\n",
    "\n",
    "from pyspark.sql.functions import translate\n",
    "data_table.select(translate(col(\"Qualification\"), \"-\", \"&\"),col(\"Qualification\")).show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a87ea75-faef-46e4-b36c-584da875bbaf",
   "metadata": {},
   "source": [
    "We can also perform something similar, like pulling out the first mentioned color:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "12334e53-0d96-4813-9d66-08d42af23957",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+--------------------+\n",
      "|New_extra_Qualification|       Qualification|\n",
      "+-----------------------+--------------------+\n",
      "|                     MS|BHMS, MD - Homeop...|\n",
      "|                     MS|BAMS, MD - Ayurve...|\n",
      "|                     MS|MBBS, MS - Otorhi...|\n",
      "|                     MS| BSc - Zoology, BAMS|\n",
      "|                     MS|                BAMS|\n",
      "|                     MS|                BAMS|\n",
      "|                     MS|                BHMS|\n",
      "|                    BDS|                 BDS|\n",
      "|                       |MBBS, MD - Genera...|\n",
      "|                    BDS|            BSc, BDS|\n",
      "|                     MS| MBBS, MS, DNB - ENT|\n",
      "|                     MS|                BAMS|\n",
      "|                    BDS|            BDS, MDS|\n",
      "|                    BDS|BDS, MDS - Oral &...|\n",
      "|                Diploma|MBBS, Diploma in ...|\n",
      "|                       |MBBS, MD - Genera...|\n",
      "|                Diploma|MBBS, Diploma in ...|\n",
      "|                       |MBBS, MF- Homeopathy|\n",
      "|                     MS|      MBBS, MS - ENT|\n",
      "|                       |                MBBS|\n",
      "+-----------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "extract_str = \"(Diploma|MS|BDS)\"\n",
    "data_table.select(regexp_extract(col(\"Qualification\"), extract_str, 1).alias(\"New_extra_Qualification\"),col(\"Qualification\")).show(20,False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd90193-1eab-4b6c-94f0-b0a9373f5cf0",
   "metadata": {},
   "source": [
    "# Working with Dates and Timestamps-\n",
    "\n",
    "The key to understanding the transformations that you are going to need to apply is to ensure that you know\n",
    "exactly what type and format you have at each given step of the way. Spark’s \"TimestampType\" class supports only second-level precision, which means that if\n",
    "you’re going to be working with milliseconds or microseconds, you’ll need to work around this\n",
    "problem by potentially operating on them as \"longs\". Any more precision when coercing to a\n",
    "TimestampType will be removed.\n",
    "\n",
    "Let’s begin with the basics and get the current date and the current timestamps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a48c4cb5-7d45-46af-8fd7-893e4971714f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n",
      "+---+----------+--------------------+\n",
      "| id|     today|                 now|\n",
      "+---+----------+--------------------+\n",
      "|  0|2023-05-02|2023-05-02 23:42:...|\n",
      "|  1|2023-05-02|2023-05-02 23:42:...|\n",
      "|  2|2023-05-02|2023-05-02 23:42:...|\n",
      "|  3|2023-05-02|2023-05-02 23:42:...|\n",
      "|  4|2023-05-02|2023-05-02 23:42:...|\n",
      "|  5|2023-05-02|2023-05-02 23:42:...|\n",
      "|  6|2023-05-02|2023-05-02 23:42:...|\n",
      "|  7|2023-05-02|2023-05-02 23:42:...|\n",
      "|  8|2023-05-02|2023-05-02 23:42:...|\n",
      "|  9|2023-05-02|2023-05-02 23:42:...|\n",
      "+---+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "dateDF = spark.range(10).withColumn(\"today\", current_date()).withColumn(\"now\", current_timestamp())         \n",
    "\n",
    "#\"current_date()\" method is used to get current date\n",
    "#\"current_timestamp()\" method is used to get current time alongwith date\n",
    "\n",
    "dateDF.createOrReplaceTempView(\"dateTable\")\n",
    "dateDF.printSchema()\n",
    "dateDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6df8cf9a-2cc6-4791-a7de-620782d01459",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+\n",
      "| id|     today|                 now|\n",
      "+---+----------+--------------------+\n",
      "|  0|2023-05-02|2023-05-02 23:42:...|\n",
      "|  1|2023-05-02|2023-05-02 23:42:...|\n",
      "|  2|2023-05-02|2023-05-02 23:42:...|\n",
      "|  3|2023-05-02|2023-05-02 23:42:...|\n",
      "|  4|2023-05-02|2023-05-02 23:42:...|\n",
      "|  5|2023-05-02|2023-05-02 23:42:...|\n",
      "|  6|2023-05-02|2023-05-02 23:42:...|\n",
      "|  7|2023-05-02|2023-05-02 23:42:...|\n",
      "|  8|2023-05-02|2023-05-02 23:42:...|\n",
      "|  9|2023-05-02|2023-05-02 23:42:...|\n",
      "+---+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.createOrReplaceTempView(\"Table_NEW\")\n",
    "spark.sql(\"\"\"select * from Table_NEW\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2b108d-4473-4ba3-ae2a-d59e8a852c02",
   "metadata": {},
   "source": [
    "Now that we have a simple DataFrame to work with, let’s add and subtract five days from today.\n",
    "These functions take a column and then the number of days to either add or subtract as the\n",
    "arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b52067c6-51fb-49e3-ab56-544afede8bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+\n",
      "|     today|date_sub(today, 5)|date_add(today, 5)|\n",
      "+----------+------------------+------------------+\n",
      "|2023-05-02|        2023-04-27|        2023-05-07|\n",
      "|2023-05-02|        2023-04-27|        2023-05-07|\n",
      "|2023-05-02|        2023-04-27|        2023-05-07|\n",
      "|2023-05-02|        2023-04-27|        2023-05-07|\n",
      "|2023-05-02|        2023-04-27|        2023-05-07|\n",
      "|2023-05-02|        2023-04-27|        2023-05-07|\n",
      "|2023-05-02|        2023-04-27|        2023-05-07|\n",
      "|2023-05-02|        2023-04-27|        2023-05-07|\n",
      "|2023-05-02|        2023-04-27|        2023-05-07|\n",
      "|2023-05-02|        2023-04-27|        2023-05-07|\n",
      "+----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub\n",
    "dateDF.select(col(\"today\"),date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5)).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c152666c-2d26-4aff-ad19-0ebd35e1b8ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+\n",
      "|     today|date_sub(today, 5)|date_add(today, 5)|\n",
      "+----------+------------------+------------------+\n",
      "|2023-05-02|        2023-04-27|        2023-05-07|\n",
      "|2023-05-02|        2023-04-27|        2023-05-07|\n",
      "|2023-05-02|        2023-04-27|        2023-05-07|\n",
      "|2023-05-02|        2023-04-27|        2023-05-07|\n",
      "|2023-05-02|        2023-04-27|        2023-05-07|\n",
      "|2023-05-02|        2023-04-27|        2023-05-07|\n",
      "|2023-05-02|        2023-04-27|        2023-05-07|\n",
      "|2023-05-02|        2023-04-27|        2023-05-07|\n",
      "|2023-05-02|        2023-04-27|        2023-05-07|\n",
      "|2023-05-02|        2023-04-27|        2023-05-07|\n",
      "+----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.createOrReplaceTempView(\"Table_NEW\")\n",
    "spark.sql(\"\"\"select today, today - 5, today + 5  from Table_NEW\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbe90fb-7bb3-45dc-9326-e4dc933c0784",
   "metadata": {},
   "source": [
    "\"datediff()\" function that will return the number of days in between two dates. \"months_between()\" gives you the number of months between two dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5dfb25c0-8a68-46fd-929e-7a23bc92197c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|datediff(week_ago, today)|\n",
      "+-------------------------+\n",
      "|                       -7|\n",
      "+-------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------------------------+\n",
      "|months_between(start, end, true)|\n",
      "+--------------------------------+\n",
      "|                    -16.67741935|\n",
      "+--------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff, months_between, to_date\n",
    "dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\\\n",
    ".select(datediff(col(\"week_ago\"), col(\"today\"))).show(1)        # \"datediff()\" method is used to get no. of days difference b/w two dates\n",
    "\n",
    "dateDF.select(\n",
    "to_date(lit(\"2016-01-01\")).alias(\"start\"),\n",
    "to_date(lit(\"2017-05-22\")).alias(\"end\"))\\\n",
    ".select(months_between(col(\"start\"), col(\"end\"))).show(1)       # \"months_between()\" method is used to get no. of months in b/w two dates provided"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08bf4bd-a9be-410a-9a7c-0b0cb6ee3f39",
   "metadata": {},
   "source": [
    "The \"to_date()\" function allows you to convert a string to a date.\n",
    "\n",
    "##### NOTE - Spark will not throw an error if it cannot parse the date; rather, it will just return null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4e635b51-7c25-4ec8-b413-ee9ebaf533e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------------------------------+--------------------------------+\n",
      "|to_date(2016-01-01)|months_between(2016-01-01, 2017-01-01, true)|datediff(2016-01-01, 2017-01-01)|\n",
      "+-------------------+--------------------------------------------+--------------------------------+\n",
      "|         2016-01-01|                                       -12.0|                            -366|\n",
      "|         2016-01-01|                                       -12.0|                            -366|\n",
      "|         2016-01-01|                                       -12.0|                            -366|\n",
      "|         2016-01-01|                                       -12.0|                            -366|\n",
      "|         2016-01-01|                                       -12.0|                            -366|\n",
      "|         2016-01-01|                                       -12.0|                            -366|\n",
      "|         2016-01-01|                                       -12.0|                            -366|\n",
      "|         2016-01-01|                                       -12.0|                            -366|\n",
      "|         2016-01-01|                                       -12.0|                            -366|\n",
      "|         2016-01-01|                                       -12.0|                            -366|\n",
      "+-------------------+--------------------------------------------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.createOrReplaceTempView(\"dateTable\")\n",
    "spark.sql(\"\"\"SELECT to_date('2016-01-01'), months_between('2016-01-01', '2017-01-01'),\n",
    "datediff('2016-01-01', '2017-01-01')\n",
    "FROM dateTable\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d7542a87-03dc-4164-ac06-7c36b239ac39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- date: string (nullable = false)\n",
      "\n",
      "root\n",
      " |-- to_date(date): date (nullable = true)\n",
      "\n",
      "+-------------+\n",
      "|to_date(date)|\n",
      "+-------------+\n",
      "|   2017-01-01|\n",
      "+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+---------------+---------------+\n",
      "|to_date(date_1)|to_date(date_2)|\n",
      "+---------------+---------------+\n",
      "|     2017-01-01|           null|\n",
      "+---------------+---------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, lit\n",
    "spark.range(5).withColumn(\"date\", lit(\"2017-01-01\")).printSchema()\n",
    "spark.range(5).withColumn(\"date\", lit(\"2017-01-01\")).select(to_date(col(\"date\"))).printSchema()\n",
    "spark.range(5).withColumn(\"date\", lit(\"2017-01-01\")).select(to_date(col(\"date\"))).show(1)\n",
    "\n",
    "\n",
    "spark.range(5).withColumn(\"date_1\", lit(\"2017-01-01\")).withColumn(\"date_2\", lit(\"2017-99-99\")).\\\n",
    "select(to_date(col(\"date_1\")), to_date(col(\"date_2\"))).show(1)     # Since \"2017-99-99\" is an incorrect date, therefore system is displaying value as \"null\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f4b0a60a-3721-46f2-af06-3eacee12c56a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+--------------------------+-------------+\n",
      "|to_date(date, yyyy-dd-MM)|to_date(date2, yyyy-dd-MM)|to_date(date)|\n",
      "+-------------------------+--------------------------+-------------+\n",
      "|               2017-11-12|                2017-12-20|   2017-11-12|\n",
      "+-------------------------+--------------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF.createOrReplaceTempView(\"dateTable2\")\n",
    "\n",
    "spark.sql(\"\"\"SELECT to_date(date, 'yyyy-dd-MM'), to_date(date2, 'yyyy-dd-MM'), to_date(date)\n",
    "FROM dateTable2\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a817dbc-4bdc-4f15-8199-16b3cb7dd6f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "We find this to be an especially tricky situation for bugs because some dates might match the\n",
    "correct format, whereas others do not.\n",
    "\n",
    "We will use two functions to fix this: \"to_date()\" and \"to_timestamp()\"  method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0d39a6bc-b980-4fbf-8c04-556bc59a8d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "dateFormat = \"yyyy-dd-MM\"\n",
    "cleanDateDF = spark.range(1).select(\n",
    "to_date(lit(\"2017-12-11\"), dateFormat).alias(\"date\"),\n",
    "to_date(lit(\"2017-20-12\"), dateFormat).alias(\"date2\"))\n",
    "\n",
    "cleanDateDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "aad33a9d-f382-4929-97fc-832d5a79f984",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+--------------------------+-------------+\n",
      "|to_date(date, yyyy-dd-MM)|to_date(date2, yyyy-dd-MM)|to_date(date)|\n",
      "+-------------------------+--------------------------+-------------+\n",
      "|               2017-11-12|                2017-12-20|   2017-11-12|\n",
      "+-------------------------+--------------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF.createOrReplaceTempView(\"dateTable2\")\n",
    "\n",
    "spark.sql(\"\"\"SELECT to_date(date, 'yyyy-dd-MM'), to_date(date2, 'yyyy-dd-MM'), to_date(date)\n",
    "\n",
    "FROM dateTable2\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e816a81c-49f2-4521-9755-ac74dd62fe4a",
   "metadata": {},
   "source": [
    "Now let’s use an example of \"to_timestamp()\", which always requires a format to be specified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "65388626-15c6-4a48-bcde-3edf8370b814",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|to_timestamp(date, yyyy-dd-MM)|\n",
      "+------------------------------+\n",
      "|           2017-11-12 00:00:00|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "dateFormat = \"yyyy-dd-MM\"\n",
    "cleanDateDF.select(to_timestamp(col(\"date\"), dateFormat)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "06bb6f45-b640-437e-8737-995f023cfda1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------------------+----------+-------------------------------+\n",
      "|      date|to_timestamp(date, yyyy-MM-dd)|     date2|to_timestamp(date2, yyyy-dd-MM)|\n",
      "+----------+------------------------------+----------+-------------------------------+\n",
      "|2017-11-12|           2017-11-12 00:00:00|2017-12-20|            2017-12-20 00:00:00|\n",
      "+----------+------------------------------+----------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## BELOW CODE NOT FUNCTIONING AS REQUIRED.....NEED SOME RCA\n",
    "\n",
    "cleanDateDF.createOrReplaceTempView(\"dateTable2\")\n",
    "\n",
    "spark.sql(\"\"\"SELECT date, to_timestamp(date, 'yyyy-MM-dd'), date2, to_timestamp(date2, 'yyyy-dd-MM') \n",
    "\n",
    "FROM dateTable2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c582e39a-389d-4daf-ba33-4037177c7688",
   "metadata": {},
   "source": [
    "After we have our date or timestamp in the correct format and type, comparing between them is\n",
    "actually quite easy. We just need to be sure to either use a date/timestamp type or specify our\n",
    "string according to the right format of yyyy-MM-dd if we’re comparing a date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "aa1ffc58-e68d-41f5-ad11-939e55fe628d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n",
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n",
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF.filter(col(\"date2\") > to_date(lit(\"2017-12-12\"))).show()\n",
    "\n",
    "cleanDateDF.filter(col(\"date2\") > lit(\"2017-12-12\")).show()\n",
    "\n",
    "#One minor point is that we can also set this as a string, which Spark parses to a literal:\n",
    "\n",
    "cleanDateDF.filter(col(\"date2\") > \"2017-12-12\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783b3c2b-d613-4cfa-a0a0-ff9c18a748fd",
   "metadata": {},
   "source": [
    "# Working with Nulls in Data-\n",
    "\n",
    "You should always use nulls to represent missing or empty data in your\n",
    "DataFrames. Spark can optimize working with null values more than it can if you use empty\n",
    "strings or other values.\n",
    "\n",
    "Use the \".na\" subpackage on a DataFrame for interacting with null values.\n",
    "\n",
    "\n",
    "There are two things you can do with null values: you can explicitly drop nulls or you can fill\n",
    "them with a value (globally or on a per-column basis). Let’s experiment with each of these now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0218d3-3343-44f0-89da-26b23260d37b",
   "metadata": {},
   "source": [
    "Coalesce()-\n",
    "Spark includes a function to allow you to select the first non-null value from a set of columns by\n",
    "using the coalesce function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c308e4bd-7dac-4c56-ad81-8fe87e2ed169",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------------------+\n",
      "|Rating|Fees|coalesce(Rating, Fees)|\n",
      "+------+----+----------------------+\n",
      "|  100%| 100|                  100%|\n",
      "|   98%| 350|                   98%|\n",
      "|  null| 300|                   300|\n",
      "|  null| 250|                   250|\n",
      "|  100%| 250|                  100%|\n",
      "|  null| 100|                   100|\n",
      "|  null| 200|                   200|\n",
      "|   99%| 200|                   99%|\n",
      "|  null| 100|                   100|\n",
      "|  null| 100|                   100|\n",
      "+------+----+----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "data_table.select(col(\"Rating\"), col(\"Fees\"),coalesce(col(\"Rating\"), col(\"Fees\"))).show(10)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dfc848-2700-4284-8852-1c0dc1ee6a1b",
   "metadata": {},
   "source": [
    "##### ifnull, nullif, nvl, and nvl2-\n",
    "\n",
    "There are several other SQL functions that you can use to achieve similar things.\n",
    "\n",
    "\"ifnull\" allows you to select the second value if the first is null, and defaults to the first.\n",
    "\n",
    "\"nullif\" returns null if the two values are equal or else returns the second if they are not.\n",
    "\n",
    "\"nvl\" returns the second value if the first is null, but defaults to the first.\n",
    "\n",
    "\"nvl2\" returns the second value if the first is not null; otherwise, it will return the last specified value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e269df6b-a162-450c-b9b9-eef3d7ac4748",
   "metadata": {
    "tags": []
   },
   "source": [
    "data_table.select(col(\"Rating\"), col(\"Fees\"),ifnull(col(\"Rating\"), col(\"Fees\"))).show(10)\n",
    "data_table.select(col(\"Rating\"), col(\"Fees\"),nullif(col(\"Rating\"), col(\"Fees\"))).show(10)    \n",
    "data_table.select(col(\"Rating\"), col(\"Fees\"),nvl(col(\"Rating\"), col(\"Fees\"))).show(10)    \n",
    "data_table.select(col(\"Rating\"), col(\"Fees\"),nvl2(col(\"Rating\"), col(\"Fees\"))).show(10)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "826f316f-189e-437e-8036-918fc4fe5d10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+------------+------------+\n",
      "|           a|   b|           c|           d|\n",
      "+------------+----+------------+------------+\n",
      "|return_value|null|return_value|return_value|\n",
      "+------------+----+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.createOrReplaceTempView(\"dfTable\")\n",
    "\n",
    "spark.sql(\"\"\" select ifnull(null, 'return_value') a,\n",
    "nullif('value', 'value') b,\n",
    "nvl(null, 'return_value') c,\n",
    "nvl2('not_null', 'return_value', \"else_value\") d\n",
    "FROM dfTable LIMIT 1\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78af0b3d-d498-4346-87c3-7742517f9a2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "drop-\n",
    "\"drop()\" removes rows that contain nulls. The default is to drop row in which any value is null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e01830c6-9c8d-47dc-b378-1cd955fc34e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+---------+--------------------+----+\n",
      "|       Qualification|         Experience|Rating|               Place|  Profile|  Miscellaneous_Info|Fees|\n",
      "+--------------------+-------------------+------+--------------------+---------+--------------------+----+\n",
      "|BHMS, MD - Homeop...|24 years experience|  100%| Kakkanad, Ernakulam|Homeopath|100% 16 Feedback ...| 100|\n",
      "|BAMS, MD - Ayurve...|12 years experience|   98%|Whitefield, Banga...| Ayurveda|98% 76 Feedback W...| 350|\n",
      "|                BAMS|20 years experience|  100%|Keelkattalai, Che...| Ayurveda|100% 4 Feedback K...| 250|\n",
      "|                 BDS|10 years experience|   99%|  Arekere, Bangalore|  Dentist|Dental Fillings C...| 200|\n",
      "|            BDS, MDS| 9 years experience|   98%|Coimbatore Raceco...|  Dentist|98% 14 Feedback C...| 200|\n",
      "+--------------------+-------------------+------+--------------------+---------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-------------------+------+--------------------+---------+--------------------+----+\n",
      "|       Qualification|         Experience|Rating|               Place|  Profile|  Miscellaneous_Info|Fees|\n",
      "+--------------------+-------------------+------+--------------------+---------+--------------------+----+\n",
      "|BHMS, MD - Homeop...|24 years experience|  100%| Kakkanad, Ernakulam|Homeopath|100% 16 Feedback ...| 100|\n",
      "|BAMS, MD - Ayurve...|12 years experience|   98%|Whitefield, Banga...| Ayurveda|98% 76 Feedback W...| 350|\n",
      "|                BAMS|20 years experience|  100%|Keelkattalai, Che...| Ayurveda|100% 4 Feedback K...| 250|\n",
      "|                 BDS|10 years experience|   99%|  Arekere, Bangalore|  Dentist|Dental Fillings C...| 200|\n",
      "|            BDS, MDS| 9 years experience|   98%|Coimbatore Raceco...|  Dentist|98% 14 Feedback C...| 200|\n",
      "+--------------------+-------------------+------+--------------------+---------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.na.drop().show(5)      ## \".na()\" drops all rows which has null in any column.\n",
    "\n",
    "data_table.na.drop(\"any\").show(5)  \t# Specifying \"any\" as an argument drops a row if any of the values are null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2c190ae0-5c10-4106-bca7-46629e974602",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+---------+--------------------+----+\n",
      "|       Qualification|         Experience|Rating|               Place|  Profile|  Miscellaneous_Info|Fees|\n",
      "+--------------------+-------------------+------+--------------------+---------+--------------------+----+\n",
      "|BHMS, MD - Homeop...|24 years experience|  100%| Kakkanad, Ernakulam|Homeopath|100% 16 Feedback ...| 100|\n",
      "|BAMS, MD - Ayurve...|12 years experience|   98%|Whitefield, Banga...| Ayurveda|98% 76 Feedback W...| 350|\n",
      "|                BAMS|20 years experience|  100%|Keelkattalai, Che...| Ayurveda|100% 4 Feedback K...| 250|\n",
      "|                 BDS|10 years experience|   99%|  Arekere, Bangalore|  Dentist|Dental Fillings C...| 200|\n",
      "|            BDS, MDS| 9 years experience|   98%|Coimbatore Raceco...|  Dentist|98% 14 Feedback C...| 200|\n",
      "+--------------------+-------------------+------+--------------------+---------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.createOrReplaceTempView(\"dfTable\")\n",
    "spark.sql(\"\"\"SELECT * FROM dfTable WHERE Qualification IS NOT NULL \n",
    "and Experience IS NOT NULL \n",
    "and Rating IS NOT NULL \n",
    "and Profile IS NOT NULL \n",
    "and Place IS NOT NULL \n",
    "and Miscellaneous_Info IS NOT NULL\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6867ca54-8313-4542-93d1-d31c8a9fddaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "|       Qualification|         Experience|Rating|               Place|         Profile|  Miscellaneous_Info|Fees|\n",
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "|BHMS, MD - Homeop...|24 years experience|  100%| Kakkanad, Ernakulam|       Homeopath|100% 16 Feedback ...| 100|\n",
      "|BAMS, MD - Ayurve...|12 years experience|   98%|Whitefield, Banga...|        Ayurveda|98% 76 Feedback W...| 350|\n",
      "|MBBS, MS - Otorhi...| 9 years experience|  null|Mathikere - BEL, ...|  ENT Specialist|                null| 300|\n",
      "| BSc - Zoology, BAMS|12 years experience|  null|Bannerghatta Road...|        Ayurveda|Bannerghatta Road...| 250|\n",
      "|                BAMS|20 years experience|  100%|Keelkattalai, Che...|        Ayurveda|100% 4 Feedback K...| 250|\n",
      "|                BAMS| 8 years experience|  null|      Porur, Chennai|        Ayurveda|                null| 100|\n",
      "|                BHMS|42 years experience|  null|   Karol Bagh, Delhi|       Homeopath|                null| 200|\n",
      "|                 BDS|10 years experience|   99%|  Arekere, Bangalore|         Dentist|Dental Fillings C...| 200|\n",
      "|MBBS, MD - Genera...|14 years experience|  null| Old City, Hyderabad|General Medicine|                null| 100|\n",
      "|            BSc, BDS|23 years experience|  null|   Athani, Ernakulam|         Dentist|                null| 100|\n",
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "|       Qualification|         Experience|Rating|               Place|         Profile|  Miscellaneous_Info|Fees|\n",
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "|BHMS, MD - Homeop...|24 years experience|  100%| Kakkanad, Ernakulam|       Homeopath|100% 16 Feedback ...| 100|\n",
      "|BAMS, MD - Ayurve...|12 years experience|   98%|Whitefield, Banga...|        Ayurveda|98% 76 Feedback W...| 350|\n",
      "| BSc - Zoology, BAMS|12 years experience|  null|Bannerghatta Road...|        Ayurveda|Bannerghatta Road...| 250|\n",
      "|                BAMS|20 years experience|  100%|Keelkattalai, Che...|        Ayurveda|100% 4 Feedback K...| 250|\n",
      "|                 BDS|10 years experience|   99%|  Arekere, Bangalore|         Dentist|Dental Fillings C...| 200|\n",
      "|            BDS, MDS| 9 years experience|   98%|Coimbatore Raceco...|         Dentist|98% 14 Feedback C...| 200|\n",
      "|BDS, MDS - Oral &...|21 years experience|  null|Jubilee Hills, Hy...|         Dentist|Dental Crowns Fac...| 350|\n",
      "|      MBBS, MS - ENT|19 years experience|   79%|     KPHB, Hyderabad|  ENT Specialist|79% 8 Feedback KP...| 400|\n",
      "|                MBBS|19 years experience|  100%|HSR Layout, Banga...|General Medicine|100% 4 Feedback H...| 150|\n",
      "|       MDS, DNB, BDS|21 years experience|  100%|Pollachi, Coimbatore|         Dentist|100% 7 Feedback P...| 100|\n",
      "+--------------------+-------------------+------+--------------------+----------------+--------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.na.drop(\"all\").show(10) \t\t# Using “all” drops the row only if all values are null or NaN for that row\n",
    "\n",
    "#We can also apply this to certain sets of columns by passing in an array of columns:\n",
    "data_table.na.drop(\"all\", subset=[\"Rating\", \"Miscellaneous_Info\"]).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9b5975-c9fe-4518-855a-dd136018d000",
   "metadata": {},
   "source": [
    "fill-\n",
    "Using the \"fill()\" function, you can fill one or more columns with a set of values.\n",
    "\n",
    "For example, to fill all null values in columns of type String, you might specify the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a662e29a-3e06-4030-91ec-d5dbead9ec7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+--------------------+--------------+--------------------+----+\n",
      "|       Qualification|         Experience|              Rating|               Place|       Profile|  Miscellaneous_Info|Fees|\n",
      "+--------------------+-------------------+--------------------+--------------------+--------------+--------------------+----+\n",
      "|BHMS, MD - Homeop...|24 years experience|                100%| Kakkanad, Ernakulam|     Homeopath|100% 16 Feedback ...| 100|\n",
      "|BAMS, MD - Ayurve...|12 years experience|                 98%|Whitefield, Banga...|      Ayurveda|98% 76 Feedback W...| 350|\n",
      "|MBBS, MS - Otorhi...| 9 years experience|GUCCI MANE GUCCI ...|Mathikere - BEL, ...|ENT Specialist|GUCCI MANE GUCCI ...| 300|\n",
      "| BSc - Zoology, BAMS|12 years experience|GUCCI MANE GUCCI ...|Bannerghatta Road...|      Ayurveda|Bannerghatta Road...| 250|\n",
      "|                BAMS|20 years experience|                100%|Keelkattalai, Che...|      Ayurveda|100% 4 Feedback K...| 250|\n",
      "+--------------------+-------------------+--------------------+--------------------+--------------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.na.fill(\"GUCCI MANE GUCCI MANE GUCCI MANE\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f23abae-4da5-4b4e-aa34-16566af31c51",
   "metadata": {},
   "source": [
    "\n",
    "We could do the same for columns of type Integer by using df.na.fill(5:Integer), or for\n",
    "Doubles df.na.fill(5:Double).To specify columns, we just pass in an array of column names like we did in the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6475fd5b-ae54-434f-8ccd-1e77b8d96740",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+----------------+--------------------+--------------+--------------------+----+\n",
      "|       Qualification|         Experience|          Rating|               Place|       Profile|  Miscellaneous_Info|Fees|\n",
      "+--------------------+-------------------+----------------+--------------------+--------------+--------------------+----+\n",
      "|BHMS, MD - Homeop...|24 years experience|            100%| Kakkanad, Ernakulam|     Homeopath|100% 16 Feedback ...| 100|\n",
      "|BAMS, MD - Ayurve...|12 years experience|             98%|Whitefield, Banga...|      Ayurveda|98% 76 Feedback W...| 350|\n",
      "|MBBS, MS - Otorhi...| 9 years experience|SUPER GUCCI MANE|Mathikere - BEL, ...|ENT Specialist|    SUPER GUCCI MANE| 300|\n",
      "| BSc - Zoology, BAMS|12 years experience|SUPER GUCCI MANE|Bannerghatta Road...|      Ayurveda|Bannerghatta Road...| 250|\n",
      "|                BAMS|20 years experience|            100%|Keelkattalai, Che...|      Ayurveda|100% 4 Feedback K...| 250|\n",
      "+--------------------+-------------------+----------------+--------------------+--------------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-------------------+--------------+--------------------+----------------+--------------------+----+\n",
      "|       Qualification|         Experience|        Rating|               Place|         Profile|  Miscellaneous_Info|Fees|\n",
      "+--------------------+-------------------+--------------+--------------------+----------------+--------------------+----+\n",
      "|BHMS, MD - Homeop...|24 years experience|          100%| Kakkanad, Ernakulam|       Homeopath|100% 16 Feedback ...| 100|\n",
      "|BAMS, MD - Ayurve...|12 years experience|           98%|Whitefield, Banga...|        Ayurveda|98% 76 Feedback W...| 350|\n",
      "|MBBS, MS - Otorhi...| 9 years experience|UNKNOWN RATING|Mathikere - BEL, ...|  ENT Specialist|             9999999| 300|\n",
      "| BSc - Zoology, BAMS|12 years experience|UNKNOWN RATING|Bannerghatta Road...|        Ayurveda|Bannerghatta Road...| 250|\n",
      "|                BAMS|20 years experience|          100%|Keelkattalai, Che...|        Ayurveda|100% 4 Feedback K...| 250|\n",
      "|                BAMS| 8 years experience|UNKNOWN RATING|      Porur, Chennai|        Ayurveda|             9999999| 100|\n",
      "|                BHMS|42 years experience|UNKNOWN RATING|   Karol Bagh, Delhi|       Homeopath|             9999999| 200|\n",
      "|                 BDS|10 years experience|           99%|  Arekere, Bangalore|         Dentist|Dental Fillings C...| 200|\n",
      "|MBBS, MD - Genera...|14 years experience|UNKNOWN RATING| Old City, Hyderabad|General Medicine|             9999999| 100|\n",
      "|            BSc, BDS|23 years experience|UNKNOWN RATING|   Athani, Ernakulam|         Dentist|             9999999| 100|\n",
      "+--------------------+-------------------+--------------+--------------------+----------------+--------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.na.fill(\"SUPER GUCCI MANE\", subset=[\"Rating\", \"Miscellaneous_Info\"]).show(5)\n",
    "\n",
    "fill_cols_vals = {\"Rating\": \"UNKNOWN RATING\" ,\"Miscellaneous_Info\" : 9999999}\n",
    "data_table.na.fill(fill_cols_vals).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd53ed5-1075-42a6-a3a6-cf697485dbc2",
   "metadata": {},
   "source": [
    "replace-\n",
    "The most common use case is to replace all values in a certain column according to their current value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "402e71b9-8465-4fc8-97ce-e011beeec874",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------+--------------------+----------------+--------------------+----+\n",
      "|       Qualification|         Experience|        Rating|               Place|         Profile|  Miscellaneous_Info|Fees|\n",
      "+--------------------+-------------------+--------------+--------------------+----------------+--------------------+----+\n",
      "|BHMS, MD - Homeop...|24 years experience|UNCLEAR RATING| Kakkanad, Ernakulam|       Homeopath|100% 16 Feedback ...| 100|\n",
      "|BAMS, MD - Ayurve...|12 years experience|           98%|Whitefield, Banga...|        Ayurveda|98% 76 Feedback W...| 350|\n",
      "|MBBS, MS - Otorhi...| 9 years experience|          null|Mathikere - BEL, ...|  ENT Specialist|                null| 300|\n",
      "| BSc - Zoology, BAMS|12 years experience|          null|Bannerghatta Road...|        Ayurveda|Bannerghatta Road...| 250|\n",
      "|                BAMS|20 years experience|UNCLEAR RATING|Keelkattalai, Che...|        Ayurveda|100% 4 Feedback K...| 250|\n",
      "|                BAMS| 8 years experience|          null|      Porur, Chennai|        Ayurveda|                null| 100|\n",
      "|                BHMS|42 years experience|          null|   Karol Bagh, Delhi|       Homeopath|                null| 200|\n",
      "|                 BDS|10 years experience|      YOYOYOYO|  Arekere, Bangalore|         Dentist|Dental Fillings C...| 200|\n",
      "|MBBS, MD - Genera...|14 years experience|          null| Old City, Hyderabad|General Medicine|                null| 100|\n",
      "|            BSc, BDS|23 years experience|          null|   Athani, Ernakulam|         Dentist|                null| 100|\n",
      "+--------------------+-------------------+--------------+--------------------+----------------+--------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.na.replace([\"100%\",\"99%\"], [\"UNCLEAR RATING\",\"YOYOYOYO\"], \"Rating\").show(10)     \n",
    "x = data_table.na.replace([\"100%\",\"99%\"], [\"UNCLEAR RATING\",\"YOYOYOYO\"], \"Rating\")\n",
    "# Replacing 100% with \"UNCLEAR RATING\" and 99% with \"YOYOYOYO\" in Rating column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d1bdc8-cf37-4bb6-ad6b-e23085e65b1b",
   "metadata": {},
   "source": [
    "Structs-\n",
    "You can think of structs as DataFrames within DataFrames.\n",
    "\n",
    "We can create a struct by wrapping a set of columns in parenthesis in a query:Ordering-\n",
    "As we discussed in Chapter 5, you can use \"asc_nulls_first\", \"desc_nulls_first\",\n",
    "\"asc_nulls_last\", or \"desc_nulls_last\" to specify where you would like your null values to\n",
    "appear in an ordered DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbcc29f-e3f6-402b-b68a-981d3f71873d",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# Working with Complex Types-\n",
    "Complex types can help you organize and structure your data in ways that make more sense for\n",
    "the problem that you are hoping to solve. \n",
    "\n",
    "There are three kinds of complex types: structs, arrays and maps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20d4b53-1e61-470c-be42-eeef3f076df5",
   "metadata": {},
   "source": [
    "Structs-\n",
    "You can think of structs as DataFrames within DataFrames.\n",
    "\n",
    "We can create a struct by wrapping a set of columns in parenthesis in a query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "19dc17b8-665e-4b63-8164-643810f0132d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|        complex|\n",
      "+---------------+\n",
      "|{186, 2982, 14}|\n",
      "|{243, 2929, 15}|\n",
      "|{162, 3051, 12}|\n",
      "|{345, 3090, 17}|\n",
      "|   {4, 3023, 9}|\n",
      "|{270, 3174, 33}|\n",
      "| {34, 3306, 25}|\n",
      "| {153, 3257, 3}|\n",
      "| {92, 2180, 32}|\n",
      "|{226, 2857, 26}|\n",
      "+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_table.selectExpr(\"(`Aspect(degrees)`, `Elevation(meters)`, `Slope(degrees)`) as complex\").show(10)\n",
    "#df.selectExpr(\"struct(Description, InvoiceNo) as complex\", \"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "325a72b3-1a0e-4c2b-89df-61180a529e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+\n",
      "|complex                                           |\n",
      "+--------------------------------------------------+\n",
      "|{Homeopath, Kakkanad, Ernakulam, 100%}            |\n",
      "|{Ayurveda, Whitefield, Bangalore, 98%}            |\n",
      "|{ENT Specialist, Mathikere - BEL, Bangalore, null}|\n",
      "|{Ayurveda, Bannerghatta Road, Bangalore, null}    |\n",
      "|{Ayurveda, Keelkattalai, Chennai, 100%}           |\n",
      "|{Ayurveda, Porur, Chennai, null}                  |\n",
      "|{Homeopath, Karol Bagh, Delhi, null}              |\n",
      "|{Dentist, Arekere, Bangalore, 99%}                |\n",
      "|{General Medicine, Old City, Hyderabad, null}     |\n",
      "|{Dentist, Athani, Ernakulam, null}                |\n",
      "+--------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import struct\n",
    "complexDF = data_table.select(struct(\"Profile\", \"Place\", \"Rating\").alias(\"complex\"))\n",
    "complexDF.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "9255fa3f-0dc9-449c-b6a4-a610f66ceae3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     complex_columns|\n",
      "+--------------------+\n",
      "|{Kakkanad, Ernaku...|\n",
      "|{Whitefield, Bang...|\n",
      "|{Mathikere - BEL,...|\n",
      "|{Bannerghatta Roa...|\n",
      "|{Keelkattalai, Ch...|\n",
      "|{Porur, Chennai, ...|\n",
      "|{Karol Bagh, Delh...|\n",
      "|{Arekere, Bangalo...|\n",
      "|{Old City, Hydera...|\n",
      "|{Athani, Ernakula...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import struct\n",
    "complexDF = data_table.select(struct(\"Place\", \"Rating\",\"Profile\").alias(\"complex_columns\"))\n",
    "complexDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f09fffca-4ee0-4b2a-b4e9-042839cde967",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               Place|\n",
      "+--------------------+\n",
      "| Kakkanad, Ernakulam|\n",
      "|Whitefield, Banga...|\n",
      "|Mathikere - BEL, ...|\n",
      "|Bannerghatta Road...|\n",
      "|Keelkattalai, Che...|\n",
      "|      Porur, Chennai|\n",
      "|   Karol Bagh, Delhi|\n",
      "|  Arekere, Bangalore|\n",
      "| Old City, Hyderabad|\n",
      "|   Athani, Ernakulam|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+----------------+\n",
      "|               Place|         Profile|\n",
      "+--------------------+----------------+\n",
      "| Kakkanad, Ernakulam|       Homeopath|\n",
      "|Whitefield, Banga...|        Ayurveda|\n",
      "|Mathikere - BEL, ...|  ENT Specialist|\n",
      "|Bannerghatta Road...|        Ayurveda|\n",
      "|Keelkattalai, Che...|        Ayurveda|\n",
      "|      Porur, Chennai|        Ayurveda|\n",
      "|   Karol Bagh, Delhi|       Homeopath|\n",
      "|  Arekere, Bangalore|         Dentist|\n",
      "| Old City, Hyderabad|General Medicine|\n",
      "|   Athani, Ernakulam|         Dentist|\n",
      "+--------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---------------------+\n",
      "|complex_columns.Place|\n",
      "+---------------------+\n",
      "|  Kakkanad, Ernakulam|\n",
      "| Whitefield, Banga...|\n",
      "| Mathikere - BEL, ...|\n",
      "| Bannerghatta Road...|\n",
      "| Keelkattalai, Che...|\n",
      "|       Porur, Chennai|\n",
      "|    Karol Bagh, Delhi|\n",
      "|   Arekere, Bangalore|\n",
      "|  Old City, Hyderabad|\n",
      "|    Athani, Ernakulam|\n",
      "+---------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------------------+-----------------------+\n",
      "|complex_columns.Rating|complex_columns.Profile|\n",
      "+----------------------+-----------------------+\n",
      "|                  100%|              Homeopath|\n",
      "|                   98%|               Ayurveda|\n",
      "|                  null|         ENT Specialist|\n",
      "|                  null|               Ayurveda|\n",
      "|                  100%|               Ayurveda|\n",
      "|                  null|               Ayurveda|\n",
      "|                  null|              Homeopath|\n",
      "|                   99%|                Dentist|\n",
      "|                  null|       General Medicine|\n",
      "|                  null|                Dentist|\n",
      "+----------------------+-----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complexDF.createOrReplaceTempView(\"complexDF\")\n",
    "#We now have a DataFrame with a column complex. We can query it just as we might another DataFrame, the only difference is that we use a dot syntax to do so, or the column method \"getField\":\n",
    "complexDF.select(\"complex_columns.Place\").show(10)\n",
    "complexDF.select(\"complex_columns.Place\", \"complex_columns.Profile\").show(10)\n",
    "\n",
    "complexDF.select(col(\"complex_columns\").getField(\"Place\")).show(10)\n",
    "complexDF.select(col(\"complex_columns\").getField(\"Rating\"), col(\"complex_columns\").getField(\"Profile\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3758d82-dedd-4b3e-9d20-5f160c768d96",
   "metadata": {},
   "source": [
    "###### Arrays-\n",
    "\n",
    "To define arrays, let’s work through a use case. With our current data, our objective is to take\n",
    "every single word in our [Description] column and convert that into a row in our DataFrame.\n",
    "The first task is to turn our [Description] column into a complex type, an array.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313a6092-0fca-4004-a563-394f988bc8f3",
   "metadata": {},
   "source": [
    "split-\n",
    "We do this by using the \"split()\" function and specify the delimiter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c2d069aa-fdee-4fbe-a192-34c1e041d68e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|split(Place, ,, -1)[0]|\n",
      "+----------------------+\n",
      "|              Kakkanad|\n",
      "|            Whitefield|\n",
      "|       Mathikere - BEL|\n",
      "|     Bannerghatta Road|\n",
      "|          Keelkattalai|\n",
      "|                 Porur|\n",
      "|            Karol Bagh|\n",
      "|               Arekere|\n",
      "|              Old City|\n",
      "|                Athani|\n",
      "+----------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------------------+\n",
      "|split(Place, ,, -1)[1]|\n",
      "+----------------------+\n",
      "|             Ernakulam|\n",
      "|             Bangalore|\n",
      "|             Bangalore|\n",
      "|             Bangalore|\n",
      "|               Chennai|\n",
      "|               Chennai|\n",
      "|                 Delhi|\n",
      "|             Bangalore|\n",
      "|             Hyderabad|\n",
      "|             Ernakulam|\n",
      "+----------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------------------+\n",
      "|split(Place, ,, -1)[2]|\n",
      "+----------------------+\n",
      "|                  null|\n",
      "|                  null|\n",
      "|                  null|\n",
      "|                  null|\n",
      "|                  null|\n",
      "|                  null|\n",
      "|                  null|\n",
      "|                  null|\n",
      "|                  null|\n",
      "|                  null|\n",
      "+----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in Python\n",
    "from pyspark.sql.functions import split\n",
    "data_table.select(split(col(\"Place\"), \",\")[0]).show(10)\n",
    "data_table.select(split(col(\"Place\"), \",\")[1]).show(10)\n",
    "data_table.select(split(col(\"Place\"), \",\")[2]).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c30751d9-8417-4ab7-a132-4272ceeffd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|split(Place, ,, -1)[0]|\n",
      "+----------------------+\n",
      "|              Kakkanad|\n",
      "|            Whitefield|\n",
      "|       Mathikere - BEL|\n",
      "|     Bannerghatta Road|\n",
      "|          Keelkattalai|\n",
      "+----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------------------+\n",
      "|split(Place, ,, -1)[1]|\n",
      "+----------------------+\n",
      "|             Ernakulam|\n",
      "|             Bangalore|\n",
      "|             Bangalore|\n",
      "|             Bangalore|\n",
      "|               Chennai|\n",
      "+----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------------------------+------------------------------+------------------------------+\n",
      "|split(Qualification, ,, -1)[0]|split(Qualification, ,, -1)[1]|split(Qualification, ,, -1)[2]|\n",
      "+------------------------------+------------------------------+------------------------------+\n",
      "|                          BHMS|               MD - Homeopathy|                          null|\n",
      "|                          BAMS|           MD - Ayurveda Me...|                          null|\n",
      "|                          MBBS|           MS - Otorhinolar...|                          null|\n",
      "|                 BSc - Zoology|                          BAMS|                          null|\n",
      "|                          BAMS|                          null|                          null|\n",
      "+------------------------------+------------------------------+------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.createOrReplaceTempView(\"dfTable\")\n",
    "spark.sql(\"\"\"SELECT split(Place, ',')[0] FROM dfTable\"\"\").show(5)\n",
    "spark.sql(\"\"\"SELECT split(Place, ',')[1] FROM dfTable\"\"\").show(5)\n",
    "spark.sql(\"\"\"SELECT split(Qualification, ',')[0],split(Qualification, ',')[1],split(Qualification, ',')[2]  FROM dfTable\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "acb773eb-e074-4316-a231-e507ff62a35d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+\n",
      "|array_col[1]|array_qua_col[0]|\n",
      "+------------+----------------+\n",
      "| Ernakulam  |BHMS            |\n",
      "| Bangalore  |BAMS            |\n",
      "| Bangalore  |MBBS            |\n",
      "| Bangalore  |BSc - Zoology   |\n",
      "| Chennai    |BAMS            |\n",
      "| Chennai    |BAMS            |\n",
      "| Delhi      |BHMS            |\n",
      "| Bangalore  |BDS             |\n",
      "| Hyderabad  |MBBS            |\n",
      "| Ernakulam  |BSc             |\n",
      "+------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------------+----------------------------------+----------------+\n",
      "|array_col[1]|array_qua_col                     |array_qua_col[0]|\n",
      "+------------+----------------------------------+----------------+\n",
      "| Ernakulam  |[BHMS,  MD ,  Homeopathy]         |BHMS            |\n",
      "| Bangalore  |[BAMS,  MD ,  Ayurveda Medicine]  |BAMS            |\n",
      "| Bangalore  |[MBBS,  MS ,  Otorhinolaryngology]|MBBS            |\n",
      "| Bangalore  |[BSc ,  Zoology,  BAMS]           |BSc             |\n",
      "| Chennai    |[BAMS]                            |BAMS            |\n",
      "| Chennai    |[BAMS]                            |BAMS            |\n",
      "| Delhi      |[BHMS]                            |BHMS            |\n",
      "| Bangalore  |[BDS]                             |BDS             |\n",
      "| Hyderabad  |[MBBS,  MD ,  General Medicine]   |MBBS            |\n",
      "| Ernakulam  |[BSc,  BDS]                       |BSc             |\n",
      "+------------+----------------------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.select(split(col(\"Place\"), \",\").alias(\"array_col\"),split(col(\"Qualification\"), \",\").alias(\"array_qua_col\"))\\\n",
    ".selectExpr(\"array_col[1]\",\"array_qua_col[0]\").show(10,False)\n",
    "\n",
    "## Splitting wrt. \",\" only. In case if we want splits wrt. more symbols like \"-\", \"_\", \"+\", then we can use OR (\"|\") in between.\n",
    "data_table.select(split(col(\"Place\"), \",\").alias(\"array_col\"),split(col(\"Qualification\"), \",|-\").alias(\"array_qua_col\"))\\\n",
    ".selectExpr(\"array_col[1]\",\"array_qua_col\",\"array_qua_col[0]\").show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f282f14-6fa4-4814-8b65-00ab3405f4ca",
   "metadata": {},
   "source": [
    "Array Length-\n",
    "We can determine the array’s length by querying for its size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e425c306-66be-42b0-8ea9-4c0c5984b3ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+----------------------------------+-----------------------------------+----------------------------------------------------------------------+\n",
      "|Qualification                 |split(Qualification, ,|-, -1)     |size(split(Qualification, ,|-, -1))|split(Miscellaneous_Info, ,|-| , -1)                                  |\n",
      "+------------------------------+----------------------------------+-----------------------------------+----------------------------------------------------------------------+\n",
      "|BHMS, MD - Homeopathy         |[BHMS,  MD ,  Homeopathy]         |3                                  |[100%, 16, Feedback, Kakkanad, , Ernakulam]                           |\n",
      "|BAMS, MD - Ayurveda Medicine  |[BAMS,  MD ,  Ayurveda Medicine]  |3                                  |[98%, 76, Feedback, Whitefield, , Bangalore]                          |\n",
      "|MBBS, MS - Otorhinolaryngology|[MBBS,  MS ,  Otorhinolaryngology]|3                                  |null                                                                  |\n",
      "|BSc - Zoology, BAMS           |[BSc ,  Zoology,  BAMS]           |3                                  |[Bannerghatta, Road, , Bangalore, ?250, Available, on, Sun, , 10, Feb]|\n",
      "|BAMS                          |[BAMS]                            |1                                  |[100%, 4, Feedback, Keelkattalai, , Chennai]                          |\n",
      "+------------------------------+----------------------------------+-----------------------------------+----------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import size\n",
    "data_table.select(col(\"Qualification\"),split(col(\"Qualification\"),\",|-\"),\\\n",
    "                  size(split(col(\"Qualification\"), \",|-\")), \\\n",
    "                  split(col(\"Miscellaneous_Info\" ),\",|-| \")).show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e98b4d8e-da4f-4257-bed2-70de44b020d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-------------------+------+----------------------------+----------------+-------------------------------------------------------------------------------+----+\n",
      "|Qualification                 |Experience         |Rating|Place                       |Profile         |Miscellaneous_Info                                                             |Fees|\n",
      "+------------------------------+-------------------+------+----------------------------+----------------+-------------------------------------------------------------------------------+----+\n",
      "|BHMS, MD - Homeopathy         |24 years experience|100%  |Kakkanad, Ernakulam         |Homeopath       |100% 16 Feedback Kakkanad, Ernakulam                                           |100 |\n",
      "|BAMS, MD - Ayurveda Medicine  |12 years experience|98%   |Whitefield, Bangalore       |Ayurveda        |98% 76 Feedback Whitefield, Bangalore                                          |350 |\n",
      "|MBBS, MS - Otorhinolaryngology|9 years experience |null  |Mathikere - BEL, Bangalore  |ENT Specialist  |null                                                                           |300 |\n",
      "|BSc - Zoology, BAMS           |12 years experience|null  |Bannerghatta Road, Bangalore|Ayurveda        |Bannerghatta Road, Bangalore ?250 Available on Sun, 10 Feb                     |250 |\n",
      "|BAMS                          |20 years experience|100%  |Keelkattalai, Chennai       |Ayurveda        |100% 4 Feedback Keelkattalai, Chennai                                          |250 |\n",
      "|BAMS                          |8 years experience |null  |Porur, Chennai              |Ayurveda        |null                                                                           |100 |\n",
      "|BHMS                          |42 years experience|null  |Karol Bagh, Delhi           |Homeopath       |null                                                                           |200 |\n",
      "|BDS                           |10 years experience|99%   |Arekere, Bangalore          |Dentist         |Dental Fillings Crowns and Bridges Fixing Impaction / Impacted Tooth Extraction|200 |\n",
      "|MBBS, MD - General Medicine   |14 years experience|null  |Old City, Hyderabad         |General Medicine|null                                                                           |100 |\n",
      "|BSc, BDS                      |23 years experience|null  |Athani, Ernakulam           |Dentist         |null                                                                           |100 |\n",
      "+------------------------------+-------------------+------+----------------------------+----------------+-------------------------------------------------------------------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "651d9c82-2a77-483b-a753-d9e3f3e41f4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------------------------------+-----------------------------------+---------------------------------------+---------------------------------------+\n",
      "|Qualification                 |split(Qualification, ,|-, -1)[0]|size(split(Qualification, ,|-, -1))|split(Miscellaneous_Info, ,|-| , -1)[0]|split(Miscellaneous_Info, ,|-| , -1)[2]|\n",
      "+------------------------------+--------------------------------+-----------------------------------+---------------------------------------+---------------------------------------+\n",
      "|BHMS, MD - Homeopathy         |BHMS                            |3                                  |100%                                   |Feedback                               |\n",
      "|BAMS, MD - Ayurveda Medicine  |BAMS                            |3                                  |98%                                    |Feedback                               |\n",
      "|MBBS, MS - Otorhinolaryngology|MBBS                            |3                                  |null                                   |null                                   |\n",
      "|BSc - Zoology, BAMS           |BSc                             |3                                  |Bannerghatta                           |                                       |\n",
      "|BAMS                          |BAMS                            |1                                  |100%                                   |Feedback                               |\n",
      "+------------------------------+--------------------------------+-----------------------------------+---------------------------------------+---------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import size\n",
    "## Selecting 1st element of the row after the split. \n",
    "## Selecting 3rd element of the row after the split.\n",
    "\n",
    "data_table.select(col(\"Qualification\"),split(col(\"Qualification\"),\",|-\")[0],\n",
    "                  size(split(col(\"Qualification\"), \",|-\")),\n",
    "                  split(col(\"Miscellaneous_Info\" ),\",|-| \")[0],   \n",
    "                  split(col(\"Miscellaneous_Info\" ),\",|-| \")[2]).show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1207d34f-52d9-4092-90e8-8fd158815171",
   "metadata": {},
   "source": [
    "array_contains-\n",
    "We can also check whether this array contains a value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "cbed40d7-4591-4a60-bc8b-c5eba93e25a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------+---------------------------------------------------------+-------------------------------------------------+\n",
      "|Qualification                                        |split(Qualification, ,, -1)                              |array_contains(split(Qualification, ,, -1), BAMS)|\n",
      "+-----------------------------------------------------+---------------------------------------------------------+-------------------------------------------------+\n",
      "|BHMS, MD - Homeopathy                                |[BHMS,  MD - Homeopathy]                                 |false                                            |\n",
      "|BAMS, MD - Ayurveda Medicine                         |[BAMS,  MD - Ayurveda Medicine]                          |true                                             |\n",
      "|MBBS, MS - Otorhinolaryngology                       |[MBBS,  MS - Otorhinolaryngology]                        |false                                            |\n",
      "|BSc - Zoology, BAMS                                  |[BSc - Zoology,  BAMS]                                   |false                                            |\n",
      "|BAMS                                                 |[BAMS]                                                   |true                                             |\n",
      "|BAMS                                                 |[BAMS]                                                   |true                                             |\n",
      "|BHMS                                                 |[BHMS]                                                   |false                                            |\n",
      "|BDS                                                  |[BDS]                                                    |false                                            |\n",
      "|MBBS, MD - General Medicine                          |[MBBS,  MD - General Medicine]                           |false                                            |\n",
      "|BSc, BDS                                             |[BSc,  BDS]                                              |false                                            |\n",
      "|MBBS, MS, DNB - ENT                                  |[MBBS,  MS,  DNB - ENT]                                  |false                                            |\n",
      "|BAMS                                                 |[BAMS]                                                   |true                                             |\n",
      "|BDS, MDS                                             |[BDS,  MDS]                                              |false                                            |\n",
      "|BDS, MDS - Oral & Maxillofacial Surgery              |[BDS,  MDS - Oral & Maxillofacial Surgery]               |false                                            |\n",
      "|MBBS, Diploma in Otorhinolaryngology (DLO), DNB - ENT|[MBBS,  Diploma in Otorhinolaryngology (DLO),  DNB - ENT]|false                                            |\n",
      "|MBBS, MD - General Medicine                          |[MBBS,  MD - General Medicine]                           |false                                            |\n",
      "|MBBS, Diploma in Otorhinolaryngology (DLO)           |[MBBS,  Diploma in Otorhinolaryngology (DLO)]            |false                                            |\n",
      "|MBBS, MF- Homeopathy                                 |[MBBS,  MF- Homeopathy]                                  |false                                            |\n",
      "|MBBS, MS - ENT                                       |[MBBS,  MS - ENT]                                        |false                                            |\n",
      "|MBBS                                                 |[MBBS]                                                   |false                                            |\n",
      "+-----------------------------------------------------+---------------------------------------------------------+-------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "data_table.select(col(\"Qualification\"),split(col(\"Qualification\"), \",\"),array_contains(split(col(\"Qualification\"), \",\"), \"BAMS\")).show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "89ead7e4-f127-4fcc-97e5-9c0e6cd4d29f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+\n",
      "|array_contains(split(Qualification, ,, -1), BAMS)|\n",
      "+-------------------------------------------------+\n",
      "|                                            false|\n",
      "|                                             true|\n",
      "|                                            false|\n",
      "|                                            false|\n",
      "|                                             true|\n",
      "|                                             true|\n",
      "|                                            false|\n",
      "|                                            false|\n",
      "|                                            false|\n",
      "|                                            false|\n",
      "+-------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.createOrReplaceTempView(\"dfTable\")\n",
    "spark.sql(\"\"\"SELECT array_contains(split(Qualification, ','), 'BAMS') FROM dfTable\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fab156-f4d3-47ae-a151-0b5f1c43bf89",
   "metadata": {},
   "source": [
    "explode-\n",
    "The \"explode()\" function takes a column that consists of arrays and creates one row (with the rest of\n",
    "the values duplicated) per value in the array.\n",
    "\n",
    "\"Hello World\", \"Other Col\" ---->(split)---->[\"Hello\", \"World\"],\"Other Col\" -----> (explode) ----->\"Hello\", \"Other Col\", \"World\", \"Other Col\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c458079b-04b9-4676-ad0a-67771add484e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-------------------------+\n",
      "|Qualification                 |exploded                 |\n",
      "+------------------------------+-------------------------+\n",
      "|BHMS, MD - Homeopathy         |BHMS                     |\n",
      "|BHMS, MD - Homeopathy         | MD - Homeopathy         |\n",
      "|BAMS, MD - Ayurveda Medicine  |BAMS                     |\n",
      "|BAMS, MD - Ayurveda Medicine  | MD - Ayurveda Medicine  |\n",
      "|MBBS, MS - Otorhinolaryngology|MBBS                     |\n",
      "|MBBS, MS - Otorhinolaryngology| MS - Otorhinolaryngology|\n",
      "|BSc - Zoology, BAMS           |BSc - Zoology            |\n",
      "|BSc - Zoology, BAMS           | BAMS                    |\n",
      "|BAMS                          |BAMS                     |\n",
      "|BAMS                          |BAMS                     |\n",
      "+------------------------------+-------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Working not understood\n",
    "\n",
    "from pyspark.sql.functions import split, explode\n",
    "data_table.withColumn(\"splitted\", split(col(\"Qualification\"), \",\"))\\\n",
    ".withColumn(\"exploded\", explode(col(\"splitted\")))\\\n",
    ".select(\"Qualification\", \"exploded\").show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "cd89a971-02fb-4a4a-8500-800b2f831f2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------------+-------------------------+\n",
      "|Qualification                 |Profile       |exploded                 |\n",
      "+------------------------------+--------------+-------------------------+\n",
      "|BHMS, MD - Homeopathy         |Homeopath     |BHMS                     |\n",
      "|BHMS, MD - Homeopathy         |Homeopath     | MD - Homeopathy         |\n",
      "|BAMS, MD - Ayurveda Medicine  |Ayurveda      |BAMS                     |\n",
      "|BAMS, MD - Ayurveda Medicine  |Ayurveda      | MD - Ayurveda Medicine  |\n",
      "|MBBS, MS - Otorhinolaryngology|ENT Specialist|MBBS                     |\n",
      "|MBBS, MS - Otorhinolaryngology|ENT Specialist| MS - Otorhinolaryngology|\n",
      "|BSc - Zoology, BAMS           |Ayurveda      |BSc - Zoology            |\n",
      "|BSc - Zoology, BAMS           |Ayurveda      | BAMS                    |\n",
      "|BAMS                          |Ayurveda      |BAMS                     |\n",
      "|BAMS                          |Ayurveda      |BAMS                     |\n",
      "+------------------------------+--------------+-------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.createOrReplaceTempView(\"dfTable\")\n",
    "spark.sql(\n",
    "\"\"\"SELECT Qualification, Profile, exploded\n",
    "FROM (SELECT *, split(Qualification, \",\") as splitted FROM dfTable)\n",
    "LATERAL VIEW explode(splitted) as exploded\"\"\").show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e49f0c2-042f-4dd2-b2f3-bd803d862f18",
   "metadata": {},
   "source": [
    "Maps-\n",
    "\n",
    "Maps are created by using the map function and key-value pairs of columns. You then can select\n",
    "them just like you might select from an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b7095840-c543-43f0-9678-1a157585df17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|complex_map               |\n",
      "+--------------------------+\n",
      "|{Homeopath -> 100%}       |\n",
      "|{Ayurveda -> 98%}         |\n",
      "|{ENT Specialist -> null}  |\n",
      "|{Ayurveda -> null}        |\n",
      "|{Ayurveda -> 100%}        |\n",
      "|{Ayurveda -> null}        |\n",
      "|{Homeopath -> null}       |\n",
      "|{Dentist -> 99%}          |\n",
      "|{General Medicine -> null}|\n",
      "|{Dentist -> null}         |\n",
      "+--------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import create_map\n",
    "data_table.select(create_map(col(\"Profile\"), col(\"Rating\")).alias(\"complex_map\")).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "70d00c06-24c4-401b-8641-7da0b1b92dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|complex_map               |\n",
      "+--------------------------+\n",
      "|{Homeopath -> 100%}       |\n",
      "|{Ayurveda -> 98%}         |\n",
      "|{ENT Specialist -> null}  |\n",
      "|{Ayurveda -> null}        |\n",
      "|{Ayurveda -> 100%}        |\n",
      "|{Ayurveda -> null}        |\n",
      "|{Homeopath -> null}       |\n",
      "|{Dentist -> 99%}          |\n",
      "|{General Medicine -> null}|\n",
      "|{Dentist -> null}         |\n",
      "+--------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.createOrReplaceTempView(\"dfTable\")\n",
    "spark.sql(\n",
    "\"\"\"SELECT map(Profile, Rating) as complex_map FROM dfTable\n",
    "\"\"\").show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "7d825435-dce0-482a-9077-e05249ee45ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Column is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[179], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data_table\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProfile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRating\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplex_map\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mselectExpr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplex_map[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAyurveda 100\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\column.py:560\u001b[0m, in \u001b[0;36mColumn.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 560\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn is not iterable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Column is not iterable"
     ]
    }
   ],
   "source": [
    "data_table.select(map(col(\"Profile\"),col(\"Rating\")).alias(\"complex_map\")).selectExpr(\"complex_map['Ayurveda 100%']\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3910572-4f43-4e62-a470-cf0412bbe64a",
   "metadata": {},
   "source": [
    "You can also explode map types, which will turn them into columns:\n",
    "\n",
    "df.select(map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\")).selectExpr(\"explode(complex_map)\").show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c36a08-c508-42a4-9389-00917b80c1ed",
   "metadata": {},
   "source": [
    "# Working with JSON-\n",
    "\n",
    "Spark has some unique support for working with JSON data. You can operate directly on strings\n",
    "of JSON in Spark and parse from JSON or extract JSON objects. Let’s begin by creating a JSON\n",
    "column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "42049402-2d6b-43da-b8fd-59a19f073e7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jsonDF = spark.range(1).selectExpr(\"\"\"'{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ec1e5fe1-c2a1-469b-a422-1b35f5355fab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+\n",
      "|jsonString                                 |\n",
      "+-------------------------------------------+\n",
      "|{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}|\n",
      "+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonDF.show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3cf0a1-3a85-4bdb-a27e-939e75e65e4b",
   "metadata": {},
   "source": [
    "You can use the \"get_json_object\" to inline query a JSON object, be it a dictionary or array.\n",
    "You can use \"json_tuple\" if this object has only one level of nesting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "08186472-24d1-44db-b8c9-a2b21f18eba1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------+\n",
      "|column|c0                     |\n",
      "+------+-----------------------+\n",
      "|2     |{\"myJSONValue\":[1,2,3]}|\n",
      "+------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import get_json_object, json_tuple\n",
    "jsonDF.select(\n",
    "get_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\").alias(\"column\"),json_tuple(col(\"jsonString\"), \"myJSONKey\")).show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "adb4ae55-bf7f-4742-8771-23a5348d4ccb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|column|\n",
      "+------+\n",
      "|  null|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonDF.selectExpr(\n",
    "\"json_tuple(jsonString, '$.myJSONKey.myJSONValue[1]') as column\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "701fed52-5ca5-4d3c-b99e-06aa17298407",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+\n",
      "|to_json(myStruct)                                                     |\n",
      "+----------------------------------------------------------------------+\n",
      "|{\"Profile\":\"Homeopath\",\"Place\":\"Kakkanad, Ernakulam\",\"Rating\":\"100%\"} |\n",
      "|{\"Profile\":\"Ayurveda\",\"Place\":\"Whitefield, Bangalore\",\"Rating\":\"98%\"} |\n",
      "|{\"Profile\":\"ENT Specialist\",\"Place\":\"Mathikere - BEL, Bangalore\"}     |\n",
      "|{\"Profile\":\"Ayurveda\",\"Place\":\"Bannerghatta Road, Bangalore\"}         |\n",
      "|{\"Profile\":\"Ayurveda\",\"Place\":\"Keelkattalai, Chennai\",\"Rating\":\"100%\"}|\n",
      "|{\"Profile\":\"Ayurveda\",\"Place\":\"Porur, Chennai\"}                       |\n",
      "|{\"Profile\":\"Homeopath\",\"Place\":\"Karol Bagh, Delhi\"}                   |\n",
      "|{\"Profile\":\"Dentist\",\"Place\":\"Arekere, Bangalore\",\"Rating\":\"99%\"}     |\n",
      "|{\"Profile\":\"General Medicine\",\"Place\":\"Old City, Hyderabad\"}          |\n",
      "|{\"Profile\":\"Dentist\",\"Place\":\"Athani, Ernakulam\"}                     |\n",
      "+----------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_json\n",
    "data_table.selectExpr(\"(Profile, Place, Rating) as myStruct\")\\\n",
    ".select(to_json(col(\"myStruct\"))).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "3abd5641-0316-4d84-b615-52523cd870e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+-----------------------------------------------------------------+\n",
      "|from_json(newJSON)                          |newJSON                                                          |\n",
      "+--------------------------------------------+-----------------------------------------------------------------+\n",
      "|{Homeopath, Kakkanad, Ernakulam}            |{\"Profile\":\"Homeopath\",\"Place\":\"Kakkanad, Ernakulam\"}            |\n",
      "|{Ayurveda, Whitefield, Bangalore}           |{\"Profile\":\"Ayurveda\",\"Place\":\"Whitefield, Bangalore\"}           |\n",
      "|{ENT Specialist, Mathikere - BEL, Bangalore}|{\"Profile\":\"ENT Specialist\",\"Place\":\"Mathikere - BEL, Bangalore\"}|\n",
      "|{Ayurveda, Bannerghatta Road, Bangalore}    |{\"Profile\":\"Ayurveda\",\"Place\":\"Bannerghatta Road, Bangalore\"}    |\n",
      "|{Ayurveda, Keelkattalai, Chennai}           |{\"Profile\":\"Ayurveda\",\"Place\":\"Keelkattalai, Chennai\"}           |\n",
      "+--------------------------------------------+-----------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import *\n",
    "parseSchema = StructType((\n",
    "StructField(\"Profile\",StringType(),True),\n",
    "StructField(\"Place\",StringType(),True)))\n",
    "data_table.selectExpr(\"(Profile, Place) as myStruct\")\\\n",
    ".select(to_json(col(\"myStruct\")).alias(\"newJSON\"))\\\n",
    ".select(from_json(col(\"newJSON\"), parseSchema), col(\"newJSON\")).show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70fc7c7-e2c9-46f5-a8ed-59a174d505dd",
   "metadata": {},
   "source": [
    "# User-Defined Functions-\n",
    "\n",
    "These user-defined functions (UDFs) make it possible for you to write your own custom\n",
    "transformations using Python or Scala and even use external libraries.\n",
    "They’re just functions that operate on the data, record by record. By default, these functions \n",
    "are registered as temporary functions to be used in that specific SparkSession or Context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c7d5fb-49d2-42e3-8417-df48020c893c",
   "metadata": {},
   "source": [
    "The first step is the actual function. We’ll create a simple one for this example. Let’s write a\n",
    "\"power3\" function that takes a number and raises it to a power of three:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "8ce873f0-b4c6-499d-a677-8bb7ddd19291",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udfExampleDF = spark.range(5).toDF(\"num\")\n",
    "def power3(double_value):\n",
    "    return double_value ** 3\n",
    "power3(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6fc1af-3266-41eb-9a35-75b3013627e8",
   "metadata": {},
   "source": [
    "Now that we’ve created these functions and tested them, we need to register them with Spark so\n",
    "that we can use them on all of our worker machines. Spark will serialize the function on the\n",
    "driver and transfer it over the network to all executor processes. This happens regardless of\n",
    "language.\n",
    "\n",
    "\n",
    "If the function is written in Python, Spark starts a Python\n",
    "process on the worker, serializes all of the data to a format that Python can understand\n",
    "(remember, it was in the JVM earlier), executes the function row by row on that data in the\n",
    "Python process, and then finally returns the results of the row operations to the JVM and Spark.\n",
    "\n",
    "First, we need to register the function to make it available as a DataFrame function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "53b41913-6ab6-44dd-9ae3-2b2426b5c4fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf,col\n",
    "power3udf = udf(power3)\n",
    "\n",
    "## Below code is not working hence commented\n",
    "#udfExampleDF.select(power3udf(col(\"num\"))).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a13930-6268-4ce2-8f34-65695ec3df0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "At this juncture, we can use this only as a DataFrame function. That is to say, we can’t use it\n",
    "within a string expression, only on an expression. However, we can also register this UDF as a\n",
    "Spark SQL function.\n",
    "\n",
    "\n",
    "#### NOTE- NEED MORE INPUTS FOR UNDERSTANDING UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "10a6dafd-a519-409a-80b9-9433f7b1a894",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#udfExampleDF.selectExpr(\"power3(num)\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d9a817-c566-4fc5-88de-f7d330eef713",
   "metadata": {},
   "source": [
    "If you specify the type that doesn’t align with the actual type returned by the function, Spark will\n",
    "not throw an error but will just return \"null\" to designate a failure. You can see this if you were to\n",
    "switch the return type in the following function to be a DoubleType:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b7a62801-3a1e-40b6-9bdf-07450a3ddb12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.power3(double_value)>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "spark.udf.register(\"power3py\", power3, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "587694ce-ef6a-4784-8bb7-ba5b9cd5ef0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#udfExampleDF.selectExpr(\"power3py(num)\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0459266-ecc3-4b6c-b3cd-de214a2fab53",
   "metadata": {},
   "source": [
    "#### NOTE- NEED MORE INPUTS FOR UNDERSTANDING UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cd2725-18a7-412f-9e3c-570e73aa1a90",
   "metadata": {},
   "source": [
    "# Aggregations-\n",
    "\n",
    "The simplest grouping is to just summarize a complete DataFrame by performing an\n",
    "aggregation in a select statement.\n",
    "\n",
    "A “group by” allows you to specify one or more keys as well as one or more\n",
    "aggregation functions to transform the value columns.\n",
    "\n",
    "A “window” gives you the ability to specify one or more keys as well as one or more\n",
    "aggregation functions to transform the value columns. However, the rows input to the\n",
    "function are somehow related to the current row.\n",
    "\n",
    "A “grouping set,” which you can use to aggregate at multiple different levels. Grouping\n",
    "sets are available as a primitive in SQL and via rollups and cubes in DataFrames.\n",
    "\n",
    "A “rollup” makes it possible for you to specify one or more keys as well as one or more\n",
    "aggregation functions to transform the value columns, which will be summarized\n",
    "hierarchically.\n",
    "\n",
    "A “cube” allows you to specify one or more keys as well as one or more aggregation\n",
    "functions to transform the value columns, which will be summarized across all\n",
    "combinations of columns.\n",
    "\n",
    "Each grouping returns a \"RelationalGroupedDataset\" on which we specify our aggregations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "f796c4df-58d7-4376-ab48-f7d9e4fec321",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------+-------------------------------------+---------------------+--------------------+-------------------+-------------------+----------------------------+-------------------+-------------------+------------------------+---------------------------+-------------------------+----------------------------+-------------------------+-------------------------+--------------------------------------+---------------------------------------+\n",
      "|count(1)|count(DISTINCT Cover_Type)|approx_count_distinct(Slope(Degrees))|first(Slope(Degrees))|last(Slope(Degrees))|min(Slope(Degrees))|max(Slope(Degrees))|sum(DISTINCT Slope(Degrees))|sum(Slope(Degrees))|avg(Slope(Degrees))|var_pop(Aspect(Degrees))|stddev_pop(Aspect(Degrees))|var_samp(Aspect(Degrees))|stddev_samp(Aspect(Degrees))|skewness(Aspect(Degrees))|kurtosis(Aspect(Degrees))|covar_pop(Aspect(Degrees), Cover_Type)|covar_samp(Aspect(Degrees), Cover_Type)|\n",
      "+--------+--------------------------+-------------------------------------+---------------------+--------------------+-------------------+-------------------+----------------------------+-------------------+-------------------+------------------------+---------------------------+-------------------------+----------------------------+-------------------------+-------------------------+--------------------------------------+---------------------------------------+\n",
      "|   29050|                         7|                                   52|                   14|                  11|                  0|                  9|                      1355.0|           407529.0|  14.02853700516351|       12568.08872283364|         112.10748736294842|        12568.52137417182|          112.10941697365044|      0.39860331522322967|      -1.2283449161756164|                   0.43111763266493003|                     0.4311324737139391|\n",
      "+--------+--------------------------+-------------------------------------+---------------------+--------------------+-------------------+-------------------+----------------------------+-------------------+-------------------+------------------------+---------------------------+-------------------------+----------------------------+-------------------------+-------------------------+--------------------------------------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import expr, count, countDistinct, approx_count_distinct,first, last,min, max,sumDistinct,sum,\\\n",
    "avg, var_pop, stddev_pop,var_samp, stddev_samp, skewness, kurtosis, covar_pop, covar_samp\n",
    "\n",
    "first_table.select(count(\"*\"), \n",
    "                   countDistinct(\"Cover_Type\"),\\\n",
    "                   approx_count_distinct(\"Slope(Degrees)\"),\\\n",
    "                   first(\"Slope(Degrees)\"),\\\n",
    "                   last(\"Slope(Degrees)\"),\\\n",
    "                  min(\"Slope(Degrees)\"),\\\n",
    "                  max(\"Slope(Degrees)\"),\\\n",
    "                  sumDistinct(\"Slope(Degrees)\"),\\\n",
    "                  sum(\"Slope(Degrees)\"),\\\n",
    "                  avg(\"Slope(Degrees)\"),\\\n",
    "                  var_pop(\"Aspect(Degrees)\"),\\\n",
    "                  stddev_pop(\"Aspect(Degrees)\"),\\\n",
    "                  var_samp(\"Aspect(Degrees)\"),\\\n",
    "                  stddev_samp(\"Aspect(Degrees)\"),\\\n",
    "                  skewness(\"Aspect(Degrees)\"),\\\n",
    "                  kurtosis(\"Aspect(Degrees)\"),\\\n",
    "                  covar_pop(\"Aspect(Degrees)\",\"Cover_Type\"),\\\n",
    "                  covar_samp(\"Aspect(Degrees)\",\"Cover_Type\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b895325-8e08-4f34-b432-464753f3dc8c",
   "metadata": {},
   "source": [
    "Aggregating to Complex Types-\n",
    "In Spark, you can perform aggregations not just of numerical values using formulas, you can also\n",
    "perform them on complex types. For example, we can collect a list of values present in a given\n",
    "column or only the unique values by collecting to a set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "0a47c028-0a29-499b-8937-5236fc7de6d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|collect_set(Rating)                                                                             |collect_list(Rating)                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "+------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[88%, 89%, 86%, 98%, 78%, 93%, 87%, 80%, 79%, 82%, 74%, 90%, 95%, 97%, 100%, 94%, 36%, 96%, 99%]|[100%, 98%, 100%, 99%, 98%, 79%, 100%, 100%, 100%, 95%, 97%, 97%, 94%, 88%, 100%, 90%, 80%, 100%, 100%, 100%, 99%, 100%, 93%, 100%, 100%, 36%, 100%, 97%, 99%, 99%, 98%, 100%, 100%, 78%, 99%, 93%, 87%, 93%, 98%, 97%, 99%, 100%, 99%, 94%, 100%, 100%, 100%, 97%, 98%, 100%, 99%, 100%, 98%, 90%, 100%, 96%, 90%, 100%, 100%, 95%, 82%, 95%, 100%, 90%, 98%, 100%, 89%, 89%, 100%, 74%, 96%, 86%, 100%, 100%, 89%]|\n",
      "+------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set, collect_list\n",
    "data_table.agg(collect_set(\"Rating\"), collect_list(\"Rating\")).show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "c23cdd43-aeca-4f58-b6ab-a9846f929354",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|collect_set(Rating)                                                                             |collect_list(Rating)                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "+------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[88%, 89%, 86%, 98%, 78%, 93%, 87%, 80%, 79%, 82%, 74%, 90%, 95%, 97%, 100%, 94%, 36%, 96%, 99%]|[100%, 98%, 100%, 99%, 98%, 79%, 100%, 100%, 100%, 95%, 97%, 97%, 94%, 88%, 100%, 90%, 80%, 100%, 100%, 100%, 99%, 100%, 93%, 100%, 100%, 36%, 100%, 97%, 99%, 99%, 98%, 100%, 100%, 78%, 99%, 93%, 87%, 93%, 98%, 97%, 99%, 100%, 99%, 94%, 100%, 100%, 100%, 97%, 98%, 100%, 99%, 100%, 98%, 90%, 100%, 96%, 90%, 100%, 100%, 95%, 82%, 95%, 100%, 90%, 98%, 100%, 89%, 89%, 100%, 74%, 96%, 86%, 100%, 100%, 89%]|\n",
      "+------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.createOrReplaceTempView(\"dfTable\")\n",
    "spark.sql(\"\"\"SELECT collect_set(Rating), collect_list(Rating) FROM dfTable\"\"\").show(1,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df22b84b-dd5b-4002-b3e6-1a6889e793c2",
   "metadata": {},
   "source": [
    "#### Grouping-\n",
    "\n",
    "A more common task is to perform calculations based on groups in the data. \n",
    "This is typically done on categorical data for\n",
    "which we group our data on one column and perform some calculations on the other columns\n",
    "that end up in that group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "85109499-a838-4367-a27b-cd5873c72f9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+-----+\n",
      "|       Profile|         Experience|count|\n",
      "+--------------+-------------------+-----+\n",
      "|       Dentist|10 years experience|    7|\n",
      "|      Ayurveda|15 years experience|    1|\n",
      "|       Dentist| 9 years experience|    3|\n",
      "|ENT Specialist|35 years experience|    1|\n",
      "|Dermatologists|33 years experience|    1|\n",
      "+--------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.groupBy(\"Profile\", \"Experience\").count().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "e90b1733-eb9a-4180-83eb-2bdf9d286e9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+--------+\n",
      "|       Profile|         Experience|count(1)|\n",
      "+--------------+-------------------+--------+\n",
      "|       Dentist|10 years experience|       7|\n",
      "|      Ayurveda|15 years experience|       1|\n",
      "|       Dentist| 9 years experience|       3|\n",
      "|ENT Specialist|35 years experience|       1|\n",
      "|Dermatologists|33 years experience|       1|\n",
      "+--------------+-------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.createOrReplaceTempView(\"dfTable\")\n",
    "spark.sql(\"\"\"SELECT Profile, Experience,count(*) FROM dfTable GROUP BY Profile, Experience\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274cb42b-61f6-40f9-8c93-c035c130d6ae",
   "metadata": {},
   "source": [
    "#### Grouping with Expressions-\n",
    "\n",
    "Rather than passing that function as an expression\n",
    "into a \"select\" statement, we specify it as within \"agg\". This makes it possible for you to pass-in\n",
    "arbitrary expressions that just need to have some aggregation specified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "6a327eb2-5510-4d93-bdcb-ee0bebe08327",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+----+-----------------+\n",
      "|         Profile|Rating|quan|count(Experience)|\n",
      "+----------------+------+----+-----------------+\n",
      "|  Dermatologists|  null|   9|                9|\n",
      "|  ENT Specialist|   36%|   1|                1|\n",
      "|        Ayurveda|  null|  12|               12|\n",
      "|General Medicine|   93%|   1|                1|\n",
      "|  Dermatologists|   97%|   5|                5|\n",
      "|        Ayurveda|   98%|   1|                1|\n",
      "|       Homeopath|   82%|   1|                1|\n",
      "|  ENT Specialist|   94%|   1|                1|\n",
      "|General Medicine|   88%|   1|                1|\n",
      "|General Medicine|   99%|   1|                1|\n",
      "|  ENT Specialist|  100%|   2|                2|\n",
      "|  ENT Specialist|   80%|   1|                1|\n",
      "|       Homeopath|  null|  10|               10|\n",
      "|  Dermatologists|   94%|   1|                1|\n",
      "|  ENT Specialist|  null|   9|                9|\n",
      "|  ENT Specialist|   74%|   1|                1|\n",
      "|General Medicine|   96%|   1|                1|\n",
      "|  Dermatologists|   93%|   1|                1|\n",
      "|       Homeopath|  100%|   5|                5|\n",
      "|  Dermatologists|   95%|   2|                2|\n",
      "+----------------+------+----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "data_table.groupBy(\"Profile\",\"Rating\").agg(count(\"Experience\").alias(\"quan\"),\\\n",
    "                                           expr(\"count(Experience)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "cbf587f6-bbc5-4d19-9ad3-4ae29c6b0499",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+----+---------+\n",
      "|               Place|         Profile|quan|max(Fees)|\n",
      "+--------------------+----------------+----+---------+\n",
      "|                null|  Dermatologists| 100|      100|\n",
      "|AS Rao Nagar, Hyd...|         Dentist| 200|      200|\n",
      "|AS Rao Nagar, Hyd...|General Medicine| 200|      200|\n",
      "| Adambakkam, Chennai|       Homeopath| 150|      150|\n",
      "|   Ambattur, Chennai|         Dentist| 100|      100|\n",
      "|Andheri West, Mumbai|         Dentist| 500|      500|\n",
      "|Andheri West, Mumbai|General Medicine| 200|      200|\n",
      "|     Andheri, Mumbai|  Dermatologists| 100|      100|\n",
      "|Anna Nagar East, ...|        Ayurveda| 250|      250|\n",
      "|  Arekere, Bangalore|         Dentist| 250|      250|\n",
      "|Ashok Nagar, Chennai|  ENT Specialist| 200|      200|\n",
      "|   Athani, Ernakulam|         Dentist| 100|      100|\n",
      "|  Attapur, Hyderabad|  Dermatologists| 350|      350|\n",
      "|BTM Layout 2nd St...|       Homeopath| 200|      200|\n",
      "|   Bakkarwala, Delhi|        Ayurveda| 150|      150|\n",
      "|Banashankari 3rd ...|  ENT Specialist| 200|      200|\n",
      "| Bandra West, Mumbai|         Dentist| 500|      500|\n",
      "|Banjara Hills, Hy...|  Dermatologists| 500|      500|\n",
      "|Bannerghatta Road...|        Ayurveda| 250|      250|\n",
      "|Bannerghatta Road...|  Dermatologists| 400|      400|\n",
      "+--------------------+----------------+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.groupBy(\"Place\",\"Profile\").agg(max(\"Fees\").alias(\"quan\"),\\\n",
    "                                           expr(\"max(Fees)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8ff213-6f9d-48e9-847c-f20a5e15942f",
   "metadata": {},
   "source": [
    "Grouping with Maps-\n",
    "Sometimes, it can be easier to specify your transformations as a series of Maps for which the key\n",
    "is the column, and the value is the aggregation function (as a string) that you would like to\n",
    "perform. You can reuse multiple column names if you specify them inline, as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "996d285b-f66e-4118-a405-1259319eac12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+------------------+---------+\n",
      "|         Profile|         avg(Fees)|  stddev_pop(Fees)|max(Fees)|\n",
      "+----------------+------------------+------------------+---------+\n",
      "|        Ayurveda|227.77777777777777| 134.6004604398413|      500|\n",
      "|         Dentist|            241.25|110.61617196413913|      500|\n",
      "|  Dermatologists|             414.0|229.13751329714648|      800|\n",
      "|  ENT Specialist|371.05263157894734|194.20268134205358|      800|\n",
      "|General Medicine| 282.7586206896552|208.96950036035432|      800|\n",
      "|       Homeopath| 283.3333333333333|             150.0|      500|\n",
      "+----------------+------------------+------------------+---------+\n",
      "\n",
      "+--------+----------------------+------------------+------------------+---------+\n",
      "| Profile|split(Place, ,, -1)[1]|         avg(Fees)|  stddev_pop(Fees)|max(Fees)|\n",
      "+--------+----------------------+------------------+------------------+---------+\n",
      "|Ayurveda|             Bangalore|             350.0| 93.54143466934853|      500|\n",
      "|Ayurveda|               Chennai|             187.5|  64.9519052838329|      250|\n",
      "|Ayurveda|                 Delhi|166.66666666666666|102.74023338281629|       50|\n",
      "|Ayurveda|             Hyderabad|233.33333333333334| 94.28090415820634|      300|\n",
      "|Ayurveda|                Mumbai|             187.5|181.57298807917437|      500|\n",
      "| Dentist|             Bangalore|             210.0|              20.0|      250|\n",
      "| Dentist|               Chennai|             180.0| 74.83314773547882|      300|\n",
      "| Dentist|            Coimbatore|             180.0| 74.83314773547883|      300|\n",
      "| Dentist|                 Delhi| 314.2857142857143| 95.29760045804524|      500|\n",
      "| Dentist|             Ernakulam|             212.5|  73.9509972887452|      300|\n",
      "+--------+----------------------+------------------+------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_table.groupBy(\"Profile\").agg(expr(\"avg(Fees)\"),expr(\"stddev_pop(Fees)\"), expr(\"max(Fees)\")).show()\n",
    "\n",
    "## in the below line of code, we are extracting city name from \"Place\" column for better groupby operation\n",
    "data_table.groupBy(\"Profile\",split(col(\"Place\"),\",\")[1]).agg(expr(\"avg(Fees)\"),expr(\"stddev_pop(Fees)\"), expr(\"max(Fees)\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "dcba2b8c-d1eb-4e3e-b299-52b3f13b167d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------------+------------------+------------------+---------+\n",
      "| Profile|split(Place, ,, -1)[1]|         avg(Fees)|  stddev_pop(Fees)|max(Fees)|\n",
      "+--------+----------------------+------------------+------------------+---------+\n",
      "|Ayurveda|             Bangalore|             350.0| 93.54143466934853|      500|\n",
      "|Ayurveda|               Chennai|             187.5|  64.9519052838329|      250|\n",
      "|Ayurveda|                 Delhi|166.66666666666666|102.74023338281629|       50|\n",
      "|Ayurveda|             Hyderabad|233.33333333333334| 94.28090415820634|      300|\n",
      "|Ayurveda|                Mumbai|             187.5|181.57298807917437|      500|\n",
      "| Dentist|             Bangalore|             210.0|              20.0|      250|\n",
      "| Dentist|               Chennai|             180.0| 74.83314773547882|      300|\n",
      "| Dentist|            Coimbatore|             180.0| 74.83314773547883|      300|\n",
      "| Dentist|                 Delhi| 314.2857142857143| 95.29760045804524|      500|\n",
      "| Dentist|             Ernakulam|             212.5|  73.9509972887452|      300|\n",
      "+--------+----------------------+------------------+------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT Profile, split(Place, \",\")[1] ,avg(Fees), stddev_pop(Fees), max(Fees) FROM dfTable\n",
    "GROUP BY Profile, split(Place, \",\")[1]\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4c5aea-7685-48a9-91d0-0b79f3e6b85c",
   "metadata": {},
   "source": [
    "## Window Functions-\n",
    "\n",
    "Spark supports three kinds of window functions: ranking functions, analytic functions,\n",
    "and aggregate functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcccd82-a24a-46d4-b780-16beb190de24",
   "metadata": {},
   "source": [
    "The first step to a window function is to create a window specification. Note that the \"partition\n",
    "by\" is unrelated to the partitioning scheme concept that we have covered thus far. It’s just a\n",
    "similar concept that describes how we will be breaking up our group. The ordering determines\n",
    "the ordering within a given partition, and, finally, the frame specification (the \"rowsBetween\"\n",
    "statement) states which rows will be included in the frame based on its reference to the current\n",
    "input row. In the following example, we look at all previous rows up to the current row:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "07df162c-397f-4bb3-8027-cbb8ed49485f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc\n",
    "windowSpec = Window\\\n",
    ".partitionBy(split(col(\"Place\"), \",\")[1], \"Profile\")\\\n",
    ".orderBy(desc(\"Fees\"))\\\n",
    ".rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "2c86f116-535b-438d-914e-34cb89705ab7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#Now we want to use an aggregation function to learn more about each specific customer. \n",
    "#For now, establishing the maximum Fees over all time.\n",
    "\n",
    "from pyspark.sql.functions import max\n",
    "maxPurchaseQuantity = max(col(\"Fees\")).over(windowSpec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "4d3d3c83-17e0-40d3-aa26-c10237e06c27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#You will notice that this returns a column (or expressions). We can now use this in a DataFrame select statement\n",
    "\n",
    "from pyspark.sql.functions import dense_rank, rank\n",
    "purchaseDenseRank = dense_rank().over(windowSpec)\n",
    "purchaseRank = rank().over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "3b452216-2997-4f1a-a15e-2913d3754b32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----------------+------------+-----------------+-------------------+----+\n",
      "|split(Place, ,, -1)[1]|         Profile|quantityRank|quantityDenseRank|maxPurchaseQuantity|Fees|\n",
      "+----------------------+----------------+------------+-----------------+-------------------+----+\n",
      "|             Bangalore|        Ayurveda|           1|                1|                500| 500|\n",
      "|             Bangalore|        Ayurveda|           2|                2|                500| 350|\n",
      "|             Bangalore|        Ayurveda|           3|                3|                500| 300|\n",
      "|             Bangalore|         Dentist|           1|                1|                250| 250|\n",
      "|             Bangalore|         Dentist|           2|                2|                250| 200|\n",
      "|             Bangalore|         Dentist|           2|                2|                250| 200|\n",
      "|             Bangalore|         Dentist|           2|                2|                250| 200|\n",
      "|             Bangalore|         Dentist|           2|                2|                250| 200|\n",
      "|             Bangalore|  Dermatologists|           1|                1|                500| 500|\n",
      "|             Bangalore|  Dermatologists|           1|                1|                500| 500|\n",
      "|             Bangalore|  Dermatologists|           3|                2|                500| 400|\n",
      "|             Bangalore|  ENT Specialist|           1|                1|                600| 600|\n",
      "|             Bangalore|  ENT Specialist|           2|                2|                600| 350|\n",
      "|             Bangalore|  ENT Specialist|           3|                3|                600| 300|\n",
      "|             Bangalore|  ENT Specialist|           4|                4|                600| 200|\n",
      "|             Bangalore|General Medicine|           1|                1|                500| 500|\n",
      "|             Bangalore|General Medicine|           1|                1|                500| 500|\n",
      "|             Bangalore|General Medicine|           3|                2|                500| 150|\n",
      "|             Bangalore|       Homeopath|           1|                1|                500| 500|\n",
      "|             Bangalore|       Homeopath|           2|                2|                500| 200|\n",
      "+----------------------+----------------+------------+-----------------+-------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This also returns a column that we can use in select statements. Now we can perform a select to view the calculated window values:\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "data_table.where(\"Rating IS NOT NULL\").orderBy(\"Fees\")\\\n",
    ".select(\n",
    "split(col(\"Place\"), \",\")[1],col(\"Profile\"),\n",
    "purchaseRank.alias(\"quantityRank\"),\n",
    "purchaseDenseRank.alias(\"quantityDenseRank\"),\n",
    "maxPurchaseQuantity.alias(\"maxPurchaseQuantity\"),col(\"Fees\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "1998025e-e6dc-4128-9772-838ccc1c5878",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----------------+------------+-----------------+-------------------+----+\n",
      "|split(Place, ,, -1)[1]|         Profile|quantityRank|quantityDenseRank|maxPurchaseQuantity|Fees|\n",
      "+----------------------+----------------+------------+-----------------+-------------------+----+\n",
      "|             Bangalore|        Ayurveda|           1|                1|                500| 500|\n",
      "|             Bangalore|        Ayurveda|           2|                2|                500| 350|\n",
      "|             Bangalore|        Ayurveda|           3|                3|                500| 300|\n",
      "|             Bangalore|         Dentist|           1|                1|                250| 250|\n",
      "|             Bangalore|         Dentist|           2|                2|                250| 200|\n",
      "|             Bangalore|         Dentist|           2|                2|                250| 200|\n",
      "|             Bangalore|         Dentist|           2|                2|                250| 200|\n",
      "|             Bangalore|         Dentist|           2|                2|                250| 200|\n",
      "|             Bangalore|  Dermatologists|           1|                1|                500| 500|\n",
      "|             Bangalore|  Dermatologists|           1|                1|                500| 500|\n",
      "|             Bangalore|  Dermatologists|           3|                2|                500| 400|\n",
      "|             Bangalore|  ENT Specialist|           1|                1|                600| 600|\n",
      "|             Bangalore|  ENT Specialist|           2|                2|                600| 350|\n",
      "|             Bangalore|  ENT Specialist|           3|                3|                600| 300|\n",
      "|             Bangalore|  ENT Specialist|           4|                4|                600| 200|\n",
      "|             Bangalore|General Medicine|           1|                1|                500| 500|\n",
      "|             Bangalore|General Medicine|           1|                1|                500| 500|\n",
      "|             Bangalore|General Medicine|           3|                2|                500| 150|\n",
      "|             Bangalore|       Homeopath|           1|                1|                500| 500|\n",
      "|             Bangalore|       Homeopath|           2|                2|                500| 200|\n",
      "+----------------------+----------------+------------+-----------------+-------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select split(Place , \",\")[1], Profile, rank(Fees) over(partition by split(Place,\",\")[1], `Profile` ORDER BY Fees DESC )as quantityRank , \n",
    "                                                    dense_rank(Fees) over(partition by split(Place,\",\")[1], Profile order by Fees desc) as quantityDenseRank,\n",
    "                                                    max(Fees) over(partition by split(Place,\",\")[1], Profile order by Fees desc) as maxPurchaseQuantity,Fees\n",
    "                            \n",
    "                from dfTable--(select * from dfTable where Rating is not NULL order by Fees) \n",
    "                where Rating is not NULL\n",
    "                order by split(Place , \",\")[1], Profile, Fees desc  \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029dbfea-8896-4408-bdd1-31864f09ce87",
   "metadata": {},
   "source": [
    "#### Grouping Sets-\n",
    "\n",
    "Sometimes we want something a bit\n",
    "more complete—an aggregation across multiple groups. We achieve this by using grouping sets.\n",
    "Grouping sets are a low-level tool for combining sets of aggregations together. They give you the\n",
    "ability to create arbitrary aggregation in their group-by statements.\n",
    "\n",
    "In below eg., we would like to get the\n",
    "total quantity of all stock codes and customers. To do so, we’ll use the following SQL\n",
    "expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "e34faa24-ce21-4386-808b-439514d54c82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------------+---------+\n",
      "|         Profile|split(Place, ,, -1)[1]|sum(Fees)|\n",
      "+----------------+----------------------+---------+\n",
      "|            null|                  null|  44450.0|\n",
      "|  Dermatologists|                 Delhi|   3400.0|\n",
      "|General Medicine|             Hyderabad|   3050.0|\n",
      "|  Dermatologists|             Hyderabad|   2900.0|\n",
      "|  ENT Specialist|                 Delhi|   2800.0|\n",
      "|         Dentist|                 Delhi|   2200.0|\n",
      "|       Homeopath|             Bangalore|   1900.0|\n",
      "|  ENT Specialist|               Chennai|   1800.0|\n",
      "|         Dentist|                Mumbai|   1800.0|\n",
      "|  ENT Specialist|             Bangalore|   1750.0|\n",
      "+----------------+----------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT Profile, split(Place,\",\")[1], sum(Fees) FROM dfTable\n",
    "GROUP BY Profile, split(Place,\",\")[1] GROUPING SETS((Profile, split(Place,\",\")[1]),())\n",
    "ORDER BY sum(Fees) DESC\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5f2831-a9d3-4f02-b6fd-c26651f332ef",
   "metadata": {},
   "source": [
    "The GROUPING SETS operator is only available in SQL. To perform the same in DataFrames, you\n",
    "use the \"rollup\" and \"cube\" operators—which allow us to get the same results.\n",
    "\n",
    "\n",
    "Rollups-\n",
    "A rollup is a multidimensional aggregation that performs a variety of group-by style calculations\n",
    "for us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "08e559fa-0621-445b-bb49-0124f570333c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Column is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[367], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m rolledUpDF \u001b[38;5;241m=\u001b[39m \u001b[43mdata_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProfile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPlace\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFees\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselectExpr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProfile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPlace\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m`sum(Fees)` as total_Fees\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\\\n\u001b[0;32m      3\u001b[0m \u001b[38;5;241m.\u001b[39morderBy(split(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlace\u001b[39m\u001b[38;5;124m\"\u001b[39m),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      4\u001b[0m rolledUpDF\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:2048\u001b[0m, in \u001b[0;36mDataFrame.selectExpr\u001b[1;34m(self, *expr)\u001b[0m\n\u001b[0;32m   2046\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(expr) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(expr[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m   2047\u001b[0m     expr \u001b[38;5;241m=\u001b[39m expr[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m-> 2048\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mselectExpr(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jseq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2049\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:1741\u001b[0m, in \u001b[0;36mDataFrame._jseq\u001b[1;34m(self, cols, converter)\u001b[0m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_jseq\u001b[39m(\n\u001b[0;32m   1736\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1737\u001b[0m     cols: Sequence,\n\u001b[0;32m   1738\u001b[0m     converter: Optional[Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrimitiveType\u001b[39m\u001b[38;5;124m\"\u001b[39m, JavaObject]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1739\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JavaObject:\n\u001b[0;32m   1740\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a JVM Seq of Columns from a list of Column or names\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1741\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\column.py:88\u001b[0m, in \u001b[0;36m_to_seq\u001b[1;34m(sc, cols, converter)\u001b[0m\n\u001b[0;32m     86\u001b[0m     cols \u001b[38;5;241m=\u001b[39m [converter(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols]\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1313\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m-> 1313\u001b[0m     args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1315\u001b[0m     command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m         args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m         proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1277\u001b[0m, in \u001b[0;36mJavaMember._build_args\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_args\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m   1276\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverters) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1277\u001b[0m         (new_args, temp_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1279\u001b[0m         new_args \u001b[38;5;241m=\u001b[39m args\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1264\u001b[0m, in \u001b[0;36mJavaMember._get_args\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m converter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39mconverters:\n\u001b[0;32m   1263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m converter\u001b[38;5;241m.\u001b[39mcan_convert(arg):\n\u001b[1;32m-> 1264\u001b[0m         temp_arg \u001b[38;5;241m=\u001b[39m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1265\u001b[0m         temp_args\u001b[38;5;241m.\u001b[39mappend(temp_arg)\n\u001b[0;32m   1266\u001b[0m         new_args\u001b[38;5;241m.\u001b[39mappend(temp_arg)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_collections.py:511\u001b[0m, in \u001b[0;36mListConverter.convert\u001b[1;34m(self, object, gateway_client)\u001b[0m\n\u001b[0;32m    509\u001b[0m java_list \u001b[38;5;241m=\u001b[39m ArrayList()\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m--> 511\u001b[0m     \u001b[43mjava_list\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m java_list\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1313\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m-> 1313\u001b[0m     args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1315\u001b[0m     command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m         args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m         proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1277\u001b[0m, in \u001b[0;36mJavaMember._build_args\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_args\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m   1276\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverters) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1277\u001b[0m         (new_args, temp_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1279\u001b[0m         new_args \u001b[38;5;241m=\u001b[39m args\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1264\u001b[0m, in \u001b[0;36mJavaMember._get_args\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m converter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39mconverters:\n\u001b[0;32m   1263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m converter\u001b[38;5;241m.\u001b[39mcan_convert(arg):\n\u001b[1;32m-> 1264\u001b[0m         temp_arg \u001b[38;5;241m=\u001b[39m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1265\u001b[0m         temp_args\u001b[38;5;241m.\u001b[39mappend(temp_arg)\n\u001b[0;32m   1266\u001b[0m         new_args\u001b[38;5;241m.\u001b[39mappend(temp_arg)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_collections.py:510\u001b[0m, in \u001b[0;36mListConverter.convert\u001b[1;34m(self, object, gateway_client)\u001b[0m\n\u001b[0;32m    508\u001b[0m ArrayList \u001b[38;5;241m=\u001b[39m JavaClass(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjava.util.ArrayList\u001b[39m\u001b[38;5;124m\"\u001b[39m, gateway_client)\n\u001b[0;32m    509\u001b[0m java_list \u001b[38;5;241m=\u001b[39m ArrayList()\n\u001b[1;32m--> 510\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m    511\u001b[0m     java_list\u001b[38;5;241m.\u001b[39madd(element)\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m java_list\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\column.py:560\u001b[0m, in \u001b[0;36mColumn.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 560\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn is not iterable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Column is not iterable"
     ]
    }
   ],
   "source": [
    "rolledUpDF = data_table.rollup(\"Profile\", split(col(\"Place\"),\",\")[1]).agg(sum(\"Fees\"))\\\n",
    ".selectExpr(\"Profile\", split(col(\"Place\"),\",\")[1], \"`sum(Fees)` as total_Fees\")\\\n",
    ".orderBy(split(col(\"Place\"),\",\")[1])\n",
    "rolledUpDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8afc7dd-8549-4cd7-b370-8926f6dac84c",
   "metadata": {},
   "source": [
    "#### Cube-\n",
    "A \"cube\" takes the \"rollup\" to a level deeper. Rather than treating elements hierarchically, a \"cube\"\n",
    "does the same thing across all dimensions.\n",
    "\n",
    "The method call is quite similar, but instead of calling rollup, we call cube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "2c34f0ce-f6c4-4b08-be6c-601bb67deba0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "all exprs should be Column",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[376], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28msum\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdata_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcube\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProfile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPlace\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msum(Fees)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\\\n\u001b[0;32m      3\u001b[0m \u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProfile\u001b[39m\u001b[38;5;124m\"\u001b[39m, col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlace\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum(Fees)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProfile\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\group.py:135\u001b[0m, in \u001b[0;36mGroupedData.agg\u001b[1;34m(self, *exprs)\u001b[0m\n\u001b[0;32m    132\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jgd\u001b[38;5;241m.\u001b[39magg(exprs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;66;03m# Columns\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(c, Column) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m exprs), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall exprs should be Column\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    136\u001b[0m     exprs \u001b[38;5;241m=\u001b[39m cast(Tuple[Column, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], exprs)\n\u001b[0;32m    137\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jgd\u001b[38;5;241m.\u001b[39magg(exprs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_jc, _to_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39m_sc, [c\u001b[38;5;241m.\u001b[39m_jc \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m exprs[\u001b[38;5;241m1\u001b[39m:]]))\n",
      "\u001b[1;31mAssertionError\u001b[0m: all exprs should be Column"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "data_table.cube(\"Profile\", col(\"Place\")).agg(\"sum(Fees)\")\\\n",
    ".select(\"Profile\", col(\"Place\"), \"sum(Fees)\").orderBy(\"Profile\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de8f5f2-28dd-456f-a500-a7f1088e8f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
